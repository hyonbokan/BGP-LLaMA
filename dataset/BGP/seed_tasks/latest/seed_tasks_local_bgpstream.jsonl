{"id": "seed_task_0", "name": "BGP Update Summary for AS4766", "instruction": "Summarize the total count of BGP update messages (announcements and withdrawals) for AS4766 within the time window from Oct 28 13:00 to Oct 28 13:15, 2024. Include the minimum, maximum, and median number of updates observed during this period.", "instances": [{"input": "Time window: Oct 28, 2024, 13:00:00 to Oct 28, 2024, 13:15:00\nTarget ASN: 4766\nDirectory: /home/hb/ris_bgp_updates/2024/10/rrc00\nRecord Type: updates (Announcements and Withdrawals)", "output": "import pybgpstream\nfrom datetime import datetime, timezone\nimport os\nimport statistics\nimport re\n\ndef process_bgp_updates(directory, target_asn, from_time_str, until_time_str):\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.gz$'\n    update_counts = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp_str = date_str + time_str\n                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_time < from_time or file_time > until_time:\n                    continue\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n                count = 0\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n                        elem_type = elem.type\n                        fields = elem.fields\n                        as_path = fields.get(\"as-path\", \"\").split()\n                        if target_asn in as_path:\n                            if elem_type in {'A', 'W'}:\n                                count += 1\n                update_counts.append(count)\n    min_updates = min(update_counts) if update_counts else 0\n    max_updates = max(update_counts) if update_counts else 0\n    median_updates = statistics.median(update_counts) if update_counts else 0\n    print(f\"Summary for AS{target_asn}:\")\n    print(f\"Minimum updates: {min_updates}\")\n    print(f\"Maximum updates: {max_updates}\")\n    print(f\"Median updates: {median_updates}\")\nif __name__ == \"__main__\":\n    directory = \"/home/hb/ris_bgp_updates/2024/10/rrc00\"\n    target_asn = \"4766\"\n    from_time_str = \"2024-10-28 13:00:00\"\n    until_time_str = \"2024-10-28 13:15:00\"\n    process_bgp_updates(directory, target_asn=target_asn, from_time_str=from_time_str, until_time_str=until_time_str)"}], "is_classification": false}
{"id": "seed_task_1", "name": "Unique Prefixes and Origin AS Summary for AS4766", "instruction": "Provide a summary of unique prefixes and origin ASes associated with AS4766 from Oct 28 13:00 to 13:15, 2024. Track the count of unique prefixes and changes in origin ASes, if any.", "instances": [{"input": "Time window: Oct 28, 2024, 13:00:00 to Oct 28, 2024, 13:15:00\nTarget ASN: 4766\nDirectory: /home/hb/ris_bgp_updates/2024/10/rrc00\nRecord Type: updates (Announcements and Withdrawals)", "output": "import os\nimport re\nimport pybgpstream\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_local_bgp_updates(directory, target_asn, from_time_str, until_time_str):\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n    unique_prefixes = set()\n    origin_as_changes = defaultdict(set)\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp_str = date_str + time_str\n                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_time < from_time or file_time > until_time:\n                    continue\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n                        elem_type = elem.type\n                        fields = elem.fields\n                        prefix = fields.get(\"prefix\")\n                        if prefix is None:\n                            continue\n                        as_path_str = fields.get('as-path', \"\")\n                        as_path = as_path_str.split()\n                        if target_asn not in as_path:\n                            continue\n                        if elem_type == 'A':\n                            unique_prefixes.add(prefix)\n                            origin_asn = as_path[-1] if as_path else None\n                            if origin_asn:\n                                origin_as_changes[prefix].add(origin_asn)\n    print(f\"Total unique prefixes associated with AS{target_asn}: {len(unique_prefixes)}\")\n    for prefix, origin_set in origin_as_changes.items():\n        print(f\"Prefix: {prefix}\")\n        print(f\"  Origin AS count: {len(origin_set)}\")\n        if len(origin_set) > 1:\n            print(f\"  MOAS conflict observed: {origin_set}\")\nif __name__ == \"__main__\":\n    directory = \"/home/hb/ris_bgp_updates/2024/10/rrc00\"\n    target_asn = \"4766\"\n    from_time_str = \"2024-10-28 13:00:00\"\n    until_time_str = \"2024-10-28 13:15:00\"\n    analyze_local_bgp_updates(directory, target_asn=target_asn, from_time_str=from_time_str, until_time_str=until_time_str)"}], "is_classification": false}
{"id": "seed_task_2", "name": "AS Path Summary for AS4766", "instruction": "Summarize the AS paths for each prefix associated with ASN AS4766 over the period Oct 28 13:00 to Oct 28 13:15, 2024. Provide minimum, maximum, and median AS path lengths and highlight any significant path changes observed in BGP updates.", "instances": [{"input": "Time window: Oct 28, 2024, 13:00:00 to Oct 28, 2024, 13:15:00\nTarget ASN: 4766\nDirectory: /home/hb/ris_bgp_updates/2024/10/rrc00\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nfrom datetime import datetime, timezone\nfrom statistics import median\nfrom collections import defaultdict\n\ndef analyze_local_as_paths(directory, target_asn, from_time_str, until_time_str):\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n    prefix_paths = defaultdict(list)\n    path_changes = defaultdict(list)\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp_str = date_str + time_str\n                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_time < from_time or file_time > until_time:\n                    continue\n                file_path = os.path.join(root, file)\n                print(f\"Processing file: {file_path}\")\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n                        fields = elem.fields\n                        prefix = fields.get(\"prefix\")\n                        if not prefix:\n                            continue\n                        as_path_str = fields.get('as-path', \"\")\n                        as_path = as_path_str.split()\n                        if target_asn not in as_path:\n                            continue\n                        if elem.type == 'A':\n                            path_len = len(as_path)\n                            prefix_paths[prefix].append((as_path, path_len, elem_time))\n    for prefix, paths in prefix_paths.items():\n        lengths = [length for _, length, _ in paths]\n        min_len = min(lengths)\n        max_len = max(lengths)\n        med_len = median(lengths)\n        prev_path = None\n        for as_path, length, time in paths:\n            if prev_path and as_path != prev_path:\n                path_changes[prefix].append((time, prev_path, as_path))\n            prev_path = as_path\n        print(f\"Prefix: {prefix}\")\n        print(f\"  Min AS path length: {min_len}\")\n        print(f\"  Max AS path length: {max_len}\")\n        print(f\"  Median AS path length: {med_len}\")\n        if path_changes[prefix]:\n            print(\"  Significant AS path changes observed:\")\n            for t, old_path, new_path in path_changes[prefix]:\n                print(f\"    - At {t}: {old_path} -> {new_path}\")\nif __name__ == \"__main__\":\n    directory = \"/home/hb/ris_bgp_updates/2024/10/rrc00\"\n    target_asn = \"4766\"\n    from_time_str = \"2024-10-28 13:00:00\"\n    until_time_str = \"2024-10-28 13:15:00\"\n    analyze_local_as_paths(directory, target_asn=target_asn, from_time_str=from_time_str, until_time_str=until_time_str)"}], "is_classification": false}
{"id": "seed_task_3", "name": "High Update Frequency Analysis for AS4766", "instruction": "Identify prefixes associated with ASN 4766 that show high update frequency (announcements and withdrawals) within the period Oct 28 13:00 to Oct 28 13:15, 2024. Summarize the prefixes, update counts, and provide an assessment of their stability.", "instances": [{"input": "Time window: Oct 28, 2024, 13:00:00 to Oct 28, 2024, 13:15:00\nTarget ASN: 4766\nDirectory: /home/hb/ris_bgp_updates/2024/10/rrc00\nRecord Type: updates (Announcements and Withdrawals)", "output": "import os\nimport re\nimport pybgpstream\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_local_high_update_frequency(directory, target_asn, from_time_str, until_time_str):\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n    update_counts = defaultdict(int)\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp_str = date_str + time_str\n                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_time < from_time or file_time > until_time:\n                    continue\n                file_path = os.path.join(root, file)\n                print(f\"Processing file: {file_path}\")\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n                        fields = elem.fields\n                        prefix = fields.get(\"prefix\")\n                        if not prefix:\n                            continue\n                        as_path_str = fields.get('as-path', \"\")\n                        as_path = as_path_str.split()\n                        if target_asn not in as_path:\n                            continue\n                        if elem.type in {'A', 'W'}:\n                            update_counts[prefix] += 1\n    print(\"\\nPrefixes referencing AS4766 and their update frequency:\")\n    for prefix, count in update_counts.items():\n        stability = \"Unstable\" if count > 10 else \"Stable\"\n        print(f\"  Prefix: {prefix}, Update Count: {count}, Stability: {stability}\")\nif __name__ == \"__main__\":\n    directory = \"/home/hb/ris_bgp_updates/2024/10/rrc00\"\n    target_asn = \"4766\"\n    from_time_str = \"2024-10-28 13:00:00\"\n    until_time_str = \"2024-10-28 13:15:00\"\n    analyze_local_high_update_frequency(directory, target_asn=target_asn, from_time_str=from_time_str, until_time_str=until_time_str)"}], "is_classification": false}
{"id": "seed_task_4", "name": "MED and Community Tag Analysis for AS4766", "instruction": "Analyze the range of MED values and the most common community tags associated with BGP update messages for ASN 4766 from Oct 28 13:00 to Oct 28 14:00, 2024. Provide a summary of average MED values and frequently observed community tags.", "instances": [{"input": "Time window: Oct 28, 2024, 13:00:00 to Oct 28, 2024, 14:00:00\nTarget ASN: 4766\nDirectory: /home/hb/ris_bgp_updates/2024/10/rrc00\nRecord Type: updates (Announcements and Withdrawals)", "output": "import os\nimport re\nimport pybgpstream\nimport statistics\nfrom datetime import datetime, timezone\nfrom collections import defaultdict, Counter\n\ndef analyze_local_med_community(directory, target_asn, from_time_str, until_time_str):\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n    med_values = []\n    community_tags = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp_str = date_str + time_str\n                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_time < from_time or file_time > until_time:\n                    continue\n                file_path = os.path.join(root, file)\n                print(f\"Processing file: {file_path}\")\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n                        fields = elem.fields\n                        as_path_str = fields.get('as-path', \"\")\n                        as_path = as_path_str.split()\n                        if target_asn not in as_path:\n                            continue\n                        med_value = fields.get('med')\n                        if med_value is not None:\n                            try:\n                                med_values.append(int(med_value))\n                            except ValueError:\n                                pass\n                        communities = fields.get('communities', [])\n                        for c in communities:\n                            community_tags.append(c)\n    if med_values:\n        min_med = min(med_values)\n        max_med = max(med_values)\n        avg_med = statistics.mean(med_values)\n        print(f\"MED Value Range: {min_med} - {max_med}\")\n        print(f\"Average MED Value: {avg_med}\")\n    else:\n        print(\"No MED values found.\")\n    if community_tags:\n        community_counter = Counter(community_tags)\n        most_common_communities = community_counter.most_common(10)\n        print(\"Most Common Community Tags:\")\n        for community, count in most_common_communities:\n            print(f\"  {community} (observed {count} times)\")\n    else:\n        print(\"No community tags found.\")\nif __name__ == \"__main__\":\n    directory = \"/home/hb/ris_bgp_updates/2024/10/rrc00\"\n    target_asn = \"4766\"\n    from_time_str = \"2024-10-28 13:00:00\"\n    until_time_str = \"2024-10-28 14:00:00\"\n    analyze_local_med_community(directory, target_asn=target_asn, from_time_str=from_time_str, until_time_str=until_time_str)"}], "is_classification": false}
{"id": "seed_task_5", "name": "Flapping Prefix Analysis for AS4766", "instruction": "Identify prefixes associated with ASN 4766 that exhibit flapping behavior within the period Oct 28 13:00 to Oct 29 13:00, 2024. Summarize the prefixes, the number of flaps, and the duration between consecutive flaps.", "instances": [{"input": "Time window: Oct 28, 2024, 13:00:00 to Oct 29, 2024, 13:00:00\nTarget ASN: 4766\nDirectory: /home/hb/ris_bgp_updates/2024/10/rrc00\nRecord Type: updates (Announcements and Withdrawals)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\nimport statistics\n\ndef analyze_local_flapping(directory, target_asn, from_time_str, until_time_str):\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n    events = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp_str = date_str + time_str\n                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_time < from_time or file_time > until_time:\n                    continue\n                file_path = os.path.join(root, file)\n                print(f\"Processing file: {file_path}\")\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n                        fields = elem.fields\n                        prefix = fields.get(\"prefix\")\n                        if not prefix:\n                            continue\n                        elem_type = elem.type\n                        as_path_str = fields.get('as-path', \"\")\n                        as_path = as_path_str.split()\n                        if as_path and as_path[-1] == target_asn:\n                            if elem_type in ('A', 'W'):\n                                events.append({\n                                    'prefix': prefix,\n                                    'timestamp': elem_time,\n                                    'event_type': elem_type\n                                })\n    df = pd.DataFrame(events)\n    if df.empty:\n        print(f\"No updates found for origin {target_asn} in the given time period.\")\n        return\n    flapping_info = []\n    for prefix, group in df.groupby('prefix'):\n        group = group.sort_values('timestamp')\n        group['change'] = group['event_type'].shift() != group['event_type']\n        flap_events = group[group['change']]\n        num_flaps = len(flap_events)\n        if num_flaps > 1:\n            durations = flap_events['timestamp'].diff().dt.total_seconds().dropna()\n            avg_duration = durations.mean() if not durations.empty else 0\n            flapping_info.append({\n                'prefix': prefix,\n                'number_of_flaps': num_flaps,\n                'average_duration_between_flaps_seconds': avg_duration\n            })\n    if not flapping_info:\n        print(f\"No flapping prefixes detected for ASN {target_asn} in the given time period.\")\n        return\n    flap_df = pd.DataFrame(flapping_info)\n    print(f\"Flapping prefixes for origin {target_asn}:\")\n    print(flap_df)\nif __name__ == \"__main__\":\n    directory = \"/home/hb/ris_bgp_updates/2024/10/rrc00\"\n    target_asn = \"4766\"\n    from_time_str = \"2024-10-28 13:00:00\"\n    until_time_str = \"2024-10-29 13:00:00\"\n    analyze_local_flapping(directory, target_asn=target_asn, from_time_str=from_time_str, until_time_str=until_time_str)"}], "is_classification": false}
{"id": "seed_task_6", "name": "Unusual AS Path Changes for AS4766", "instruction": "Detect unusual AS path changes for prefixes associated with ASN 4766 from Oct 28 13:00 to Oct 28 13:15, 2024. Summarize the prefixes, number of path changes, and frequency of these changes.", "instances": [{"input": "Time window: Oct 28, 2024, 13:00:00 to Oct 28, 2024, 13:15:00\nTarget ASN: 4766\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_local_as_path_changes():\n    from_time_str = \"2024-10-28 13:00:00\"\n    until_time_str = \"2024-10-28 13:15:00\"\n    target_asn = \"4766\"\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    directory = \"/home/hb/ris_bgp_updates/2024/10/rrc00\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n    prefix_as_paths = defaultdict(set)\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp_str = date_str + time_str\n                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_time < from_time or file_time > until_time:\n                    continue\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n                            if not prefix:\n                                continue\n                            as_path_str = fields.get('as-path', '')\n                            as_path = as_path_str.split()\n                            prefix_as_paths[prefix].add(as_path_str)\n    as_path_changes = []\n    for prefix, path_set in prefix_as_paths.items():\n        if len(path_set) > 1:\n            as_path_changes.append({\n                'prefix': prefix,\n                'number_of_path_changes': len(path_set),\n                'as_paths': list(path_set)\n            })\n    df_as_changes = pd.DataFrame(as_path_changes)\n    if not df_as_changes.empty:\n        print(f\"Prefixes with AS path changes for ASN {target_asn}:\")\n        print(df_as_changes)\n    else:\n        print(f\"No unusual AS path changes detected for ASN {target_asn} in the given time period.\")\nif __name__ == \"__main__\":\n    analyze_local_as_path_changes()"}], "is_classification": false}
{"id": "seed_task_7", "name": "Suspected BGP Hijacking for AS16509", "instruction": "Identify suspected BGP hijacking incidents involving AS16509 or its prefixes (177.93.174.0/23, 138.59.238.0/23, 177.93.168.0/23) from April 24 10:00 to 12:00, 2018. Summarize the prefixes, unauthorized ASNs, and frequency of suspected hijacks.", "instances": [{"input": "Time window: April 24, 2018, 10:00:00 to April 24, 2018, 12:00:00\nTarget ASN: 16509\nKnown Prefixes: 177.93.174.0/23, 138.59.238.0/23, 177.93.168.0/23\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_local_hijacking():\n    asn = \"16509\"\n    from_time_str = \"2018-04-24 10:00:00\"\n    until_time_str = \"2018-04-24 12:00:00\"\n    known_prefixes = {\"177.93.174.0/23\", \"138.59.238.0/23\", \"177.93.168.0/23\"}\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    directory = \"/home/hb/ris_bgp_updates/2018/04/rrc00\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n    hijack_attempts = defaultdict(lambda: {\"unauthorized_asns\": set(), \"count\": 0})\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp_str = date_str + time_str\n                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_time < from_time or file_time > until_time:\n                    continue\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n                        if elem.type == \"A\":\n                            fields = elem.fields\n                            prefix = fields.get(\"prefix\")\n                            if not prefix:\n                                continue\n                            as_path_str = fields.get(\"as-path\", \"\")\n                            as_path_list = as_path_str.strip().split()\n                            origin_asn = as_path_list[-1] if as_path_list else None\n                            if prefix in known_prefixes and origin_asn != asn:\n                                hijack_attempts[prefix][\"unauthorized_asns\"].add(origin_asn)\n                                hijack_attempts[prefix][\"count\"] += 1\n    results = []\n    for prefix, data in hijack_attempts.items():\n        results.append({\n            \"prefix\": prefix,\n            \"unauthorized_asns\": list(data[\"unauthorized_asns\"]),\n            \"frequency_of_suspected_hijacks\": data[\"count\"]\n        })\n    df_hijacks = pd.DataFrame(results)\n    if not df_hijacks.empty:\n        print(f\"Suspected BGP hijacking incidents involving ASN {asn}:\")\n        print(df_hijacks)\n    else:\n        print(f\"No suspected hijacking incidents detected for ASN {asn} in the given time period.\")\nif __name__ == \"__main__\":\n    analyze_local_hijacking()"}], "is_classification": false}
{"id": "seed_task_8", "name": "Multiple Origin AS Detection for Example Prefixes", "instruction": "Detect multiple origin AS (MOAS) occurrences for a set of example prefix ranges from Jan 1 00:00 to Jan 1 01:00, 2025. Summarize the MOAS prefixes, the distinct origin ASes, and the number of occurrences.", "instances": [{"input": "Time window: Jan 1, 2025, 00:00:00 to Jan 1, 2025, 01:00:00\nObserved Prefixes: 192.0.2.0/24, 198.51.100.0/24\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef detect_moas():\n    from_time_str = \"2025-01-01 00:00:00\"\n    until_time_str = \"2025-01-01 01:00:00\"\n    observed_prefixes = {\"192.0.2.0/24\", \"198.51.100.0/24\"}\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    prefix_to_origins = defaultdict(set)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp = date_str + time_str\n                file_dt = datetime.strptime(file_timestamp, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n                            if prefix in observed_prefixes:\n                                as_path_str = fields.get('as-path', '')\n                                as_path = as_path_str.split()\n                                if as_path:\n                                    origin_as = as_path[-1]\n                                    prefix_to_origins[prefix].add(origin_as)\n\n    moas_results = []\n    for pfx, origins in prefix_to_origins.items():\n        if len(origins) > 1:\n            moas_results.append({\n                'prefix': pfx,\n                'distinct_origins': list(origins),\n                'occurrences': len(origins)\n            })\n\n    df_moas = pd.DataFrame(moas_results)\n    if not df_moas.empty:\n        print(\"MOAS occurrences detected:\")\n        print(df_moas)\n    else:\n        print(\"No MOAS occurrences detected in the given time window.\")\n\nif __name__ == \"__main__\":\n    detect_moas()\n"}], "is_classification": false}
{"id": "seed_task_9", "name": "Time-Series Trend Analysis of Announcements vs. Withdrawals", "instruction": "From March 15 10:00 to 10:30, 2023, generate a time-series view (5-minute intervals) of route announcements and withdrawals for prefix 8.8.8.0/24. Summarize results in tabular form showing timestamp, announcements count, and withdrawals count.", "instances": [{"input": "Time window: March 15, 2023, 10:00:00 to March 15, 2023, 10:30:00\nTarget Prefix: 8.8.8.0/24\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone, timedelta\nfrom collections import defaultdict\n\ndef analyze_time_series():\n    prefix_of_interest = \"8.8.8.0/24\"\n    from_time_str = \"2023-03-15 10:00:00\"\n    until_time_str = \"2023-03-15 10:30:00\"\n    interval_minutes = 5\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    # Prepare time buckets\n    current_bucket_start = from_time\n    time_buckets = []\n    while current_bucket_start < until_time:\n        bucket_end = current_bucket_start + timedelta(minutes=interval_minutes)\n        time_buckets.append((current_bucket_start, bucket_end))\n        current_bucket_start = bucket_end\n\n    # Counters for announcements and withdrawals per time bucket\n    bucket_counters = [defaultdict(int) for _ in time_buckets]\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp = date_str + time_str\n                file_dt = datetime.strptime(file_timestamp, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.fields.get('prefix') == prefix_of_interest:\n                            for i, (bucket_start, bucket_end) in enumerate(time_buckets):\n                                if bucket_start <= elem_dt < bucket_end:\n                                    if elem.type == 'A':\n                                        bucket_counters[i]['announcements'] += 1\n                                    elif elem.type == 'W':\n                                        bucket_counters[i]['withdrawals'] += 1\n                                    break\n\n    # Build results\n    results = []\n    for i, (bucket_start, bucket_end) in enumerate(time_buckets):\n        results.append({\n            'time_bucket': f\"{bucket_start.strftime('%Y-%m-%d %H:%M')} - {bucket_end.strftime('%H:%M')}\",\n            'announcements': bucket_counters[i].get('announcements', 0),\n            'withdrawals': bucket_counters[i].get('withdrawals', 0)\n        })\n\n    df_trends = pd.DataFrame(results)\n    print(\"Time-series trend for prefix 8.8.8.0/24:\")\n    print(df_trends)\n\nif __name__ == \"__main__\":\n    analyze_time_series()\n"}], "is_classification": false}
{"id": "seed_task_10", "name": "BGP Community and Policy Analysis", "instruction": "Identify and summarize BGP communities observed for prefix 9.9.9.0/24 from Feb 1 08:00 to Feb 1 10:00, 2025. List unique communities and the frequency of their appearance in route announcements.", "instances": [{"input": "Time window: Feb 1, 2025, 08:00:00 to Feb 1, 2025, 10:00:00\nTarget Prefix: 9.9.9.0/24\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_bgp_communities():\n    prefix_of_interest = \"9.9.9.0/24\"\n    from_time_str = \"2025-02-01 08:00:00\"\n    until_time_str = \"2025-02-01 10:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    community_counts = defaultdict(int)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp = date_str + time_str\n                file_dt = datetime.strptime(file_timestamp, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n\n                            if prefix == prefix_of_interest:\n                                community_list = fields.get('communities', [])\n                                # communities is typically a list of (ASN, value) in PyBGPStream\n                                # But may vary based on version. Checking if we have them:\n                                if community_list:\n                                    for comm in community_list:\n                                        # Convert tuple to string\n                                        community_str = f\"{comm[0]}:{comm[1]}\"\n                                        community_counts[community_str] += 1\n\n    if community_counts:\n        df = pd.DataFrame(\n            [(comm, freq) for comm, freq in community_counts.items()],\n            columns=[\"community\", \"frequency\"]\n        ).sort_values(by=\"frequency\", ascending=False)\n        print(\"BGP Community Analysis for 9.9.9.0/24\")\n        print(\"-------------------------------------\")\n        print(df)\n    else:\n        print(\"No BGP communities detected for the prefix in the given time window.\")\n\nif __name__ == \"__main__\":\n    analyze_bgp_communities()\n"}], "is_classification": false}
{"id": "seed_task_11", "name": "Route Leak Detection for AS1312", "instruction": "Detect route leaks originating from AS1312 for any monitored prefix between Dec 10 00:00 and Dec 10 02:00, 2024. Summarize suspect announcements where AS1312 appears as an intermediate AS, but not as the legitimate origin.", "instances": [{"input": "Time window: Dec 10, 2024, 00:00:00 to Dec 10, 2024, 02:00:00\nSuspicious ASN: 1312\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef detect_route_leaks():\n    suspicious_asn = \"1312\"\n    from_time_str = \"2024-12-10 00:00:00\"\n    until_time_str = \"2024-12-10 02:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    leak_data = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp = date_str + time_str\n                file_dt = datetime.strptime(file_timestamp, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n                            as_path_str = fields.get('as-path', '')\n                            as_path_list = as_path_str.split()\n\n                            # If suspicious ASN is in the path, but not the last AS in path\n                            if suspicious_asn in as_path_list:\n                                if as_path_list[-1] != suspicious_asn:  # not the origin AS\n                                    leak_data.append({\n                                        \"timestamp\": elem_dt,\n                                        \"prefix\": prefix,\n                                        \"as_path\": as_path_str\n                                    })\n\n    if leak_data:\n        df_leaks = pd.DataFrame(leak_data)\n        print(f\"Route Leaks Detected Involving AS{suspicious_asn}:\")\n        print(df_leaks)\n    else:\n        print(f\"No route leaks detected for AS{suspicious_asn} in the given time window.\")\n\nif __name__ == \"__main__\":\n    detect_route_leaks()\n"}], "is_classification": false}
{"id": "seed_task_12", "name": "Prefix Flapping Detection for 203.0.113.0/24", "instruction": "Determine if prefix 203.0.113.0/24 is flapping (excessive announcements and withdrawals) within the time window May 5 09:00 to May 5 10:00, 2025. Summarize total announcements, total withdrawals, and flag a flap if announcements+withdrawals exceed 20 events in any 5-minute interval.", "instances": [{"input": "Time window: May 5, 2025, 09:00:00 to May 5, 2025, 10:00:00\nTarget Prefix: 203.0.113.0/24\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone, timedelta\nfrom collections import defaultdict\n\ndef detect_prefix_flapping():\n    target_prefix = \"203.0.113.0/24\"\n    from_time_str = \"2025-05-05 09:00:00\"\n    until_time_str = \"2025-05-05 10:00:00\"\n    interval_minutes = 5\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    # Prepare 5-minute intervals\n    intervals = []\n    current_start = from_time\n    while current_start < until_time:\n        current_end = current_start + timedelta(minutes=interval_minutes)\n        intervals.append((current_start, current_end))\n        current_start = current_end\n\n    # Counters for announcements and withdrawals in each interval\n    flap_counters = [defaultdict(int) for _ in intervals]\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp = date_str + time_str\n                file_dt = datetime.strptime(file_timestamp, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.fields.get('prefix') == target_prefix:\n                            # Identify which interval this update falls into\n                            for i, (start_i, end_i) in enumerate(intervals):\n                                if start_i <= elem_dt < end_i:\n                                    if elem.type == 'A':\n                                        flap_counters[i]['announcements'] += 1\n                                    elif elem.type == 'W':\n                                        flap_counters[i]['withdrawals'] += 1\n                                    break\n\n    # Summarize results\n    flap_report = []\n    for i, (start_i, end_i) in enumerate(intervals):\n        ann = flap_counters[i].get('announcements', 0)\n        wdr = flap_counters[i].get('withdrawals', 0)\n        total = ann + wdr\n        flap_report.append({\n            'interval': f\"{start_i.strftime('%Y-%m-%d %H:%M')} - {end_i.strftime('%H:%M')}\",\n            'announcements': ann,\n            'withdrawals': wdr,\n            'total_events': total,\n            'flap_detected': total > 20\n        })\n\n    df_flaps = pd.DataFrame(flap_report)\n    print(\"Prefix Flapping Detection Report for 203.0.113.0/24\")\n    print(\"-------------------------------------------------\")\n    print(df_flaps)\n\nif __name__ == \"__main__\":\n    detect_prefix_flapping()\n"}], "is_classification": false}
{"id": "seed_task_13", "name": "AS Relationship and Topology Analysis for AS65000", "instruction": "Identify upstream providers for AS65000 by analyzing announcements in the local update files from July 1 00:00 to July 1 01:00, 2025. Summarize unique next-hop ASes seen directly before AS65000 in the AS path.", "instances": [{"input": "Time window: July 1, 2025, 00:00:00 to July 1, 2025, 01:00:00\nTarget ASN: 65000\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_as_relationships():\n    target_asn = \"65000\"\n    from_time_str = \"2025-07-01 00:00:00\"\n    until_time_str = \"2025-07-01 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    # Store each next-hop AS found directly before the target in the path\n    upstream_providers = defaultdict(int)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp = date_str + time_str\n                file_dt = datetime.strptime(file_timestamp, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            as_path_str = elem.fields.get('as-path', '')\n                            as_path_list = as_path_str.split()\n\n                            # Find index of target_asn if it exists\n                            if target_asn in as_path_list:\n                                idx = as_path_list.index(target_asn)\n                                # If target_asn is not the first in the path, there's a next-hop before it\n                                if idx > 0:\n                                    upstream_as = as_path_list[idx - 1]\n                                    upstream_providers[upstream_as] += 1\n\n    if upstream_providers:\n        df = pd.DataFrame(\n            [(asn, count) for asn, count in upstream_providers.items()],\n            columns=[\"upstream_as\", \"occurrences\"]\n        ).sort_values(by=\"occurrences\", ascending=False)\n\n        print(f\"Upstream Providers for AS{target_asn}:\")\n        print(df)\n    else:\n        print(f\"No upstream providers found for AS{target_asn} in the given time window.\")\n\nif __name__ == \"__main__\":\n    analyze_as_relationships()\n"}], "is_classification": false}
{"id": "seed_task_14", "name": "Correlation of Prefix Announcements with Historical Data", "instruction": "Compare current announcements for prefix 198.18.0.0/16 from Aug 1 00:00 to Aug 1 01:00, 2025 with historical data from Jul 1 00:00 to Jul 1 01:00, 2025. Identify differences in AS path length, origin AS, and announcement volume.", "instances": [{"input": "Current Time Window: Aug 1, 2025, 00:00:00 to Aug 1, 2025, 01:00:00\nHistorical Baseline: Jul 1, 2025, 00:00:00 to Jul 1, 2025, 01:00:00\nTarget Prefix: 198.18.0.0/16\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\nCURRENT_DIRECTORY = \"/path/to/local/bgp/current_updates\"  # for Aug 1 2025\nHISTORICAL_DIRECTORY = \"/path/to/local/bgp/historical_updates\"  # for Jul 1 2025\n\npattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n# Helper function to gather stats from a given directory/time range\n\ndef gather_stats(directory, from_time, until_time, prefix):\n    stats = {\n        \"as_path_lengths\": [],\n        \"origin_counts\": defaultdict(int),\n        \"announcement_count\": 0\n    }\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp = date_str + time_str\n                file_dt = datetime.strptime(file_timestamp, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            pfx = elem.fields.get('prefix')\n                            if pfx == prefix:\n                                stats[\"announcement_count\"] += 1\n\n                                as_path_str = elem.fields.get('as-path', '')\n                                as_path_list = as_path_str.split()\n                                if as_path_list:\n                                    stats[\"as_path_lengths\"].append(len(as_path_list))\n                                    origin_as = as_path_list[-1]\n                                    stats[\"origin_counts\"][origin_as] += 1\n    return stats\n\ndef compare_current_vs_historical():\n    prefix_of_interest = \"198.18.0.0/16\"\n\n    # Current window: Aug 1 00:00 to Aug 1 01:00, 2025\n    current_from_str = \"2025-08-01 00:00:00\"\n    current_until_str = \"2025-08-01 01:00:00\"\n    current_from = datetime.strptime(current_from_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    current_until = datetime.strptime(current_until_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    # Historical window: Jul 1 00:00 to Jul 1 01:00, 2025\n    hist_from_str = \"2025-07-01 00:00:00\"\n    hist_until_str = \"2025-07-01 01:00:00\"\n    hist_from = datetime.strptime(hist_from_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    hist_until = datetime.strptime(hist_until_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    current_stats = gather_stats(CURRENT_DIRECTORY, current_from, current_until, prefix_of_interest)\n    historical_stats = gather_stats(HISTORICAL_DIRECTORY, hist_from, hist_until, prefix_of_interest)\n\n    # Compare AS path lengths\n    curr_len_series = pd.Series(current_stats[\"as_path_lengths\"]) if current_stats[\"as_path_lengths\"] else pd.Series([0])\n    hist_len_series = pd.Series(historical_stats[\"as_path_lengths\"]) if historical_stats[\"as_path_lengths\"] else pd.Series([0])\n\n    print(\"Current vs Historical BGP Data for Prefix 198.18.0.0/16\")\n    print(\"--------------------------------------------------------\")\n\n    print(\"\\nAnnouncement Volume:\")\n    print(f\"  Current: {current_stats['announcement_count']}\")\n    print(f\"  Historical: {historical_stats['announcement_count']}\")\n\n    print(\"\\nAS Path Length Statistics (Current):\")\n    print(curr_len_series.describe())\n\n    print(\"\\nAS Path Length Statistics (Historical):\")\n    print(hist_len_series.describe())\n\n    print(\"\\nTop Origin ASes (Current):\")\n    df_curr_origins = pd.DataFrame(\n        [(asn, count) for asn, count in current_stats[\"origin_counts\"].items()],\n        columns=[\"ASN\", \"Count\"]\n    ).sort_values(\"Count\", ascending=False)\n    print(df_curr_origins)\n\n    print(\"\\nTop Origin ASes (Historical):\")\n    df_hist_origins = pd.DataFrame(\n        [(asn, count) for asn, count in historical_stats[\"origin_counts\"].items()],\n        columns=[\"ASN\", \"Count\"]\n    ).sort_values(\"Count\", ascending=False)\n    print(df_hist_origins)\n\nif __name__ == \"__main__\":\n    compare_current_vs_historical()\n"}], "is_classification": false}
{"id": "seed_task_15", "name": "Network Reliability and Performance for 192.168.0.0/16", "instruction": "Measure network reliability by counting the frequency of route changes (announcements and withdrawals) for prefix 192.168.0.0/16 between Jan 1 00:00 and Jan 1 02:00, 2025. Identify intervals with high churn.", "instances": [{"input": "Time window: Jan 1, 2025, 00:00:00 to Jan 1, 2025, 02:00:00\nTarget Prefix: 192.168.0.0/16\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone, timedelta\nfrom collections import defaultdict\n\ndef analyze_network_reliability():\n    prefix_of_interest = \"192.168.0.0/16\"\n    from_time_str = \"2025-01-01 00:00:00\"\n    until_time_str = \"2025-01-01 02:00:00\"\n    interval_minutes = 10  # aggregate churn in 10-minute intervals\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    # Prepare intervals\n    intervals = []\n    current_start = from_time\n    while current_start < until_time:\n        current_end = current_start + timedelta(minutes=interval_minutes)\n        intervals.append((current_start, current_end))\n        current_start = current_end\n\n    churn_counters = [defaultdict(int) for _ in intervals]\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.fields.get('prefix') == prefix_of_interest:\n                            for i, (start_i, end_i) in enumerate(intervals):\n                                if start_i <= elem_dt < end_i:\n                                    if elem.type == 'A':\n                                        churn_counters[i]['announcements'] += 1\n                                    elif elem.type == 'W':\n                                        churn_counters[i]['withdrawals'] += 1\n                                    break\n\n    results = []\n    for i, (start_i, end_i) in enumerate(intervals):\n        ann = churn_counters[i].get('announcements', 0)\n        wdr = churn_counters[i].get('withdrawals', 0)\n        total = ann + wdr\n        results.append({\n            'interval': f\"{start_i.strftime('%Y-%m-%d %H:%M')} - {end_i.strftime('%H:%M')}\",\n            'announcements': ann,\n            'withdrawals': wdr,\n            'total_changes': total\n        })\n\n    df_churn = pd.DataFrame(results)\n    print(\"Network Reliability and Performance Metrics for 192.168.0.0/16\")\n    print(\"----------------------------------------------------------------\")\n    print(df_churn)\n\n    high_churn_intervals = df_churn[df_churn['total_changes'] > 20]\n    if not high_churn_intervals.empty:\n        print(\"\\nHigh churn detected in the following intervals:\")\n        print(high_churn_intervals)\n    else:\n        print(\"\\nNo high churn intervals detected in the given time range.\")\n\nif __name__ == \"__main__\":\n    analyze_network_reliability()\n"}], "is_classification": false}
{"id": "seed_task_16", "name": "Security-related BGP Behavior: Route Leak Investigation", "instruction": "Investigate potential route leaks involving prefix 100.64.0.0/10 by AS64496 from April 1 00:00 to April 1 01:00, 2025. Check if AS64496 appears in paths where it is neither the origin nor a legitimate transit AS based on prior knowledge (AS64496 should not be an upstream for this prefix). Summarize suspicious announcements.", "instances": [{"input": "Time window: April 1, 2025, 00:00:00 to April 1, 2025, 01:00:00\nSuspected ASN: 64496\nTarget Prefix: 100.64.0.0/10\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\n\ndef investigate_route_leaks():\n    suspect_asn = \"64496\"\n    target_prefix = \"100.64.0.0/10\"\n    from_time_str = \"2025-04-01 00:00:00\"\n    until_time_str = \"2025-04-01 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    suspicious_announcements = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            pfx = elem.fields.get('prefix')\n                            if pfx == target_prefix:\n                                as_path_str = elem.fields.get('as-path', '')\n                                as_path_list = as_path_str.split()\n\n                                if suspect_asn in as_path_list:\n                                    # If suspect_asn is not last (origin) or recognized transit\n                                    # Here, we simply check if suspect_asn is not the last in the path\n                                    # Real logic could compare with known policy data\n                                    if as_path_list[-1] != suspect_asn:\n                                        suspicious_announcements.append({\n                                            \"timestamp\": elem_dt,\n                                            \"prefix\": pfx,\n                                            \"as_path\": as_path_str\n                                        })\n\n    if suspicious_announcements:\n        df_leaks = pd.DataFrame(suspicious_announcements)\n        print(\"Suspicious Route Leaks involving AS64496:\")\n        print(df_leaks)\n    else:\n        print(\"No suspicious route leaks detected for AS64496 in the given timeframe.\")\n\nif __name__ == \"__main__\":\n    investigate_route_leaks()\n"}], "is_classification": false}
{"id": "seed_task_17", "name": "BGP Aggregator Attribute Analysis", "instruction": "Examine the aggregator attribute for any prefixes originated by AS12345 between Feb 10 10:00 and Feb 10 11:00, 2025. Summarize the distinct aggregator ASNs and IPs along with occurrence counts.", "instances": [{"input": "Time window: Feb 10, 2025, 10:00:00 to Feb 10, 2025, 11:00:00\nTarget ASN: 12345\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_aggregator_attribute():\n    target_asn = \"12345\"\n    from_time_str = \"2025-02-10 10:00:00\"\n    until_time_str = \"2025-02-10 11:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    aggregator_info = defaultdict(int)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            as_path_str = elem.fields.get('as-path', '')\n                            as_path_list = as_path_str.split()\n                            if as_path_list:\n                                origin_as = as_path_list[-1]\n                                if origin_as == target_asn:\n                                    agg_asn = elem.fields.get('aggregator', {}).get('asn')\n                                    agg_ip = elem.fields.get('aggregator', {}).get('ip')\n                                    if agg_asn and agg_ip:\n                                        key = f\"AS{agg_asn} {agg_ip}\"\n                                        aggregator_info[key] += 1\n\n    if aggregator_info:\n        df_agg = pd.DataFrame(\n            [(agg, count) for agg, count in aggregator_info.items()],\n            columns=[\"aggregator\", \"count\"]\n        ).sort_values(by=\"count\", ascending=False)\n        print(\"Aggregator Attributes for Origin AS12345\")\n        print(df_agg)\n    else:\n        print(\"No aggregator attribute data found for the specified time window.\")\n\nif __name__ == \"__main__\":\n    analyze_aggregator_attribute()\n"}], "is_classification": false}
{"id": "seed_task_18", "name": "Longest and Shortest AS Path Detection", "instruction": "Identify the longest and shortest AS paths for any announcements of prefix 10.0.0.0/8 from Mar 1 00:00 to Mar 1 01:00, 2025. Summarize the paths, their lengths, and the corresponding origin AS.", "instances": [{"input": "Time window: Mar 1, 2025, 00:00:00 to Mar 1, 2025, 01:00:00\nTarget Prefix: 10.0.0.0/8\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\n\ndef detect_longest_shortest_as_paths():\n    target_prefix = \"10.0.0.0/8\"\n    from_time_str = \"2025-03-01 00:00:00\"\n    until_time_str = \"2025-03-01 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    paths_collected = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            pfx = elem.fields.get('prefix')\n                            if pfx == target_prefix:\n                                as_path_str = elem.fields.get('as-path', '')\n                                as_path_list = as_path_str.split()\n                                if as_path_list:\n                                    path_length = len(as_path_list)\n                                    origin_as = as_path_list[-1]\n                                    paths_collected.append({\n                                        'as_path': as_path_str,\n                                        'path_length': path_length,\n                                        'origin_as': origin_as\n                                    })\n\n    if paths_collected:\n        df_paths = pd.DataFrame(paths_collected)\n        longest_path_row = df_paths.loc[df_paths['path_length'].idxmax()]\n        shortest_path_row = df_paths.loc[df_paths['path_length'].idxmin()]\n\n        print(\"Longest AS Path Detected:\")\n        print(longest_path_row)\n        print(\"\\nShortest AS Path Detected:\")\n        print(shortest_path_row)\n    else:\n        print(\"No AS paths were collected for the target prefix in the given time window.\")\n\nif __name__ == \"__main__\":\n    detect_longest_shortest_as_paths()\n"}], "is_classification": false}
{"id": "seed_task_19", "name": "MED Variation Analysis for AS65001", "instruction": "Track the changes in the Multi-Exit Discriminator (MED) attribute for routes originating from AS65001 from June 10 00:00 to June 10 01:00, 2025. Summarize each prefix, the range of MED values observed, and any unusual fluctuations.", "instances": [{"input": "Time window: June 10, 2025, 00:00:00 to June 10, 2025, 01:00:00\nTarget ASN: 65001\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_med_variation():\n    target_asn = \"65001\"\n    from_time_str = \"2025-06-10 00:00:00\"\n    until_time_str = \"2025-06-10 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    med_records = defaultdict(list)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                # Filter by time window\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            as_path = fields.get('as-path', '').split()\n                            if as_path and as_path[-1] == target_asn:\n                                prefix = fields.get('prefix')\n                                med_value = fields.get('med')\n                                if prefix and med_value is not None:\n                                    med_records[prefix].append(med_value)\n\n    results = []\n    for prefix, med_list in med_records.items():\n        if med_list:\n            min_med = min(med_list)\n            max_med = max(med_list)\n            avg_med = sum(med_list) / len(med_list)\n            results.append({\n                'prefix': prefix,\n                'min_med': min_med,\n                'max_med': max_med,\n                'avg_med': avg_med,\n                'observations': len(med_list)\n            })\n\n    if results:\n        df_results = pd.DataFrame(results)\n        print(\"MED Variation Analysis for AS65001:\")\n        print(df_results)\n    else:\n        print(\"No MED data found for AS65001 in the specified time window.\")\n\nif __name__ == \"__main__\":\n    analyze_med_variation()\n"}], "is_classification": false}
{"id": "seed_task_20", "name": "Newly Appearing Prefixes Detection", "instruction": "Identify any newly appearing prefixes in the local BGP updates data from Sept 5 12:00 to Sept 5 13:00, 2025 by comparing against a known set of historical prefixes. Summarize the newly introduced prefixes, their origin AS, and first-seen timestamps.", "instances": [{"input": "Current Time Window: Sept 5, 2025, 12:00:00 to Sept 5, 2025, 13:00:00\nKnown Historical Prefixes File: /path/to/historical_prefixes.txt\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\n\n# Assume we have a file listing historically known prefixes\ndef load_historical_prefixes(filepath):\n    with open(filepath, 'r') as f:\n        return set(line.strip() for line in f if line.strip())\n\ndef detect_newly_appearing_prefixes():\n    known_prefixes_file = \"/path/to/historical_prefixes.txt\"\n    known_prefixes = load_historical_prefixes(known_prefixes_file)\n\n    from_time_str = \"2025-09-05 12:00:00\"\n    until_time_str = \"2025-09-05 13:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    new_prefix_data = {}\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            prefix = elem.fields.get('prefix')\n                            if prefix and prefix not in known_prefixes:\n                                # If this prefix is not known historically, record it\n                                if prefix not in new_prefix_data:\n                                    as_path_str = elem.fields.get('as-path', '')\n                                    as_path_list = as_path_str.split()\n                                    origin_as = as_path_list[-1] if as_path_list else None\n                                    new_prefix_data[prefix] = {\n                                        'origin_as': origin_as,\n                                        'first_seen': elem_dt\n                                    }\n\n    if new_prefix_data:\n        df_new = pd.DataFrame([\n            {\n                'prefix': pfx,\n                'origin_as': data['origin_as'],\n                'first_seen': data['first_seen']\n            } \n            for pfx, data in new_prefix_data.items()\n        ])\n        print(\"Newly Appearing Prefixes Detected:\")\n        print(df_new)\n    else:\n        print(\"No newly appearing prefixes were detected in the specified time window.\")\n\nif __name__ == \"__main__\":\n    detect_newly_appearing_prefixes()\n"}], "is_classification": false}
{"id": "seed_task_21", "name": "Extended Community Analysis for AS65002", "instruction": "Analyze extended BGP communities for routes originated by AS65002 from Oct 10 01:00 to Oct 10 02:00, 2025. Summarize the distinct extended communities, their frequencies, and any unusual occurrences (e.g., large or unexpected community values).", "instances": [{"input": "Time window: Oct 10, 2025, 01:00:00 to Oct 10, 2025, 02:00:00\nTarget ASN: 65002\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_extended_communities():\n    target_asn = \"65002\"\n    from_time_str = \"2025-10-10 01:00:00\"\n    until_time_str = \"2025-10-10 02:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    ext_community_counts = defaultdict(int)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            as_path = fields.get('as-path', '').split()\n                            if as_path and as_path[-1] == target_asn:\n                                # In some PyBGPStream versions, extended communities may be in 'ext-communities'\n                                extended_communities = fields.get('ext-communities', [])\n                                if extended_communities:\n                                    for ext_comm in extended_communities:\n                                        # Typically ext_comm could be a tuple or a string; handle both\n                                        if isinstance(ext_comm, tuple):\n                                            ext_comm_str = \":\".join(map(str, ext_comm))\n                                        else:\n                                            ext_comm_str = str(ext_comm)\n                                        ext_community_counts[ext_comm_str] += 1\n\n    if ext_community_counts:\n        df = pd.DataFrame(\n            [(comm, freq) for comm, freq in ext_community_counts.items()],\n            columns=[\"extended_community\", \"frequency\"]\n        ).sort_values(\"frequency\", ascending=False)\n        print(\"Extended Community Analysis for AS65002\")\n        print(df)\n    else:\n        print(\"No extended community data found for the specified time window.\")\n\nif __name__ == \"__main__\":\n    analyze_extended_communities()\n"}], "is_classification": false}
{"id": "seed_task_22", "name": "MOAS and Prefix Hijack Cross-Check", "instruction": "Detect any MOAS (Multiple Origin AS) events for prefix 203.1.2.0/24 from Nov 1 12:00 to Nov 1 13:00, 2025, and cross-check if any of these additional origins might indicate a hijack (e.g., unknown or suspicious AS not typically announcing the prefix). Summarize potential hijack incidents.", "instances": [{"input": "Time window: Nov 1, 2025, 12:00:00 to Nov 1, 2025, 13:00:00\nTarget Prefix: 203.1.2.0/24\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\n# Hypothetical known legitimate origins for the prefix\nLEGITIMATE_ORIGINS = {\"65010\", \"65020\"}\n\ndef detect_moas_and_hijack():\n    target_prefix = \"203.1.2.0/24\"\n    from_time_str = \"2025-11-01 12:00:00\"\n    until_time_str = \"2025-11-01 13:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    origins_seen = defaultdict(set)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            pfx = elem.fields.get('prefix')\n                            if pfx == target_prefix:\n                                as_path = elem.fields.get('as-path', '').split()\n                                if as_path:\n                                    origin_as = as_path[-1]\n                                    origins_seen[pfx].add(origin_as)\n\n    potential_hijacks = []\n    for pfx, origin_asns in origins_seen.items():\n        if len(origin_asns) > 1:\n            # MOAS scenario\n            for asn in origin_asns:\n                if asn not in LEGITIMATE_ORIGINS:\n                    # Potential hijack\n                    potential_hijacks.append({\n                        'prefix': pfx,\n                        'suspicious_origin_as': asn,\n                        'all_origins': list(origin_asns)\n                    })\n\n    if potential_hijacks:\n        df_hijacks = pd.DataFrame(potential_hijacks)\n        print(\"MOAS and Potential Hijack Incidents:\")\n        print(df_hijacks)\n    else:\n        print(\"No MOAS or hijack events detected for the specified time window.\")\n\nif __name__ == \"__main__\":\n    detect_moas_and_hijack()\n"}], "is_classification": false}
{"id": "seed_task_23", "name": "AS Prepending Detection for Prefix 192.0.2.0/24", "instruction": "Detect instances of AS path prepending in announcements for prefix 192.0.2.0/24 from Dec 20 00:00 to Dec 20 02:00, 2025. Summarize the number of times an AS repeats consecutively and how frequently it occurs in the observed data.", "instances": [{"input": "Time window: Dec 20, 2025, 00:00:00 to Dec 20, 2025, 02:00:00\nTarget Prefix: 192.0.2.0/24\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef detect_as_prepending():\n    prefix_of_interest = \"192.0.2.0/24\"\n    from_time_str = \"2025-12-20 00:00:00\"\n    until_time_str = \"2025-12-20 02:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    prepend_counts = defaultdict(int)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        ts = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if ts < from_time or ts > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n\n                            if prefix == prefix_of_interest:\n                                as_path_str = fields.get('as-path', '')\n                                as_path_list = as_path_str.split()\n                                if len(as_path_list) > 1:\n                                    # Detect consecutive repeats in the AS path\n                                    consecutive_count = 1\n                                    for i in range(1, len(as_path_list)):\n                                        if as_path_list[i] == as_path_list[i-1]:\n                                            consecutive_count += 1\n                                        else:\n                                            if consecutive_count > 1:\n                                                # We found a run of repeated ASNs\n                                                run_asn = as_path_list[i-1]\n                                                prepend_counts[run_asn] += 1\n                                            consecutive_count = 1\n                                    # Check the last run\n                                    if consecutive_count > 1:\n                                        run_asn = as_path_list[-1]\n                                        prepend_counts[run_asn] += 1\n\n    if prepend_counts:\n        df_prepends = pd.DataFrame(\n            [(asn, count) for asn, count in prepend_counts.items()],\n            columns=[\"asn\", \"consecutive_repeat_occurrences\"]\n        ).sort_values(by=\"consecutive_repeat_occurrences\", ascending=False)\n\n        print(\"AS Prepending Analysis for 192.0.2.0/24\")\n        print(df_prepends)\n    else:\n        print(\"No AS path prepending detected for prefix 192.0.2.0/24 in the given time window.\")\n\nif __name__ == \"__main__\":\n    detect_as_prepending()\n"}], "is_classification": false}
{"id": "seed_task_24", "name": "Origin AS Change Tracking for Prefix 203.0.113.0/24", "instruction": "Track changes in the origin AS for prefix 203.0.113.0/24 from Nov 10 10:00 to Nov 10 11:00, 2025. Report any occurrences where the origin AS differs from the previously seen origin, indicating an origin change or possible hijack.", "instances": [{"input": "Time window: Nov 10, 2025, 10:00:00 to Nov 10, 2025, 11:00:00\nTarget Prefix: 203.0.113.0/24\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\n\ndef track_origin_as_changes():\n    prefix_of_interest = \"203.0.113.0/24\"\n    from_time_str = \"2025-11-10 10:00:00\"\n    until_time_str = \"2025-11-10 11:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    origin_changes = []\n    last_origin_as = None\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        ts = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if ts < from_time or ts > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n                            if prefix == prefix_of_interest:\n                                as_path_str = fields.get('as-path', '')\n                                as_path_list = as_path_str.split()\n                                if as_path_list:\n                                    current_origin_as = as_path_list[-1]\n                                    if last_origin_as is None:\n                                        last_origin_as = current_origin_as\n                                    elif current_origin_as != last_origin_as:\n                                        origin_changes.append({\n                                            'timestamp': ts,\n                                            'old_origin_as': last_origin_as,\n                                            'new_origin_as': current_origin_as\n                                        })\n                                        last_origin_as = current_origin_as\n\n    if origin_changes:\n        df_changes = pd.DataFrame(origin_changes)\n        print(\"Origin AS Changes Detected for 203.0.113.0/24:\")\n        print(df_changes)\n    else:\n        print(\"No changes in origin AS detected for the given time window.\")\n\nif __name__ == \"__main__\":\n    track_origin_as_changes()\n"}], "is_classification": false}
{"id": "seed_task_25", "name": "Local Preference Changes for AS65501", "instruction": "Analyze changes in the LOCAL_PREF (Local Preference) attribute for routes advertised by AS65501 from Jan 2 08:00 to Jan 2 09:00, 2026. Track distinct LOCAL_PREF values and how often they change for each prefix.", "instances": [{"input": "Time window: Jan 2, 2026, 08:00:00 to Jan 2, 2026, 09:00:00\nTarget ASN: 65501\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_local_pref_changes():\n    target_asn = \"65501\"\n    from_time_str = \"2026-01-02 08:00:00\"\n    until_time_str = \"2026-01-02 09:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    # Dictionary: prefix -> set of observed local_prefs\n    local_pref_data = defaultdict(set)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        ts = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if ts < from_time or ts > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            as_path = fields.get('as-path', '').split()\n                            if as_path and as_path[-1] == target_asn:\n                                prefix = fields.get('prefix')\n                                local_pref = fields.get('local-pref')  # depends on PyBGPStream version\n                                if prefix and local_pref is not None:\n                                    local_pref_data[prefix].add(local_pref)\n\n    results = []\n    for pfx, prefs in local_pref_data.items():\n        results.append({\n            'prefix': pfx,\n            'distinct_local_prefs': list(prefs),\n            'changes_count': len(prefs)\n        })\n\n    if results:\n        df_results = pd.DataFrame(results)\n        print(\"Local Preference Changes for AS65501:\")\n        print(df_results)\n    else:\n        print(\"No LOCAL_PREF data found for AS65501 in the specified time window.\")\n\nif __name__ == \"__main__\":\n    analyze_local_pref_changes()\n"}], "is_classification": false}
{"id": "seed_task_26", "name": "AS Path Loop Detection for Prefix 8.8.0.0/16", "instruction": "Detect any loops in the AS path for prefix 8.8.0.0/16 from Feb 1 00:00 to Feb 1 02:00, 2026. An AS path loop occurs if the same ASN appears multiple times non-consecutively within the path. Summarize each path and the loop encountered.", "instances": [{"input": "Time window: Feb 1, 2026, 00:00:00 to Feb 1, 2026, 02:00:00\nTarget Prefix: 8.8.0.0/16\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\n\n\ndef detect_as_path_loops():\n    target_prefix = \"8.8.0.0/16\"\n    from_time_str = \"2026-02-01 00:00:00\"\n    until_time_str = \"2026-02-01 02:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    loop_records = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                file_date_str = match.group(1)\n                file_time_str = match.group(2)\n                file_ts_str = file_date_str + file_time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        ts = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if ts < from_time or ts > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n\n                            if prefix == target_prefix:\n                                as_path_str = fields.get('as-path', '')\n                                as_path_list = as_path_str.split()\n\n                                visited = set()\n                                loop_as = None\n                                for asn in as_path_list:\n                                    if asn in visited:\n                                        loop_as = asn\n                                        break\n                                    visited.add(asn)\n                                \n                                if loop_as:\n                                    loop_records.append({\n                                        'timestamp': ts,\n                                        'as_path': as_path_str,\n                                        'loop_asn': loop_as\n                                    })\n\n    if loop_records:\n        df_loops = pd.DataFrame(loop_records)\n        print(\"AS Path Loop Detected for 8.8.0.0/16:\")\n        print(df_loops)\n    else:\n        print(\"No AS path loops were detected for the target prefix in the given time window.\")\n\nif __name__ == \"__main__\":\n    detect_as_path_loops()\n"}], "is_classification": false}
{"id": "seed_task_27", "name": "Top 10 Origin ASNs by Unique Prefix Count", "instruction": "Calculate the top 10 ASNs by the number of unique prefixes they originate in the local BGP updates from March 5 00:00 to March 5 01:00, 2026. Summarize each ASN and the count of unique prefixes.", "instances": [{"input": "Time window: March 5, 2026, 00:00:00 to March 5, 2026, 01:00:00\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef find_top_10_asn_by_prefix_count():\n    from_time_str = \"2026-03-05 00:00:00\"\n    until_time_str = \"2026-03-05 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    # Dictionary: origin_as -> set of prefixes\n    asn_prefixes = defaultdict(set)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        ts = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if ts < from_time or ts > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n                            as_path_str = fields.get('as-path', '')\n                            as_path_list = as_path_str.split()\n                            if as_path_list:\n                                origin_as = as_path_list[-1]\n                                if prefix:\n                                    asn_prefixes[origin_as].add(prefix)\n\n    # Compute the number of unique prefixes for each origin ASN\n    summary_list = []\n    for origin_as, pfx_set in asn_prefixes.items():\n        summary_list.append((origin_as, len(pfx_set)))\n\n    df_summary = pd.DataFrame(summary_list, columns=[\"origin_as\", \"unique_prefix_count\"])\n    df_summary_sorted = df_summary.sort_values(by=\"unique_prefix_count\", ascending=False)\n    top_10 = df_summary_sorted.head(10)\n\n    print(\"Top 10 Origin ASNs by Unique Prefix Count\")\n    print(top_10)\n\nif __name__ == \"__main__\":\n    find_top_10_asn_by_prefix_count()\n"}], "is_classification": false}
{"id": "seed_task_28", "name": "Subprefix Deaggregation Analysis", "instruction": "Analyze deaggregation for any subprefixes of 10.1.0.0/16 in the local BGP updates from May 1 10:00 to May 1 11:00, 2026. Identify newly announced longer subprefixes and compare their origin AS to the origin of the parent prefix.", "instances": [{"input": "Time window: May 1, 2026, 10:00:00 to May 1, 2026, 11:00:00\nParent Prefix: 10.1.0.0/16\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport ipaddress\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_subprefix_deaggregation():\n    parent_prefix_str = \"10.1.0.0/16\"\n    from_time_str = \"2026-05-01 10:00:00\"\n    until_time_str = \"2026-05-01 11:00:00\"\n\n    parent_network = ipaddress.ip_network(parent_prefix_str)\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    # Track origin of parent prefix, if seen\n    parent_origin_as = None\n    # Track subprefix origins\n    subprefix_origins = defaultdict(set)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        ts = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if ts < from_time or ts > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            prefix_str = elem.fields.get('prefix')\n                            as_path_str = elem.fields.get('as-path', '')\n                            as_path_list = as_path_str.split()\n\n                            if prefix_str and as_path_list:\n                                try:\n                                    announced_net = ipaddress.ip_network(prefix_str)\n                                except ValueError:\n                                    continue\n\n                                origin_as = as_path_list[-1]\n\n                                # Check if it's the parent prefix\n                                if announced_net == parent_network:\n                                    parent_origin_as = origin_as\n                                else:\n                                    # Check if it's a subprefix of the parent\n                                    if announced_net.subnet_of(parent_network) and announced_net != parent_network:\n                                        subprefix_origins[prefix_str].add(origin_as)\n\n    # Summarize subprefix data\n    results = []\n    for spfx, origins in subprefix_origins.items():\n        results.append({\n            'subprefix': spfx,\n            'announced_origins': list(origins),\n            'matches_parent_origin': (parent_origin_as in origins) if parent_origin_as else False\n        })\n\n    df_results = pd.DataFrame(results)\n    if not df_results.empty:\n        print(\"Subprefix Deaggregation Analysis for 10.1.0.0/16\")\n        print(df_results)\n    else:\n        print(\"No subprefix deaggregation detected in the given time window.\")\n\nif __name__ == \"__main__\":\n    analyze_subprefix_deaggregation()\n"}], "is_classification": false}
{"id": "seed_task_29", "name": "Well-Known BGP Communities Usage Analysis", "instruction": "Analyze usage of well-known BGP communities (e.g., NO_EXPORT, NO_ADVERTISE, GRACEFUL_SHUTDOWN, etc.) for any announcements from AS66666 between June 1 00:00 and June 1 01:00, 2026. Summarize each well-known community and its frequency.", "instances": [{"input": "Time window: June 1, 2026, 00:00:00 to June 1, 2026, 01:00:00\nTarget ASN: 66666\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\n# Define a mapping of well-known communities for easy reference\nWELL_KNOWN_COMMUNITIES = {\n    (65535, 65281): \"NO_EXPORT\",       # 65535:65281\n    (65535, 65282): \"NO_ADVERTISE\",    # 65535:65282\n    (65535, 65283): \"NO_EXPORT_SUBCONFED\",\n    (65535, 65284): \"NO_PEER\",\n    # Additional well-known communities can be included here as needed\n}\n\ndef analyze_well_known_communities():\n    target_asn = \"66666\"\n    from_time_str = \"2026-06-01 00:00:00\"\n    until_time_str = \"2026-06-01 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    community_counts = defaultdict(int)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            as_path = elem.fields.get('as-path', '').split()\n                            if as_path and as_path[-1] == target_asn:\n                                # Look for communities in the announcement\n                                communities = elem.fields.get('communities', [])\n                                # communities is generally a list of (asn, value) pairs\n                                for comm in communities:\n                                    if tuple(comm) in WELL_KNOWN_COMMUNITIES:\n                                        community_counts[WELL_KNOWN_COMMUNITIES[tuple(comm)]] += 1\n\n    if community_counts:\n        df_communities = pd.DataFrame(\n            [(comm, count) for comm, count in community_counts.items()],\n            columns=[\"well_known_community\", \"frequency\"]\n        ).sort_values(by=\"frequency\", ascending=False)\n        print(\"Well-Known BGP Communities Usage:\")\n        print(df_communities)\n    else:\n        print(\"No well-known BGP communities detected for the specified time window.\")\n\nif __name__ == \"__main__\":\n    analyze_well_known_communities()\n"}], "is_classification": false}
{"id": "seed_task_30", "name": "Community String Policy Enforcement Check", "instruction": "Check if certain BGP community strings (e.g., 65000:100, 65000:200) are properly enforced on inbound routes for AS65535 between July 10 09:00 and July 10 10:00, 2026. Log occurrences and any violation where the community is present but should not be.", "instances": [{"input": "Time window: July 10, 2026, 09:00:00 to July 10, 2026, 10:00:00\nTarget ASN: 65535\nEnforced Communities: [65000:100, 65000:200]\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\n# Hypothetical rules: communities 65000:100 and 65000:200 must be present on certain routes or disallowed\n# This script only detects presence, actual policy logic can vary\nENFORCED_COMMUNITIES = {\"65000:100\", \"65000:200\"}\n\n\ndef check_community_enforcement():\n    target_asn = \"65535\"\n    from_time_str = \"2026-07-10 09:00:00\"\n    until_time_str = \"2026-07-10 10:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    # Track compliance\n    compliance_log = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            as_path_list = fields.get('as-path', '').split()\n                            if as_path_list and as_path_list[-1] == target_asn:\n                                prefix = fields.get('prefix')\n                                communities = fields.get('communities', [])\n                                # Convert (asn, val) pairs to \"asn:val\" strings\n                                comm_str_set = set(f\"{c[0]}:{c[1]}\" for c in communities)\n                                # Check for presence/absence of enforced communities\n                                present_comms = ENFORCED_COMMUNITIES.intersection(comm_str_set)\n                                missing_comms = ENFORCED_COMMUNITIES - comm_str_set\n\n                                compliance_log.append({\n                                    'timestamp': elem_dt,\n                                    'prefix': prefix,\n                                    'present_enforced_communities': list(present_comms),\n                                    'missing_enforced_communities': list(missing_comms),\n                                })\n\n    df_compliance = pd.DataFrame(compliance_log)\n    if not df_compliance.empty:\n        print(\"Community Enforcement Check Results:\")\n        print(df_compliance)\n    else:\n        print(\"No announcements found for the specified time window and ASN.\")\n\nif __name__ == \"__main__\":\n    check_community_enforcement()\n"}], "is_classification": false}
{"id": "seed_task_31", "name": "Route Dampening Trigger Analysis for Prefix 198.51.100.0/24", "instruction": "Determine if prefix 198.51.100.0/24 triggered any route dampening penalties due to excessive flaps from Aug 15 12:00 to Aug 15 13:00, 2026. Summarize the number of flaps detected and any observed penalty thresholds crossed.", "instances": [{"input": "Time window: Aug 15, 2026, 12:00:00 to Aug 15, 2026, 13:00:00\nTarget Prefix: 198.51.100.0/24\nRecord Type: updates (Announcements and Withdrawals)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone, timedelta\n\n# Hypothetical dampening config\nMAX_PENALTY = 2000  # Example maximum penalty before route suppressed\nFLAP_PENALTY = 100   # Penalty each time a flap occurs\nDECAY_INTERVAL = 5   # 5-minute half-life decay\n\n\ndef compute_decay(penalty, elapsed_minutes):\n    # Simple exponential decay function for demonstration\n    # Real damping is more complex, but we approximate here\n    half_life_periods = elapsed_minutes / DECAY_INTERVAL\n    return penalty * (0.5 ** half_life_periods)\n\n\ndef analyze_route_dampening():\n    target_prefix = \"198.51.100.0/24\"\n    from_time_str = \"2026-08-15 12:00:00\"\n    until_time_str = \"2026-08-15 13:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    # Track events (announcements or withdrawals)\n    events = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts = date_str + time_str\n                file_dt = datetime.strptime(file_ts, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.fields.get('prefix') == target_prefix:\n                            if elem.type in ['A', 'W']:\n                                events.append((elem_dt, elem.type))\n\n    events.sort(key=lambda x: x[0])  # Sort by timestamp\n\n    if not events:\n        print(\"No flaps detected for this prefix in the given time window.\")\n        return\n\n    # Simulate damping\n    penalty = 0.0\n    last_event_time = events[0][0]\n    dampening_log = []\n\n    for i, (ts, ev_type) in enumerate(events):\n        # Decay penalty based on time since last event\n        elapsed_minutes = (ts - last_event_time).total_seconds() / 60.0\n        penalty = compute_decay(penalty, elapsed_minutes)\n        # Apply penalty for flap (announcement or withdrawal both count as a flap in this example)\n        penalty += FLAP_PENALTY\n\n        # Record data\n        dampening_log.append({\n            'timestamp': ts,\n            'event_type': ev_type,\n            'post_event_penalty': penalty,\n            'route_suppressed': penalty > MAX_PENALTY\n        })\n\n        last_event_time = ts\n\n    df_dampening = pd.DataFrame(dampening_log)\n    print(\"Route Dampening Analysis for 198.51.100.0/24\")\n    print(df_dampening)\n\n    suppressed_events = df_dampening[df_dampening['route_suppressed']]\n    if not suppressed_events.empty:\n        print(\"\\nRoute was suppressed during the following timestamps:\")\n        print(suppressed_events)\n    else:\n        print(\"\\nNo suppression threshold was exceeded in the given time window.\")\n\nif __name__ == \"__main__\":\n    analyze_route_dampening()\n"}], "is_classification": false}
{"id": "seed_task_32", "name": "IPv6 Route Coverage Analysis", "instruction": "Examine the presence and coverage of IPv6 prefixes in the local BGP updates from Sept 1 00:00 to Sept 1 02:00, 2026. Compare the number of unique IPv6 routes and origin ASNs to a reference list of expected IPv6 routes. Summarize missing or newly present routes.", "instances": [{"input": "Time window: Sept 1, 2026, 00:00:00 to Sept 1, 2026, 02:00:00\nReference IPv6 Route List: /path/to/reference_ipv6_routes.txt\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nimport ipaddress\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\n# Load a reference list of IPv6 routes we expect\ndef load_reference_ipv6_routes(filepath):\n    with open(filepath, 'r') as f:\n        return set(line.strip() for line in f if line.strip())\n\ndef analyze_ipv6_route_coverage():\n    reference_file = \"/path/to/reference_ipv6_routes.txt\"\n    reference_routes = load_reference_ipv6_routes(reference_file)\n\n    from_time_str = \"2026-09-01 00:00:00\"\n    until_time_str = \"2026-09-01 02:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    observed_prefixes = set()\n    observed_origins = defaultdict(set)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            prefix = elem.fields.get('prefix')\n                            if prefix:\n                                # Check if this is an IPv6 prefix\n                                try:\n                                    net = ipaddress.ip_network(prefix)\n                                    if net.version == 6:\n                                        observed_prefixes.add(prefix)\n                                        as_path_str = elem.fields.get('as-path', '')\n                                        as_path_list = as_path_str.split()\n                                        if as_path_list:\n                                            origin_as = as_path_list[-1]\n                                            observed_origins[prefix].add(origin_as)\n                                except ValueError:\n                                    # Not a valid prefix\n                                    continue\n\n    # Compare observed prefixes to reference\n    missing_from_observed = reference_routes - observed_prefixes\n    newly_seen = observed_prefixes - reference_routes\n\n    print(\"IPv6 Route Coverage Analysis\")\n    print(\"--------------------------------\")\n    print(f\"Total IPv6 prefixes observed: {len(observed_prefixes)}\")\n    print(f\"Total IPv6 prefixes in reference: {len(reference_routes)}\")\n\n    if missing_from_observed:\n        print(\"\\nMissing routes from observation:\")\n        for route in missing_from_observed:\n            print(route)\n    else:\n        print(\"\\nNo missing routes from reference.\")\n\n    if newly_seen:\n        print(\"\\nNewly seen routes not in reference:\")\n        for route in newly_seen:\n            print(route)\n    else:\n        print(\"\\nNo newly seen routes.\")\n\n    # Print some basic origin stats\n    all_origins = set()\n    for origs in observed_origins.values():\n        all_origins.update(origs)\n\n    print(\"\\nUnique origin ASNs for observed IPv6 routes:\")\n    print(len(all_origins))\n\nif __name__ == \"__main__\":\n    analyze_ipv6_route_coverage()\n"}], "is_classification": false}
{"id": "seed_task_33", "name": "Next-Hop Variation Analysis for Prefix 1.1.1.0/24", "instruction": "Analyze the next-hop address variations in BGP updates for prefix 1.1.1.0/24 from Oct 1 00:00 to Oct 1 01:00, 2026. Summarize each distinct next-hop IP and how frequently it appeared in announcements.", "instances": [{"input": "Time window: Oct 1, 2026, 00:00:00 to Oct 1, 2026, 01:00:00\nTarget Prefix: 1.1.1.0/24\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_next_hop_variations():\n    prefix_of_interest = \"1.1.1.0/24\"\n    from_time_str = \"2026-10-01 00:00:00\"\n    until_time_str = \"2026-10-01 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    # Dictionary to track next-hop counts\n    next_hop_counts = defaultdict(int)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n\n                # Set up BGPStream\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                # Process each record\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n                            if prefix == prefix_of_interest:\n                                next_hop = fields.get('next-hop')\n                                if next_hop:\n                                    next_hop_counts[next_hop] += 1\n\n    if next_hop_counts:\n        df = pd.DataFrame(\n            [(ip, count) for ip, count in next_hop_counts.items()],\n            columns=[\"next_hop\", \"occurrences\"]\n        ).sort_values(\"occurrences\", ascending=False)\n        print(\"Next-Hop Variation Analysis for 1.1.1.0/24\")\n        print(df)\n    else:\n        print(\"No next-hop variations found for 1.1.1.0/24 in the specified time window.\")\n\nif __name__ == \"__main__\":\n    analyze_next_hop_variations()\n"}], "is_classification": false}
{"id": "seed_task_34", "name": "Presence of Private or Reserved ASNs in AS Path", "instruction": "Check for any private (64512–65534, 4200000000–4294967294) or reserved ASNs in the AS paths of announcements from Jan 3 12:00 to Jan 3 13:00, 2027. Summarize the prefixes and AS paths where such ASNs appear.", "instances": [{"input": "Time window: Jan 3, 2027, 12:00:00 to Jan 3, 2027, 13:00:00\nPrivate ASN Ranges: [64512–65534, 4200000000–4294967294]\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\n\n# Helper function to check if an ASN is in the private/reserved range\nPRIVATE_RANGES = [\n    (64512, 65534),\n    (4200000000, 4294967294)\n]\n\ndef is_private_asn(asn_str):\n    # Attempt integer conversion\n    try:\n        asn = int(asn_str)\n    except ValueError:\n        return False\n    for lower, upper in PRIVATE_RANGES:\n        if lower <= asn <= upper:\n            return True\n    return False\n\n\ndef detect_private_asn_presence():\n    from_time_str = \"2027-01-03 12:00:00\"\n    until_time_str = \"2027-01-03 13:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    suspicious_paths = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            prefix = elem.fields.get('prefix')\n                            as_path_str = elem.fields.get('as-path', '')\n                            as_path_list = as_path_str.split()\n\n                            # Check each ASN in the path\n                            if any(is_private_asn(asn) for asn in as_path_list):\n                                suspicious_paths.append({\n                                    'timestamp': elem_dt,\n                                    'prefix': prefix,\n                                    'as_path': as_path_str\n                                })\n\n    if suspicious_paths:\n        df_suspicious = pd.DataFrame(suspicious_paths)\n        print(\"Private/Reserved ASNs Detected in AS Paths:\")\n        print(df_suspicious)\n    else:\n        print(\"No private/reserved ASNs found in the given time window.\")\n\nif __name__ == \"__main__\":\n    detect_private_asn_presence()\n"}], "is_classification": false}
{"id": "seed_task_35", "name": "Peer Session State Changes Detection", "instruction": "Identify and log BGP peer session up/down events for any monitored peers within the time window Mar 2 00:00 to Mar 2 02:00, 2027. Summarize each session's state changes and the timestamps of these events.", "instances": [{"input": "Time window: Mar 2, 2027, 00:00:00 to Mar 2, 2027, 02:00:00\nMonitored Peers: All peers present in local updates\nRecord Type: updates (Announcements, Withdrawals, and State Changes if available)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\n\n# Note: The detection of peer up/down events typically depends on the collector data.\n# Some data feeds may provide BGP state change records. If not, this logic might rely\n# on absent announcements or special fields. We'll assume state change support.\n\ndef detect_peer_session_changes():\n    from_time_str = \"2027-03-02 00:00:00\"\n    until_time_str = \"2027-03-02 02:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    events = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    # If the collector or PyBGPStream version exposes peer state changes:\n                    if rec.type == 'state':\n                        peer_address = rec.collector  # or rec.project/rec.collector, depending on config\n                        state = rec.state  # The peer state: maybe 'up', 'down', etc.\n                        rec_time = datetime.utcfromtimestamp(rec.time).replace(tzinfo=timezone.utc)\n\n                        if from_time <= rec_time <= until_time:\n                            events.append({\n                                'timestamp': rec_time,\n                                'peer': peer_address,\n                                'new_state': state\n                            })\n\n    if events:\n        df_events = pd.DataFrame(events)\n        df_events.sort_values(by='timestamp', inplace=True)\n        print(\"BGP Peer Session State Changes:\")\n        print(df_events)\n    else:\n        print(\"No peer session state changes detected in the given time window.\")\n\nif __name__ == \"__main__\":\n    detect_peer_session_changes()\n"}], "is_classification": false}
{"id": "seed_task_36", "name": "Longitudinal Prefix Growth Analysis", "instruction": "Examine the growth in the number of announced IPv4 prefixes over daily intervals from Jan 1 2026 to Jan 10 2026. Summarize the total prefix counts per day and highlight days with abnormal growth or decline.", "instances": [{"input": "Time window: Jan 1, 2026, 00:00:00 to Jan 10, 2026, 23:59:59\nRecord Type: updates (Announcements)\nAnalyze daily prefix counts", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timedelta, timezone\nfrom collections import defaultdict\n\n\ndef analyze_prefix_growth():\n    start_date_str = \"2026-01-01 00:00:00\"\n    end_date_str = \"2026-01-10 23:59:59\"\n\n    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    end_date = datetime.strptime(end_date_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    # We'll track day -> set of prefixes to count unique daily prefix announcements\n    daily_prefixes = defaultdict(set)\n\n    # Generate a list of days to check\n    current_day = start_date\n    while current_day <= end_date:\n        day_str = current_day.strftime(\"%Y%m%d\")\n        daily_prefixes[day_str] = set()\n        current_day += timedelta(days=1)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)  # e.g., 20260101\n                time_str = match.group(2)  # e.g., 1200\n\n                # Check if date_str is in our range\n                file_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_dt < start_date or file_dt > end_date:\n                    continue\n\n                file_path = os.path.join(root, file)\n\n                # The day string from the file\n                day_key = date_str  # e.g., 20260101\n                if day_key not in daily_prefixes:\n                    daily_prefixes[day_key] = set()\n\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if start_date <= elem_dt <= end_date and elem.type == 'A':\n                            prefix = elem.fields.get('prefix')\n                            if prefix:\n                                daily_prefixes[day_key].add(prefix)\n\n    # Build a summary\n    summary = []\n    for day_key in sorted(daily_prefixes.keys()):\n        summary.append({\n            'date': day_key,\n            'prefix_count': len(daily_prefixes[day_key])\n        })\n\n    df_summary = pd.DataFrame(summary)\n    print(\"Daily IPv4 Prefix Growth Analysis\")\n    print(df_summary)\n\n    # Simple detection of abnormal growth: e.g., a day with 20% difference from previous day\n    df_summary['prefix_count'] = df_summary['prefix_count'].astype(int)\n    df_summary['pct_change'] = df_summary['prefix_count'].pct_change() * 100\n\n    abnormal = df_summary[ (df_summary['pct_change'] > 20) | (df_summary['pct_change'] < -20) ]\n    if not abnormal.empty:\n        print(\"\\nDays with abnormal growth or decline (> 20%):\")\n        print(abnormal)\n    else:\n        print(\"\\nNo abnormal growth or decline detected.\")\n\nif __name__ == \"__main__\":\n    analyze_prefix_growth()\n"}], "is_classification": false}
{"id": "seed_task_37", "name": "Subprefix Deaggregation Detection for 10.1.0.0/16", "instruction": "Identify any newly announced subprefixes of 10.1.0.0/16 from Jan 1 00:00 to Jan 1 01:00, 2027 and check if their origin AS differs from that of the parent prefix.", "instances": [{"input": "Time window: Jan 1, 2027, 00:00:00 to Jan 1, 2027, 01:00:00\nParent Prefix: 10.1.0.0/16\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport ipaddress\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef detect_subprefix_deaggregation():\n    parent_prefix_str = \"10.1.0.0/16\"\n    from_time_str = \"2027-01-01 00:00:00\"\n    until_time_str = \"2027-01-01 01:00:00\"\n\n    parent_network = ipaddress.ip_network(parent_prefix_str)\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    # Track the parent prefix origin (if announced)\n    parent_origin_as = None\n    # Track subprefix origins\n    subprefix_origins = defaultdict(set)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            prefix_str = elem.fields.get('prefix')\n                            as_path_str = elem.fields.get('as-path', '')\n                            as_path_list = as_path_str.split()\n\n                            if prefix_str and as_path_list:\n                                try:\n                                    announced_net = ipaddress.ip_network(prefix_str)\n                                except ValueError:\n                                    continue\n\n                                origin_as = as_path_list[-1]\n\n                                # Check if it's the parent prefix itself\n                                if announced_net == parent_network:\n                                    parent_origin_as = origin_as\n                                else:\n                                    # Check if it's a subprefix of the parent\n                                    if announced_net.subnet_of(parent_network) and announced_net != parent_network:\n                                        subprefix_origins[prefix_str].add(origin_as)\n\n    results = []\n    for spfx, origins in subprefix_origins.items():\n        results.append({\n            'subprefix': spfx,\n            'origin_as_set': list(origins),\n            'matches_parent_origin': (parent_origin_as in origins) if parent_origin_as else False\n        })\n\n    if results:\n        df_results = pd.DataFrame(results)\n        print(\"Subprefix Deaggregation for 10.1.0.0/16\")\n        print(df_results)\n    else:\n        print(\"No subprefix deaggregation detected in the specified time window.\")\n\nif __name__ == \"__main__\":\n    detect_subprefix_deaggregation()\n"}], "is_classification": false}
{"id": "seed_task_38", "name": "Blackhole Community Usage Analysis", "instruction": "Identify usage of blackhole communities (e.g., 65535:666) in announcements from March 1 00:00 to March 1 01:00, 2027. Summarize each prefix announcing these communities, the frequency of appearances, and the origin AS.", "instances": [{"input": "Time window: March 1, 2027, 00:00:00 to March 1, 2027, 01:00:00\nBlackhole Community Example: 65535:666\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\n# Example blackhole community to detect\nBLACKHOLE_COMM = \"65535:666\"\n\n# Depending on PyBGPStream version, 'communities' can be a list of tuples or list of strings\n\ndef detect_blackhole_community_usage():\n    from_time_str = \"2027-03-01 00:00:00\"\n    until_time_str = \"2027-03-01 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    blackhole_usage = defaultdict(int)\n    blackhole_origins = defaultdict(set)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt_str = date_str + time_str\n                file_dt = datetime.strptime(file_dt_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            communities = fields.get('communities', [])\n\n                            # Convert each community tuple to the \"asn:value\" format if needed\n                            # or check if the string matches BLACKHOLE_COMM\n                            comm_strs = set()\n                            for comm in communities:\n                                if isinstance(comm, tuple):\n                                    comm_strs.add(f\"{comm[0]}:{comm[1]}\")\n                                else:\n                                    comm_strs.add(str(comm))\n\n                            if BLACKHOLE_COMM in comm_strs:\n                                prefix = fields.get('prefix')\n                                as_path = fields.get('as-path', '').split()\n                                origin_as = as_path[-1] if as_path else None\n                                blackhole_usage[prefix] += 1\n                                if origin_as:\n                                    blackhole_origins[prefix].add(origin_as)\n\n    if blackhole_usage:\n        results = []\n        for pfx, count in blackhole_usage.items():\n            origins = list(blackhole_origins[pfx])\n            results.append({\n                'prefix': pfx,\n                'announce_count': count,\n                'origin_as': origins\n            })\n        df_results = pd.DataFrame(results)\n        print(\"Blackhole Community Usage Analysis:\")\n        print(df_results)\n    else:\n        print(\"No announcements containing the blackhole community were detected in the specified time window.\")\n\nif __name__ == \"__main__\":\n    detect_blackhole_community_usage()\n"}], "is_classification": false}
{"id": "seed_task_39", "name": "Graceful Shutdown Community Usage for AS65500", "instruction": "Analyze usage of the Graceful Shutdown community (commonly 65535:0 or similar) for routes originated by AS65500 from June 1 00:00 to June 1 01:00, 2027. Summarize each prefix announcing this community, the origin AS, and occurrence counts.", "instances": [{"input": "Time window: June 1, 2027, 00:00:00 to June 1, 2027, 01:00:00\nTarget ASN: 65500\nGraceful Shutdown Community (example): 65535:0\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\nGRACEFUL_SHUTDOWN_COMM = \"65535:0\"  # Example of a common GS community\n\ndef analyze_graceful_shutdown_usage():\n    target_asn = \"65500\"\n    from_time_str = \"2027-06-01 00:00:00\"\n    until_time_str = \"2027-06-01 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    comm_usage_counts = defaultdict(int)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt_str = date_str + time_str\n                file_dt = datetime.strptime(file_dt_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            as_path_list = fields.get('as-path', '').split()\n                            if as_path_list and as_path_list[-1] == target_asn:\n                                communities = fields.get('communities', [])\n                                comm_strs = set()\n                                for comm in communities:\n                                    if isinstance(comm, tuple):\n                                        comm_strs.add(f\"{comm[0]}:{comm[1]}\")\n                                    else:\n                                        comm_strs.add(str(comm))\n\n                                if GRACEFUL_SHUTDOWN_COMM in comm_strs:\n                                    prefix = fields.get('prefix')\n                                    comm_usage_counts[prefix] += 1\n\n    if comm_usage_counts:\n        results = []\n        for pfx, count in comm_usage_counts.items():\n            results.append({\n                'prefix': pfx,\n                'gs_community_count': count\n            })\n        df = pd.DataFrame(results)\n        print(\"Graceful Shutdown Community Usage for AS65500:\")\n        print(df)\n    else:\n        print(\"No announcements using Graceful Shutdown community found for AS65500 in the given window.\")\n\nif __name__ == \"__main__\":\n    analyze_graceful_shutdown_usage()\n"}], "is_classification": false}
{"id": "seed_task_40", "name": "BGP Update Storm Detection for 203.0.113.0/24", "instruction": "Identify potential update storms for prefix 203.0.113.0/24 from July 1 10:00 to July 1 10:15, 2027 by counting consecutive announcements and withdrawals. Mark intervals with more than 50 updates as an update storm.", "instances": [{"input": "Time window: July 1, 2027, 10:00:00 to July 1, 2027, 10:15:00\nTarget Prefix: 203.0.113.0/24\nThreshold: > 50 updates in 5-minute intervals\nRecord Type: updates (Announcements and Withdrawals)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone, timedelta\nfrom collections import defaultdict\n\ndef detect_update_storm():\n    target_prefix = \"203.0.113.0/24\"\n    from_time_str = \"2027-07-01 10:00:00\"\n    until_time_str = \"2027-07-01 10:15:00\"\n    interval_minutes = 5\n    threshold = 50\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    intervals = []\n    current_start = from_time\n    while current_start < until_time:\n        interval_end = current_start + timedelta(minutes=interval_minutes)\n        intervals.append((current_start, interval_end))\n        current_start = interval_end\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    interval_counts = [0] * len(intervals)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.fields.get('prefix') == target_prefix and elem.type in ['A', 'W']:\n                            for i, (start_i, end_i) in enumerate(intervals):\n                                if start_i <= elem_dt < end_i:\n                                    interval_counts[i] += 1\n                                    break\n\n    # Summarize results\n    results = []\n    for i, (start_i, end_i) in enumerate(intervals):\n        updates_count = interval_counts[i]\n        storm_detected = updates_count > threshold\n        results.append({\n            'interval': f\"{start_i.strftime('%Y-%m-%d %H:%M')} - {end_i.strftime('%H:%M')}\",\n            'updates_count': updates_count,\n            'update_storm': storm_detected\n        })\n\n    df_results = pd.DataFrame(results)\n    print(\"BGP Update Storm Detection for 203.0.113.0/24\")\n    print(df_results)\n\nif __name__ == \"__main__\":\n    detect_update_storm()\n"}], "is_classification": false}
{"id": "seed_task_41", "name": "Potential RPKI Invalid Announcements for AS64496", "instruction": "Check for potential RPKI-invalid announcements originating from AS64496 between Aug 10 00:00 and Aug 10 01:00, 2027. Summarize each prefix flagged as invalid, its origin AS, and the reason for invalidity (e.g., max length exceeded).", "instances": [{"input": "Time window: Aug 10, 2027, 00:00:00 to Aug 10, 2027, 01:00:00\nTarget ASN: 64496\nRecord Type: updates (Announcements)\nValidation Source: Local ROA data or external validator", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\n\n# Hypothetical function to check RPKI validity\n# In reality, an external validator is often used, or a local ROA database.\nROA_DB = {\n    # Example: 'prefix': (max_length, valid_origin_asn)\n    '203.0.113.0/24': (24, '64496'),\n    '198.51.100.0/24': (24, '64497')\n}\n\ndef rpki_check(prefix, origin_as):\n    # Check if prefix is in ROA_DB\n    for roa_prefix, (max_len, valid_asn) in ROA_DB.items():\n        if prefix == roa_prefix:\n            # Match prefix exactly, check origin asn and max length\n            if origin_as == valid_asn:\n                return 'valid'\n            else:\n                return 'invalid-as'\n    # If not found in ROA_DB, it might be 'unknown'\n    return 'unknown'\n\n\ndef detect_rpki_invalid_announcements():\n    target_asn = \"64496\"\n    from_time_str = \"2027-08-10 00:00:00\"\n    until_time_str = \"2027-08-10 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    invalid_announcements = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n                            as_path = fields.get('as-path', '').split()\n\n                            if as_path and as_path[-1] == target_asn:\n                                status = rpki_check(prefix, target_asn)\n                                if status.startswith('invalid'):\n                                    invalid_announcements.append({\n                                        'timestamp': elem_dt,\n                                        'prefix': prefix,\n                                        'origin_as': target_asn,\n                                        'rpki_status': status\n                                    })\n\n    if invalid_announcements:\n        df_invalid = pd.DataFrame(invalid_announcements)\n        print(\"Potential RPKI-Invalid Announcements for AS64496\")\n        print(df_invalid)\n    else:\n        print(\"No RPKI-invalid announcements detected for AS64496 in the given time window.\")\n\nif __name__ == \"__main__\":\n    detect_rpki_invalid_announcements()\n"}], "is_classification": false}
{"id": "seed_task_42", "name": "AS Prepend Trend Analysis for Prefix 10.10.10.0/24", "instruction": "Monitor changes in AS path prepend lengths for prefix 10.10.10.0/24 from Sep 1 00:00 to Sep 1 00:30, 2027. Report any periods where the same ASN is repeated more than 3 times consecutively and how frequently it occurs.", "instances": [{"input": "Time window: Sep 1, 2027, 00:00:00 to Sep 1, 2027, 00:30:00\nTarget Prefix: 10.10.10.0/24\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_as_prepend_trends():\n    target_prefix = \"10.10.10.0/24\"\n    from_time_str = \"2027-09-01 00:00:00\"\n    until_time_str = \"2027-09-01 00:30:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    prepend_occurrences = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n\n                            if prefix == target_prefix:\n                                as_path_str = fields.get('as-path', '')\n                                as_path_list = as_path_str.split()\n                                if len(as_path_list) > 1:\n                                    current_as = as_path_list[0]\n                                    count = 1\n                                    for i in range(1, len(as_path_list)):\n                                        if as_path_list[i] == current_as:\n                                            count += 1\n                                        else:\n                                            if count > 3:\n                                                prepend_occurrences.append({\n                                                    'timestamp': elem_dt,\n                                                    'asn': current_as,\n                                                    'repeat_count': count,\n                                                    'as_path': as_path_str\n                                                })\n                                            current_as = as_path_list[i]\n                                            count = 1\n                                    # Check the final run\n                                    if count > 3:\n                                        prepend_occurrences.append({\n                                            'timestamp': elem_dt,\n                                            'asn': current_as,\n                                            'repeat_count': count,\n                                            'as_path': as_path_str\n                                        })\n\n    if prepend_occurrences:\n        df_prepends = pd.DataFrame(prepend_occurrences)\n        print(\"AS Prepend Trend Analysis for 10.10.10.0/24\")\n        print(df_prepends)\n    else:\n        print(\"No excessive AS path prepending detected in the given time window.\")\n\nif __name__ == \"__main__\":\n    analyze_as_prepend_trends()\n"}], "is_classification": false}
{"id": "seed_task_43", "name": "ATOMIC_AGGREGATE Attribute Usage for 192.168.1.0/24", "instruction": "Detect and summarize usage of the ATOMIC_AGGREGATE path attribute in announcements for prefix 192.168.1.0/24 from Dec 10 00:00 to Dec 10 01:00, 2027. List each occurrence, the origin AS, and the as-path when ATOMIC_AGGREGATE is set.", "instances": [{"input": "Time window: Dec 10, 2027, 00:00:00 to Dec 10, 2027, 01:00:00\nTarget Prefix: 192.168.1.0/24\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef detect_atomic_aggregate_usage():\n    prefix_target = \"192.168.1.0/24\"\n    from_time_str = \"2027-12-10 00:00:00\"\n    until_time_str = \"2027-12-10 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    usage_records = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n                            if prefix == prefix_target:\n                                as_path_str = fields.get('as-path', '')\n                                as_path_list = as_path_str.split()\n                                # Check if ATOMIC_AGGREGATE is set\n                                atomic_aggregate = rec.status.get('atomic-aggregate') if rec.status else None\n                                # Depending on pybgpstream version, it might be in fields or rec attributes\n                                if atomic_aggregate:  # or fields.get('atomic-aggregate')\n                                    origin_as = as_path_list[-1] if as_path_list else None\n                                    usage_records.append({\n                                        'timestamp': elem_time,\n                                        'origin_as': origin_as,\n                                        'as_path': as_path_str\n                                    })\n\n    if usage_records:\n        df_usage = pd.DataFrame(usage_records)\n        print(\"ATOMIC_AGGREGATE usage for prefix 192.168.1.0/24\")\n        print(df_usage)\n    else:\n        print(\"No ATOMIC_AGGREGATE attribute usage detected for 192.168.1.0/24 in the given time window.\")\n\nif __name__ == \"__main__\":\n    detect_atomic_aggregate_usage()\n"}], "is_classification": false}
{"id": "seed_task_44", "name": "BGP Path Diversity Analysis for 8.8.4.0/24", "instruction": "Analyze AS path diversity for prefix 8.8.4.0/24 from Jan 10 00:00 to Jan 10 01:00, 2028. Collect all unique AS paths, count their occurrences, and identify the top 5 most common paths.", "instances": [{"input": "Time window: Jan 10, 2028, 00:00:00 to Jan 10, 2028, 01:00:00\nTarget Prefix: 8.8.4.0/24\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_bgp_path_diversity():\n    target_prefix = \"8.8.4.0/24\"\n    from_time_str = \"2028-01-10 00:00:00\"\n    until_time_str = \"2028-01-10 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    path_counter = defaultdict(int)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n                            if prefix == target_prefix:\n                                as_path_str = fields.get('as-path', '')\n                                if as_path_str:\n                                    path_counter[as_path_str] += 1\n\n    if path_counter:\n        df_paths = pd.DataFrame(\n            [(p, c) for p, c in path_counter.items()],\n            columns=[\"as_path\", \"count\"]\n        ).sort_values(by=\"count\", ascending=False)\n        top_5 = df_paths.head(5)\n        print(\"Unique AS Paths for 8.8.4.0/24:\")\n        print(df_paths)\n        print(\"\\nTop 5 most common paths:\")\n        print(top_5)\n    else:\n        print(\"No announcements found for prefix 8.8.4.0/24 in the given time window.\")\n\nif __name__ == \"__main__\":\n    analyze_bgp_path_diversity()\n"}], "is_classification": false}
{"id": "seed_task_45", "name": "Flapping Prefix Correlation with Outage Events", "instruction": "Correlate significant prefix flapping for 192.0.2.0/24 with a known outage window (Mar 15 00:00 to Mar 15 00:30, 2028). Identify announcements/withdrawals exceeding 30 events in 5 minutes and check if they align with reported outages.", "instances": [{"input": "Time window: Mar 15, 2028, 00:00:00 to Mar 15, 2028, 00:30:00\nTarget Prefix: 192.0.2.0/24\nOutage threshold: > 30 events in 5-minute intervals", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone, timedelta\nfrom collections import defaultdict\n\ndef correlate_flapping_with_outage():\n    prefix_of_interest = \"192.0.2.0/24\"\n    from_time_str = \"2028-03-15 00:00:00\"\n    until_time_str = \"2028-03-15 00:30:00\"\n    event_threshold = 30\n    interval_minutes = 5\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    # Create intervals\n    intervals = []\n    current_start = from_time\n    while current_start < until_time:\n        interval_end = current_start + timedelta(minutes=interval_minutes)\n        intervals.append((current_start, interval_end))\n        current_start = interval_end\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    # Count announcements/withdrawals in each interval\n    interval_counts = [0] * len(intervals)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.fields.get('prefix') == prefix_of_interest and elem.type in ['A', 'W']:\n                            for i, (start_i, end_i) in enumerate(intervals):\n                                if start_i <= elem_dt < end_i:\n                                    interval_counts[i] += 1\n                                    break\n\n    results = []\n    for i, (start_i, end_i) in enumerate(intervals):\n        updates_num = interval_counts[i]\n        outage_suspected = (updates_num > event_threshold)\n        results.append({\n            'interval': f\"{start_i.strftime('%Y-%m-%d %H:%M')} - {end_i.strftime('%H:%M')}\",\n            'event_count': updates_num,\n            'outage_correlated': outage_suspected\n        })\n\n    df_results = pd.DataFrame(results)\n    print(\"Flapping Prefix Correlation with Outage Events\")\n    print(df_results)\n\nif __name__ == \"__main__\":\n    correlate_flapping_with_outage()\n"}], "is_classification": false}
{"id": "seed_task_46", "name": "Community-based Traffic Engineering Tracking for AS12399", "instruction": "Track usage of specific traffic engineering communities (e.g., 12399:1000 for local preference adjustments) in announcements from AS12399 from Apr 5 12:00 to Apr 5 13:00, 2028. Summarize the frequency of these communities, associated prefixes, and any repeated patterns.", "instances": [{"input": "Time window: Apr 5, 2028, 12:00:00 to Apr 5, 2028, 13:00:00\nTarget ASN: 12399\nTraffic Engineering Community: 12399:1000\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef track_traffic_engineering_community():\n    target_asn = \"12399\"\n    te_community = \"12399:1000\"\n    from_time_str = \"2028-04-05 12:00:00\"\n    until_time_str = \"2028-04-05 13:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    community_counts = defaultdict(int)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            as_path = fields.get('as-path', '').split()\n                            if as_path and as_path[-1] == target_asn:\n                                prefix = fields.get('prefix')\n                                communities = fields.get('communities', [])\n\n                                # Convert tuple to string if needed\n                                comm_strings = set()\n                                for comm in communities:\n                                    if isinstance(comm, tuple):\n                                        comm_strings.add(f\"{comm[0]}:{comm[1]}\")\n                                    else:\n                                        comm_strings.add(str(comm))\n\n                                if te_community in comm_strings:\n                                    community_counts[prefix] += 1\n\n    if community_counts:\n        df = pd.DataFrame(\n            [(pfx, count) for pfx, count in community_counts.items()],\n            columns=[\"prefix\", \"te_community_occurrences\"]\n        ).sort_values(by=\"te_community_occurrences\", ascending=False)\n        print(\"Traffic Engineering Community Usage for AS12399\")\n        print(df)\n    else:\n        print(\"No usage of the traffic engineering community was found for AS12399 in the given time window.\")\n\nif __name__ == \"__main__\":\n    track_traffic_engineering_community()\n"}], "is_classification": false}
{"id": "seed_task_47", "name": "AS Path Length Comparison for IPv6 vs. IPv4", "instruction": "Compare the average AS path length for IPv6 routes vs. IPv4 routes announced between May 10 00:00 and May 10 01:00, 2028. Summarize the count of unique IPv4 prefixes, unique IPv6 prefixes, and their respective mean AS path lengths.", "instances": [{"input": "Time window: May 10, 2028, 00:00:00 to May 10, 2028, 01:00:00\nRecord Type: updates (Announcements)\nCompare IPv4 vs IPv6 AS Path Lengths", "output": "import os\nimport re\nimport ipaddress\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\n\n\ndef compare_ipv4_ipv6_as_path_lengths():\n    from_time_str = \"2028-05-10 00:00:00\"\n    until_time_str = \"2028-05-10 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    ipv4_path_lengths = []\n    ipv6_path_lengths = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            prefix = elem.fields.get('prefix')\n                            if not prefix:\n                                continue\n\n                            try:\n                                net = ipaddress.ip_network(prefix)\n                            except ValueError:\n                                continue\n\n                            as_path_str = elem.fields.get('as-path', '')\n                            as_path_list = as_path_str.split()\n                            path_len = len(as_path_list)\n\n                            if net.version == 4:\n                                ipv4_path_lengths.append(path_len)\n                            elif net.version == 6:\n                                ipv6_path_lengths.append(path_len)\n\n    # Summarize results\n    ipv4_count = len(ipv4_path_lengths)\n    ipv6_count = len(ipv6_path_lengths)\n\n    if ipv4_count > 0:\n        ipv4_mean = sum(ipv4_path_lengths) / ipv4_count\n    else:\n        ipv4_mean = 0\n\n    if ipv6_count > 0:\n        ipv6_mean = sum(ipv6_path_lengths) / ipv6_count\n    else:\n        ipv6_mean = 0\n\n    print(\"IPv4 Route Count:\", ipv4_count)\n    print(\"IPv4 Mean AS Path Length:\", f\"{ipv4_mean:.2f}\")\n    print(\"IPv6 Route Count:\", ipv6_count)\n    print(\"IPv6 Mean AS Path Length:\", f\"{ipv6_mean:.2f}\")\n\nif __name__ == \"__main__\":\n    compare_ipv4_ipv6_as_path_lengths()\n"}], "is_classification": false}
{"id": "seed_task_48", "name": "Detect AS Looping for Prefix 192.168.10.0/24", "instruction": "Find any AS path loops for prefix 192.168.10.0/24 from Oct 20 00:00 to Oct 20 01:00, 2028. An AS loop occurs if an ASN reappears non-consecutively within the path. Summarize looped paths and timestamps.", "instances": [{"input": "Time window: Oct 20, 2028, 00:00:00 to Oct 20, 2028, 01:00:00\nTarget Prefix: 192.168.10.0/24\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\n\ndef detect_as_loops_for_prefix():\n    target_prefix = \"192.168.10.0/24\"\n    from_time_str = \"2028-10-20 00:00:00\"\n    until_time_str = \"2028-10-20 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    loop_records = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        rec_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if rec_time < from_time or rec_time > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            pfx = elem.fields.get('prefix')\n                            if pfx == target_prefix:\n                                as_path_str = elem.fields.get('as-path', '')\n                                as_path_list = as_path_str.split()\n                                visited = set()\n                                loop_asn = None\n\n                                for asn in as_path_list:\n                                    if asn in visited:\n                                        loop_asn = asn\n                                        break\n                                    visited.add(asn)\n\n                                if loop_asn:\n                                    loop_records.append({\n                                        'timestamp': rec_time,\n                                        'as_path': as_path_str,\n                                        'loop_asn': loop_asn\n                                    })\n\n    if loop_records:\n        df_loops = pd.DataFrame(loop_records)\n        print(\"AS Looping Detected for 192.168.10.0/24:\")\n        print(df_loops)\n    else:\n        print(\"No AS loops detected for prefix 192.168.10.0/24 in the given time window.\")\n\nif __name__ == \"__main__\":\n    detect_as_loops_for_prefix()\n"}], "is_classification": false}
{"id": "seed_task_49", "name": "Overlapping Prefix Detection for 203.0.113.0/24", "instruction": "Identify any overlapping prefixes with 203.0.113.0/24 announced from Nov 1 12:00 to Nov 1 13:00, 2028. Compare announced ranges to see if any smaller or larger networks intersect, and summarize their origin AS.", "instances": [{"input": "Time window: Nov 1, 2028, 12:00:00 to Nov 1, 2028, 13:00:00\nTarget Prefix: 203.0.113.0/24\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport ipaddress\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\n\n\ndef detect_overlapping_prefixes():\n    target_prefix_str = \"203.0.113.0/24\"\n    from_time_str = \"2028-11-01 12:00:00\"\n    until_time_str = \"2028-11-01 13:00:00\"\n\n    target_net = ipaddress.ip_network(target_prefix_str)\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    overlaps = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                # Filter time range\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n\n                # Initialize BGPStream\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                # Process each record\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix_str = fields.get('prefix')\n                            as_path_str = fields.get('as-path', '')\n                            as_path_list = as_path_str.split()\n\n                            if prefix_str:\n                                try:\n                                    announced_net = ipaddress.ip_network(prefix_str)\n                                except ValueError:\n                                    continue\n\n                                # Check for overlap with target_net, ignoring exact match\n                                if announced_net.overlaps(target_net) and announced_net != target_net:\n                                    origin_as = as_path_list[-1] if as_path_list else None\n                                    overlaps.append({\n                                        'timestamp': elem_dt,\n                                        'overlapping_prefix': prefix_str,\n                                        'origin_as': origin_as\n                                    })\n\n    if overlaps:\n        df_overlaps = pd.DataFrame(overlaps)\n        print(\"Overlapping Prefixes with 203.0.113.0/24:\")\n        print(df_overlaps)\n    else:\n        print(\"No overlapping prefixes detected for the given time window.\")\n\nif __name__ == \"__main__\":\n    detect_overlapping_prefixes()\n"}], "is_classification": false}
{"id": "seed_task_50", "name": "AS Adjacency Analysis for AS65001", "instruction": "Analyze the set of AS adjacencies (neighbors) for AS65001 in announcements from Dec 1 00:00 to Dec 1 01:00, 2028. Collect all ASNs that appear immediately before or after AS65001 in the AS path and summarize their frequency.", "instances": [{"input": "Time window: Dec 1, 2028, 00:00:00 to Dec 1, 2028, 01:00:00\nTarget ASN: 65001\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_as_adjacencies():\n    target_asn = \"65001\"\n    from_time_str = \"2028-12-01 00:00:00\"\n    until_time_str = \"2028-12-01 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    adjacency_counts = defaultdict(int)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            as_path_str = elem.fields.get('as-path', '')\n                            as_path_list = as_path_str.split()\n\n                            # Find target_asn in the path\n                            for i, asn in enumerate(as_path_list):\n                                if asn == target_asn:\n                                    # Check for neighbors\n                                    # Previous neighbor\n                                    if i > 0:\n                                        prev_asn = as_path_list[i - 1]\n                                        adjacency_counts[prev_asn] += 1\n                                    # Next neighbor\n                                    if i < len(as_path_list) - 1:\n                                        next_asn = as_path_list[i + 1]\n                                        adjacency_counts[next_asn] += 1\n\n    if adjacency_counts:\n        df_adjacencies = pd.DataFrame(\n            [(neighbor, count) for neighbor, count in adjacency_counts.items()],\n            columns=[\"adjacent_as\", \"frequency\"]\n        ).sort_values(by=\"frequency\", ascending=False)\n        print(f\"AS Adjacency Analysis for AS{target_asn}:\")\n        print(df_adjacencies)\n    else:\n        print(f\"No adjacency data found for AS{target_asn} in the specified time window.\")\n\nif __name__ == \"__main__\":\n    analyze_as_adjacencies()\n"}], "is_classification": false}