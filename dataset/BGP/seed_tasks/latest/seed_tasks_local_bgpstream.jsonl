{"id": "seed_task_0", "name": "BGP Update Summary for AS4766", "instruction": "Summarize the total count of BGP update messages (announcements and withdrawals) for AS4766 within the time window from Oct 28 13:00 to Oct 28 13:15, 2024. Include the minimum, maximum, and median number of updates observed during this period.", "instances": [{"input": "Time window: Oct 28, 2024, 13:00:00 to Oct 28, 2024, 13:15:00\nTarget ASN: 4766\nDirectory: /home/hb/ris_bgp_updates/2024/10/rrc00\nRecord Type: updates (Announcements and Withdrawals)", "output": "import pybgpstream\nfrom datetime import datetime, timezone\nimport os\nimport statistics\nimport re\n\ndef process_bgp_updates(directory, target_asn, from_time_str, until_time_str):\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.gz$'\n    update_counts = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp_str = date_str + time_str\n                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_time < from_time or file_time > until_time:\n                    continue\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n                count = 0\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n                        elem_type = elem.type\n                        fields = elem.fields\n                        as_path = fields.get(\"as-path\", \"\").split()\n                        if target_asn in as_path:\n                            if elem_type in {'A', 'W'}:\n                                count += 1\n                update_counts.append(count)\n    min_updates = min(update_counts) if update_counts else 0\n    max_updates = max(update_counts) if update_counts else 0\n    median_updates = statistics.median(update_counts) if update_counts else 0\n    print(f\"Summary for AS{target_asn}:\")\n    print(f\"Minimum updates: {min_updates}\")\n    print(f\"Maximum updates: {max_updates}\")\n    print(f\"Median updates: {median_updates}\")\nif __name__ == \"__main__\":\n    directory = \"/home/hb/ris_bgp_updates/2024/10/rrc00\"\n    target_asn = \"4766\"\n    from_time_str = \"2024-10-28 13:00:00\"\n    until_time_str = \"2024-10-28 13:15:00\"\n    process_bgp_updates(directory, target_asn=target_asn, from_time_str=from_time_str, until_time_str=until_time_str)"}], "is_classification": false}
{"id": "seed_task_1", "name": "Unique Prefixes and Origin AS Summary for AS4766", "instruction": "Provide a summary of unique prefixes and origin ASes associated with AS4766 from Oct 28 13:00 to 13:15, 2024. Track the count of unique prefixes and changes in origin ASes, if any.", "instances": [{"input": "Time window: Oct 28, 2024, 13:00:00 to Oct 28, 2024, 13:15:00\nTarget ASN: 4766\nDirectory: /home/hb/ris_bgp_updates/2024/10/rrc00\nRecord Type: updates (Announcements and Withdrawals)", "output": "import os\nimport re\nimport pybgpstream\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_local_bgp_updates(directory, target_asn, from_time_str, until_time_str):\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n    unique_prefixes = set()\n    origin_as_changes = defaultdict(set)\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp_str = date_str + time_str\n                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_time < from_time or file_time > until_time:\n                    continue\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n                        elem_type = elem.type\n                        fields = elem.fields\n                        prefix = fields.get(\"prefix\")\n                        if prefix is None:\n                            continue\n                        as_path_str = fields.get('as-path', \"\")\n                        as_path = as_path_str.split()\n                        if target_asn not in as_path:\n                            continue\n                        if elem_type == 'A':\n                            unique_prefixes.add(prefix)\n                            origin_asn = as_path[-1] if as_path else None\n                            if origin_asn:\n                                origin_as_changes[prefix].add(origin_asn)\n    print(f\"Total unique prefixes associated with AS{target_asn}: {len(unique_prefixes)}\")\n    for prefix, origin_set in origin_as_changes.items():\n        print(f\"Prefix: {prefix}\")\n        print(f\"  Origin AS count: {len(origin_set)}\")\n        if len(origin_set) > 1:\n            print(f\"  MOAS conflict observed: {origin_set}\")\nif __name__ == \"__main__\":\n    directory = \"/home/hb/ris_bgp_updates/2024/10/rrc00\"\n    target_asn = \"4766\"\n    from_time_str = \"2024-10-28 13:00:00\"\n    until_time_str = \"2024-10-28 13:15:00\"\n    analyze_local_bgp_updates(directory, target_asn=target_asn, from_time_str=from_time_str, until_time_str=until_time_str)"}], "is_classification": false}
{"id": "seed_task_2", "name": "AS Path Summary for AS4766", "instruction": "Summarize the AS paths for each prefix associated with ASN AS4766 over the period Oct 28 13:00 to Oct 28 13:15, 2024. Provide minimum, maximum, and median AS path lengths and highlight any significant path changes observed in BGP updates.", "instances": [{"input": "Time window: Oct 28, 2024, 13:00:00 to Oct 28, 2024, 13:15:00\nTarget ASN: 4766\nDirectory: /home/hb/ris_bgp_updates/2024/10/rrc00\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nfrom datetime import datetime, timezone\nfrom statistics import median\nfrom collections import defaultdict\n\ndef analyze_local_as_paths(directory, target_asn, from_time_str, until_time_str):\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n    prefix_paths = defaultdict(list)\n    path_changes = defaultdict(list)\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp_str = date_str + time_str\n                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_time < from_time or file_time > until_time:\n                    continue\n                file_path = os.path.join(root, file)\n                print(f\"Processing file: {file_path}\")\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n                        fields = elem.fields\n                        prefix = fields.get(\"prefix\")\n                        if not prefix:\n                            continue\n                        as_path_str = fields.get('as-path', \"\")\n                        as_path = as_path_str.split()\n                        if target_asn not in as_path:\n                            continue\n                        if elem.type == 'A':\n                            path_len = len(as_path)\n                            prefix_paths[prefix].append((as_path, path_len, elem_time))\n    for prefix, paths in prefix_paths.items():\n        lengths = [length for _, length, _ in paths]\n        min_len = min(lengths)\n        max_len = max(lengths)\n        med_len = median(lengths)\n        prev_path = None\n        for as_path, length, time in paths:\n            if prev_path and as_path != prev_path:\n                path_changes[prefix].append((time, prev_path, as_path))\n            prev_path = as_path\n        print(f\"Prefix: {prefix}\")\n        print(f\"  Min AS path length: {min_len}\")\n        print(f\"  Max AS path length: {max_len}\")\n        print(f\"  Median AS path length: {med_len}\")\n        if path_changes[prefix]:\n            print(\"  Significant AS path changes observed:\")\n            for t, old_path, new_path in path_changes[prefix]:\n                print(f\"    - At {t}: {old_path} -> {new_path}\")\nif __name__ == \"__main__\":\n    directory = \"/home/hb/ris_bgp_updates/2024/10/rrc00\"\n    target_asn = \"4766\"\n    from_time_str = \"2024-10-28 13:00:00\"\n    until_time_str = \"2024-10-28 13:15:00\"\n    analyze_local_as_paths(directory, target_asn=target_asn, from_time_str=from_time_str, until_time_str=until_time_str)"}], "is_classification": false}
{"id": "seed_task_3", "name": "High Update Frequency Analysis for AS4766", "instruction": "Identify prefixes associated with ASN 4766 that show high update frequency (announcements and withdrawals) within the period Oct 28 13:00 to Oct 28 13:15, 2024. Summarize the prefixes, update counts, and provide an assessment of their stability.", "instances": [{"input": "Time window: Oct 28, 2024, 13:00:00 to Oct 28, 2024, 13:15:00\nTarget ASN: 4766\nDirectory: /home/hb/ris_bgp_updates/2024/10/rrc00\nRecord Type: updates (Announcements and Withdrawals)", "output": "import os\nimport re\nimport pybgpstream\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_local_high_update_frequency(directory, target_asn, from_time_str, until_time_str):\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n    update_counts = defaultdict(int)\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp_str = date_str + time_str\n                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_time < from_time or file_time > until_time:\n                    continue\n                file_path = os.path.join(root, file)\n                print(f\"Processing file: {file_path}\")\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n                        fields = elem.fields\n                        prefix = fields.get(\"prefix\")\n                        if not prefix:\n                            continue\n                        as_path_str = fields.get('as-path', \"\")\n                        as_path = as_path_str.split()\n                        if target_asn not in as_path:\n                            continue\n                        if elem.type in {'A', 'W'}:\n                            update_counts[prefix] += 1\n    print(\"\\nPrefixes referencing AS4766 and their update frequency:\")\n    for prefix, count in update_counts.items():\n        stability = \"Unstable\" if count > 10 else \"Stable\"\n        print(f\"  Prefix: {prefix}, Update Count: {count}, Stability: {stability}\")\nif __name__ == \"__main__\":\n    directory = \"/home/hb/ris_bgp_updates/2024/10/rrc00\"\n    target_asn = \"4766\"\n    from_time_str = \"2024-10-28 13:00:00\"\n    until_time_str = \"2024-10-28 13:15:00\"\n    analyze_local_high_update_frequency(directory, target_asn=target_asn, from_time_str=from_time_str, until_time_str=until_time_str)"}], "is_classification": false}
{"id": "seed_task_4", "name": "MED and Community Tag Analysis for AS4766", "instruction": "Analyze the range of MED values and the most common community tags associated with BGP update messages for ASN 4766 from Oct 28 13:00 to Oct 28 14:00, 2024. Provide a summary of average MED values and frequently observed community tags.", "instances": [{"input": "Time window: Oct 28, 2024, 13:00:00 to Oct 28, 2024, 14:00:00\nTarget ASN: 4766\nDirectory: /home/hb/ris_bgp_updates/2024/10/rrc00\nRecord Type: updates (Announcements and Withdrawals)", "output": "import os\nimport re\nimport pybgpstream\nimport statistics\nfrom datetime import datetime, timezone\nfrom collections import defaultdict, Counter\n\ndef analyze_local_med_community(directory, target_asn, from_time_str, until_time_str):\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n    med_values = []\n    community_tags = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp_str = date_str + time_str\n                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_time < from_time or file_time > until_time:\n                    continue\n                file_path = os.path.join(root, file)\n                print(f\"Processing file: {file_path}\")\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n                        fields = elem.fields\n                        as_path_str = fields.get('as-path', \"\")\n                        as_path = as_path_str.split()\n                        if target_asn not in as_path:\n                            continue\n                        med_value = fields.get('med')\n                        if med_value is not None:\n                            try:\n                                med_values.append(int(med_value))\n                            except ValueError:\n                                pass\n                        communities = fields.get('communities', [])\n                        for c in communities:\n                            community_tags.append(c)\n    if med_values:\n        min_med = min(med_values)\n        max_med = max(med_values)\n        avg_med = statistics.mean(med_values)\n        print(f\"MED Value Range: {min_med} - {max_med}\")\n        print(f\"Average MED Value: {avg_med}\")\n    else:\n        print(\"No MED values found.\")\n    if community_tags:\n        community_counter = Counter(community_tags)\n        most_common_communities = community_counter.most_common(10)\n        print(\"Most Common Community Tags:\")\n        for community, count in most_common_communities:\n            print(f\"  {community} (observed {count} times)\")\n    else:\n        print(\"No community tags found.\")\nif __name__ == \"__main__\":\n    directory = \"/home/hb/ris_bgp_updates/2024/10/rrc00\"\n    target_asn = \"4766\"\n    from_time_str = \"2024-10-28 13:00:00\"\n    until_time_str = \"2024-10-28 14:00:00\"\n    analyze_local_med_community(directory, target_asn=target_asn, from_time_str=from_time_str, until_time_str=until_time_str)"}], "is_classification": false}
{"id": "seed_task_5", "name": "Flapping Prefix Analysis for AS4766", "instruction": "Identify prefixes associated with ASN 4766 that exhibit flapping behavior within the period Oct 28 13:00 to Oct 29 13:00, 2024. Summarize the prefixes, the number of flaps, and the duration between consecutive flaps.", "instances": [{"input": "Time window: Oct 28, 2024, 13:00:00 to Oct 29, 2024, 13:00:00\nTarget ASN: 4766\nDirectory: /home/hb/ris_bgp_updates/2024/10/rrc00\nRecord Type: updates (Announcements and Withdrawals)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\nimport statistics\n\ndef analyze_local_flapping(directory, target_asn, from_time_str, until_time_str):\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n    events = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp_str = date_str + time_str\n                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_time < from_time or file_time > until_time:\n                    continue\n                file_path = os.path.join(root, file)\n                print(f\"Processing file: {file_path}\")\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n                        fields = elem.fields\n                        prefix = fields.get(\"prefix\")\n                        if not prefix:\n                            continue\n                        elem_type = elem.type\n                        as_path_str = fields.get('as-path', \"\")\n                        as_path = as_path_str.split()\n                        if as_path and as_path[-1] == target_asn:\n                            if elem_type in ('A', 'W'):\n                                events.append({\n                                    'prefix': prefix,\n                                    'timestamp': elem_time,\n                                    'event_type': elem_type\n                                })\n    df = pd.DataFrame(events)\n    if df.empty:\n        print(f\"No updates found for origin {target_asn} in the given time period.\")\n        return\n    flapping_info = []\n    for prefix, group in df.groupby('prefix'):\n        group = group.sort_values('timestamp')\n        group['change'] = group['event_type'].shift() != group['event_type']\n        flap_events = group[group['change']]\n        num_flaps = len(flap_events)\n        if num_flaps > 1:\n            durations = flap_events['timestamp'].diff().dt.total_seconds().dropna()\n            avg_duration = durations.mean() if not durations.empty else 0\n            flapping_info.append({\n                'prefix': prefix,\n                'number_of_flaps': num_flaps,\n                'average_duration_between_flaps_seconds': avg_duration\n            })\n    if not flapping_info:\n        print(f\"No flapping prefixes detected for ASN {target_asn} in the given time period.\")\n        return\n    flap_df = pd.DataFrame(flapping_info)\n    print(f\"Flapping prefixes for origin {target_asn}:\")\n    print(flap_df)\nif __name__ == \"__main__\":\n    directory = \"/home/hb/ris_bgp_updates/2024/10/rrc00\"\n    target_asn = \"4766\"\n    from_time_str = \"2024-10-28 13:00:00\"\n    until_time_str = \"2024-10-29 13:00:00\"\n    analyze_local_flapping(directory, target_asn=target_asn, from_time_str=from_time_str, until_time_str=until_time_str)"}], "is_classification": false}
{"id": "seed_task_6", "name": "Unusual AS Path Changes for AS4766", "instruction": "Detect unusual AS path changes for prefixes associated with ASN 4766 from Oct 28 13:00 to Oct 28 13:15, 2024. Summarize the prefixes, number of path changes, and frequency of these changes.", "instances": [{"input": "Time window: Oct 28, 2024, 13:00:00 to Oct 28, 2024, 13:15:00\nTarget ASN: 4766\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_local_as_path_changes():\n    from_time_str = \"2024-10-28 13:00:00\"\n    until_time_str = \"2024-10-28 13:15:00\"\n    target_asn = \"4766\"\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    directory = \"/home/hb/ris_bgp_updates/2024/10/rrc00\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n    prefix_as_paths = defaultdict(set)\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp_str = date_str + time_str\n                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_time < from_time or file_time > until_time:\n                    continue\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n                            if not prefix:\n                                continue\n                            as_path_str = fields.get('as-path', '')\n                            as_path = as_path_str.split()\n                            prefix_as_paths[prefix].add(as_path_str)\n    as_path_changes = []\n    for prefix, path_set in prefix_as_paths.items():\n        if len(path_set) > 1:\n            as_path_changes.append({\n                'prefix': prefix,\n                'number_of_path_changes': len(path_set),\n                'as_paths': list(path_set)\n            })\n    df_as_changes = pd.DataFrame(as_path_changes)\n    if not df_as_changes.empty:\n        print(f\"Prefixes with AS path changes for ASN {target_asn}:\")\n        print(df_as_changes)\n    else:\n        print(f\"No unusual AS path changes detected for ASN {target_asn} in the given time period.\")\nif __name__ == \"__main__\":\n    analyze_local_as_path_changes()"}], "is_classification": false}
{"id": "seed_task_7", "name": "Suspected BGP Hijacking for AS16509", "instruction": "Identify suspected BGP hijacking incidents involving AS16509 or its prefixes (177.93.174.0/23, 138.59.238.0/23, 177.93.168.0/23) from April 24 10:00 to 12:00, 2018. Summarize the prefixes, unauthorized ASNs, and frequency of suspected hijacks.", "instances": [{"input": "Time window: April 24, 2018, 10:00:00 to April 24, 2018, 12:00:00\nTarget ASN: 16509\nKnown Prefixes: 177.93.174.0/23, 138.59.238.0/23, 177.93.168.0/23\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_local_hijacking():\n    asn = \"16509\"\n    from_time_str = \"2018-04-24 10:00:00\"\n    until_time_str = \"2018-04-24 12:00:00\"\n    known_prefixes = {\"177.93.174.0/23\", \"138.59.238.0/23\", \"177.93.168.0/23\"}\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    directory = \"/home/hb/ris_bgp_updates/2018/04/rrc00\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n    hijack_attempts = defaultdict(lambda: {\"unauthorized_asns\": set(), \"count\": 0})\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp_str = date_str + time_str\n                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n                if file_time < from_time or file_time > until_time:\n                    continue\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_time < from_time or elem_time > until_time:\n                            continue\n                        if elem.type == \"A\":\n                            fields = elem.fields\n                            prefix = fields.get(\"prefix\")\n                            if not prefix:\n                                continue\n                            as_path_str = fields.get(\"as-path\", \"\")\n                            as_path_list = as_path_str.strip().split()\n                            origin_asn = as_path_list[-1] if as_path_list else None\n                            if prefix in known_prefixes and origin_asn != asn:\n                                hijack_attempts[prefix][\"unauthorized_asns\"].add(origin_asn)\n                                hijack_attempts[prefix][\"count\"] += 1\n    results = []\n    for prefix, data in hijack_attempts.items():\n        results.append({\n            \"prefix\": prefix,\n            \"unauthorized_asns\": list(data[\"unauthorized_asns\"]),\n            \"frequency_of_suspected_hijacks\": data[\"count\"]\n        })\n    df_hijacks = pd.DataFrame(results)\n    if not df_hijacks.empty:\n        print(f\"Suspected BGP hijacking incidents involving ASN {asn}:\")\n        print(df_hijacks)\n    else:\n        print(f\"No suspected hijacking incidents detected for ASN {asn} in the given time period.\")\nif __name__ == \"__main__\":\n    analyze_local_hijacking()"}], "is_classification": false}
{"id": "seed_task_8", "name": "Multiple Origin AS Detection for Example Prefixes", "instruction": "Detect multiple origin AS (MOAS) occurrences for a set of example prefix ranges from Jan 1 00:00 to Jan 1 01:00, 2025. Summarize the MOAS prefixes, the distinct origin ASes, and the number of occurrences.", "instances": [{"input": "Time window: Jan 1, 2025, 00:00:00 to Jan 1, 2025, 01:00:00\nObserved Prefixes: 192.0.2.0/24, 198.51.100.0/24\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef detect_moas():\n    from_time_str = \"2025-01-01 00:00:00\"\n    until_time_str = \"2025-01-01 01:00:00\"\n    observed_prefixes = {\"192.0.2.0/24\", \"198.51.100.0/24\"}\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    prefix_to_origins = defaultdict(set)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp = date_str + time_str\n                file_dt = datetime.strptime(file_timestamp, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n                            if prefix in observed_prefixes:\n                                as_path_str = fields.get('as-path', '')\n                                as_path = as_path_str.split()\n                                if as_path:\n                                    origin_as = as_path[-1]\n                                    prefix_to_origins[prefix].add(origin_as)\n\n    moas_results = []\n    for pfx, origins in prefix_to_origins.items():\n        if len(origins) > 1:\n            moas_results.append({\n                'prefix': pfx,\n                'distinct_origins': list(origins),\n                'occurrences': len(origins)\n            })\n\n    df_moas = pd.DataFrame(moas_results)\n    if not df_moas.empty:\n        print(\"MOAS occurrences detected:\")\n        print(df_moas)\n    else:\n        print(\"No MOAS occurrences detected in the given time window.\")\n\nif __name__ == \"__main__\":\n    detect_moas()\n"}], "is_classification": false}
{"id": "seed_task_9", "name": "Time-Series Trend Analysis of Announcements vs. Withdrawals", "instruction": "From March 15 10:00 to 10:30, 2023, generate a time-series view (5-minute intervals) of route announcements and withdrawals for prefix 8.8.8.0/24. Summarize results in tabular form showing timestamp, announcements count, and withdrawals count.", "instances": [{"input": "Time window: March 15, 2023, 10:00:00 to March 15, 2023, 10:30:00\nTarget Prefix: 8.8.8.0/24\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone, timedelta\nfrom collections import defaultdict\n\ndef analyze_time_series():\n    prefix_of_interest = \"8.8.8.0/24\"\n    from_time_str = \"2023-03-15 10:00:00\"\n    until_time_str = \"2023-03-15 10:30:00\"\n    interval_minutes = 5\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    # Prepare time buckets\n    current_bucket_start = from_time\n    time_buckets = []\n    while current_bucket_start < until_time:\n        bucket_end = current_bucket_start + timedelta(minutes=interval_minutes)\n        time_buckets.append((current_bucket_start, bucket_end))\n        current_bucket_start = bucket_end\n\n    # Counters for announcements and withdrawals per time bucket\n    bucket_counters = [defaultdict(int) for _ in time_buckets]\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp = date_str + time_str\n                file_dt = datetime.strptime(file_timestamp, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.fields.get('prefix') == prefix_of_interest:\n                            for i, (bucket_start, bucket_end) in enumerate(time_buckets):\n                                if bucket_start <= elem_dt < bucket_end:\n                                    if elem.type == 'A':\n                                        bucket_counters[i]['announcements'] += 1\n                                    elif elem.type == 'W':\n                                        bucket_counters[i]['withdrawals'] += 1\n                                    break\n\n    # Build results\n    results = []\n    for i, (bucket_start, bucket_end) in enumerate(time_buckets):\n        results.append({\n            'time_bucket': f\"{bucket_start.strftime('%Y-%m-%d %H:%M')} - {bucket_end.strftime('%H:%M')}\",\n            'announcements': bucket_counters[i].get('announcements', 0),\n            'withdrawals': bucket_counters[i].get('withdrawals', 0)\n        })\n\n    df_trends = pd.DataFrame(results)\n    print(\"Time-series trend for prefix 8.8.8.0/24:\")\n    print(df_trends)\n\nif __name__ == \"__main__\":\n    analyze_time_series()\n"}], "is_classification": false}
{"id": "seed_task_10", "name": "BGP Community and Policy Analysis", "instruction": "Identify and summarize BGP communities observed for prefix 9.9.9.0/24 from Feb 1 08:00 to Feb 1 10:00, 2025. List unique communities and the frequency of their appearance in route announcements.", "instances": [{"input": "Time window: Feb 1, 2025, 08:00:00 to Feb 1, 2025, 10:00:00\nTarget Prefix: 9.9.9.0/24\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_bgp_communities():\n    prefix_of_interest = \"9.9.9.0/24\"\n    from_time_str = \"2025-02-01 08:00:00\"\n    until_time_str = \"2025-02-01 10:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    community_counts = defaultdict(int)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp = date_str + time_str\n                file_dt = datetime.strptime(file_timestamp, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n\n                            if prefix == prefix_of_interest:\n                                community_list = fields.get('communities', [])\n                                # communities is typically a list of (ASN, value) in PyBGPStream\n                                # But may vary based on version. Checking if we have them:\n                                if community_list:\n                                    for comm in community_list:\n                                        # Convert tuple to string\n                                        community_str = f\"{comm[0]}:{comm[1]}\"\n                                        community_counts[community_str] += 1\n\n    if community_counts:\n        df = pd.DataFrame(\n            [(comm, freq) for comm, freq in community_counts.items()],\n            columns=[\"community\", \"frequency\"]\n        ).sort_values(by=\"frequency\", ascending=False)\n        print(\"BGP Community Analysis for 9.9.9.0/24\")\n        print(\"-------------------------------------\")\n        print(df)\n    else:\n        print(\"No BGP communities detected for the prefix in the given time window.\")\n\nif __name__ == \"__main__\":\n    analyze_bgp_communities()\n"}], "is_classification": false}
{"id": "seed_task_11", "name": "Route Leak Detection for AS1312", "instruction": "Detect route leaks originating from AS1312 for any monitored prefix between Dec 10 00:00 and Dec 10 02:00, 2024. Summarize suspect announcements where AS1312 appears as an intermediate AS, but not as the legitimate origin.", "instances": [{"input": "Time window: Dec 10, 2024, 00:00:00 to Dec 10, 2024, 02:00:00\nSuspicious ASN: 1312\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef detect_route_leaks():\n    suspicious_asn = \"1312\"\n    from_time_str = \"2024-12-10 00:00:00\"\n    until_time_str = \"2024-12-10 02:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    leak_data = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp = date_str + time_str\n                file_dt = datetime.strptime(file_timestamp, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n                            as_path_str = fields.get('as-path', '')\n                            as_path_list = as_path_str.split()\n\n                            # If suspicious ASN is in the path, but not the last AS in path\n                            if suspicious_asn in as_path_list:\n                                if as_path_list[-1] != suspicious_asn:  # not the origin AS\n                                    leak_data.append({\n                                        \"timestamp\": elem_dt,\n                                        \"prefix\": prefix,\n                                        \"as_path\": as_path_str\n                                    })\n\n    if leak_data:\n        df_leaks = pd.DataFrame(leak_data)\n        print(f\"Route Leaks Detected Involving AS{suspicious_asn}:\")\n        print(df_leaks)\n    else:\n        print(f\"No route leaks detected for AS{suspicious_asn} in the given time window.\")\n\nif __name__ == \"__main__\":\n    detect_route_leaks()\n"}], "is_classification": false}
{"id": "seed_task_12", "name": "Prefix Flapping Detection for 203.0.113.0/24", "instruction": "Determine if prefix 203.0.113.0/24 is flapping (excessive announcements and withdrawals) within the time window May 5 09:00 to May 5 10:00, 2025. Summarize total announcements, total withdrawals, and flag a flap if announcements+withdrawals exceed 20 events in any 5-minute interval.", "instances": [{"input": "Time window: May 5, 2025, 09:00:00 to May 5, 2025, 10:00:00\nTarget Prefix: 203.0.113.0/24\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone, timedelta\nfrom collections import defaultdict\n\ndef detect_prefix_flapping():\n    target_prefix = \"203.0.113.0/24\"\n    from_time_str = \"2025-05-05 09:00:00\"\n    until_time_str = \"2025-05-05 10:00:00\"\n    interval_minutes = 5\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    # Prepare 5-minute intervals\n    intervals = []\n    current_start = from_time\n    while current_start < until_time:\n        current_end = current_start + timedelta(minutes=interval_minutes)\n        intervals.append((current_start, current_end))\n        current_start = current_end\n\n    # Counters for announcements and withdrawals in each interval\n    flap_counters = [defaultdict(int) for _ in intervals]\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp = date_str + time_str\n                file_dt = datetime.strptime(file_timestamp, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.fields.get('prefix') == target_prefix:\n                            # Identify which interval this update falls into\n                            for i, (start_i, end_i) in enumerate(intervals):\n                                if start_i <= elem_dt < end_i:\n                                    if elem.type == 'A':\n                                        flap_counters[i]['announcements'] += 1\n                                    elif elem.type == 'W':\n                                        flap_counters[i]['withdrawals'] += 1\n                                    break\n\n    # Summarize results\n    flap_report = []\n    for i, (start_i, end_i) in enumerate(intervals):\n        ann = flap_counters[i].get('announcements', 0)\n        wdr = flap_counters[i].get('withdrawals', 0)\n        total = ann + wdr\n        flap_report.append({\n            'interval': f\"{start_i.strftime('%Y-%m-%d %H:%M')} - {end_i.strftime('%H:%M')}\",\n            'announcements': ann,\n            'withdrawals': wdr,\n            'total_events': total,\n            'flap_detected': total > 20\n        })\n\n    df_flaps = pd.DataFrame(flap_report)\n    print(\"Prefix Flapping Detection Report for 203.0.113.0/24\")\n    print(\"-------------------------------------------------\")\n    print(df_flaps)\n\nif __name__ == \"__main__\":\n    detect_prefix_flapping()\n"}], "is_classification": false}
{"id": "seed_task_13", "name": "AS Relationship and Topology Analysis for AS65000", "instruction": "Identify upstream providers for AS65000 by analyzing announcements in the local update files from July 1 00:00 to July 1 01:00, 2025. Summarize unique next-hop ASes seen directly before AS65000 in the AS path.", "instances": [{"input": "Time window: July 1, 2025, 00:00:00 to July 1, 2025, 01:00:00\nTarget ASN: 65000\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_as_relationships():\n    target_asn = \"65000\"\n    from_time_str = \"2025-07-01 00:00:00\"\n    until_time_str = \"2025-07-01 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    # Store each next-hop AS found directly before the target in the path\n    upstream_providers = defaultdict(int)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp = date_str + time_str\n                file_dt = datetime.strptime(file_timestamp, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            as_path_str = elem.fields.get('as-path', '')\n                            as_path_list = as_path_str.split()\n\n                            # Find index of target_asn if it exists\n                            if target_asn in as_path_list:\n                                idx = as_path_list.index(target_asn)\n                                # If target_asn is not the first in the path, there's a next-hop before it\n                                if idx > 0:\n                                    upstream_as = as_path_list[idx - 1]\n                                    upstream_providers[upstream_as] += 1\n\n    if upstream_providers:\n        df = pd.DataFrame(\n            [(asn, count) for asn, count in upstream_providers.items()],\n            columns=[\"upstream_as\", \"occurrences\"]\n        ).sort_values(by=\"occurrences\", ascending=False)\n\n        print(f\"Upstream Providers for AS{target_asn}:\")\n        print(df)\n    else:\n        print(f\"No upstream providers found for AS{target_asn} in the given time window.\")\n\nif __name__ == \"__main__\":\n    analyze_as_relationships()\n"}], "is_classification": false}
{"id": "seed_task_14", "name": "Correlation of Prefix Announcements with Historical Data", "instruction": "Compare current announcements for prefix 198.18.0.0/16 from Aug 1 00:00 to Aug 1 01:00, 2025 with historical data from Jul 1 00:00 to Jul 1 01:00, 2025. Identify differences in AS path length, origin AS, and announcement volume.", "instances": [{"input": "Current Time Window: Aug 1, 2025, 00:00:00 to Aug 1, 2025, 01:00:00\nHistorical Baseline: Jul 1, 2025, 00:00:00 to Jul 1, 2025, 01:00:00\nTarget Prefix: 198.18.0.0/16\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\nCURRENT_DIRECTORY = \"/path/to/local/bgp/current_updates\"  # for Aug 1 2025\nHISTORICAL_DIRECTORY = \"/path/to/local/bgp/historical_updates\"  # for Jul 1 2025\n\npattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n# Helper function to gather stats from a given directory/time range\n\ndef gather_stats(directory, from_time, until_time, prefix):\n    stats = {\n        \"as_path_lengths\": [],\n        \"origin_counts\": defaultdict(int),\n        \"announcement_count\": 0\n    }\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_timestamp = date_str + time_str\n                file_dt = datetime.strptime(file_timestamp, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            pfx = elem.fields.get('prefix')\n                            if pfx == prefix:\n                                stats[\"announcement_count\"] += 1\n\n                                as_path_str = elem.fields.get('as-path', '')\n                                as_path_list = as_path_str.split()\n                                if as_path_list:\n                                    stats[\"as_path_lengths\"].append(len(as_path_list))\n                                    origin_as = as_path_list[-1]\n                                    stats[\"origin_counts\"][origin_as] += 1\n    return stats\n\ndef compare_current_vs_historical():\n    prefix_of_interest = \"198.18.0.0/16\"\n\n    # Current window: Aug 1 00:00 to Aug 1 01:00, 2025\n    current_from_str = \"2025-08-01 00:00:00\"\n    current_until_str = \"2025-08-01 01:00:00\"\n    current_from = datetime.strptime(current_from_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    current_until = datetime.strptime(current_until_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    # Historical window: Jul 1 00:00 to Jul 1 01:00, 2025\n    hist_from_str = \"2025-07-01 00:00:00\"\n    hist_until_str = \"2025-07-01 01:00:00\"\n    hist_from = datetime.strptime(hist_from_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    hist_until = datetime.strptime(hist_until_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    current_stats = gather_stats(CURRENT_DIRECTORY, current_from, current_until, prefix_of_interest)\n    historical_stats = gather_stats(HISTORICAL_DIRECTORY, hist_from, hist_until, prefix_of_interest)\n\n    # Compare AS path lengths\n    curr_len_series = pd.Series(current_stats[\"as_path_lengths\"]) if current_stats[\"as_path_lengths\"] else pd.Series([0])\n    hist_len_series = pd.Series(historical_stats[\"as_path_lengths\"]) if historical_stats[\"as_path_lengths\"] else pd.Series([0])\n\n    print(\"Current vs Historical BGP Data for Prefix 198.18.0.0/16\")\n    print(\"--------------------------------------------------------\")\n\n    print(\"\\nAnnouncement Volume:\")\n    print(f\"  Current: {current_stats['announcement_count']}\")\n    print(f\"  Historical: {historical_stats['announcement_count']}\")\n\n    print(\"\\nAS Path Length Statistics (Current):\")\n    print(curr_len_series.describe())\n\n    print(\"\\nAS Path Length Statistics (Historical):\")\n    print(hist_len_series.describe())\n\n    print(\"\\nTop Origin ASes (Current):\")\n    df_curr_origins = pd.DataFrame(\n        [(asn, count) for asn, count in current_stats[\"origin_counts\"].items()],\n        columns=[\"ASN\", \"Count\"]\n    ).sort_values(\"Count\", ascending=False)\n    print(df_curr_origins)\n\n    print(\"\\nTop Origin ASes (Historical):\")\n    df_hist_origins = pd.DataFrame(\n        [(asn, count) for asn, count in historical_stats[\"origin_counts\"].items()],\n        columns=[\"ASN\", \"Count\"]\n    ).sort_values(\"Count\", ascending=False)\n    print(df_hist_origins)\n\nif __name__ == \"__main__\":\n    compare_current_vs_historical()\n"}], "is_classification": false}
{"id": "seed_task_15", "name": "Network Reliability and Performance for 192.168.0.0/16", "instruction": "Measure network reliability by counting the frequency of route changes (announcements and withdrawals) for prefix 192.168.0.0/16 between Jan 1 00:00 and Jan 1 02:00, 2025. Identify intervals with high churn.", "instances": [{"input": "Time window: Jan 1, 2025, 00:00:00 to Jan 1, 2025, 02:00:00\nTarget Prefix: 192.168.0.0/16\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone, timedelta\nfrom collections import defaultdict\n\ndef analyze_network_reliability():\n    prefix_of_interest = \"192.168.0.0/16\"\n    from_time_str = \"2025-01-01 00:00:00\"\n    until_time_str = \"2025-01-01 02:00:00\"\n    interval_minutes = 10  # aggregate churn in 10-minute intervals\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    # Prepare intervals\n    intervals = []\n    current_start = from_time\n    while current_start < until_time:\n        current_end = current_start + timedelta(minutes=interval_minutes)\n        intervals.append((current_start, current_end))\n        current_start = current_end\n\n    churn_counters = [defaultdict(int) for _ in intervals]\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.fields.get('prefix') == prefix_of_interest:\n                            for i, (start_i, end_i) in enumerate(intervals):\n                                if start_i <= elem_dt < end_i:\n                                    if elem.type == 'A':\n                                        churn_counters[i]['announcements'] += 1\n                                    elif elem.type == 'W':\n                                        churn_counters[i]['withdrawals'] += 1\n                                    break\n\n    results = []\n    for i, (start_i, end_i) in enumerate(intervals):\n        ann = churn_counters[i].get('announcements', 0)\n        wdr = churn_counters[i].get('withdrawals', 0)\n        total = ann + wdr\n        results.append({\n            'interval': f\"{start_i.strftime('%Y-%m-%d %H:%M')} - {end_i.strftime('%H:%M')}\",\n            'announcements': ann,\n            'withdrawals': wdr,\n            'total_changes': total\n        })\n\n    df_churn = pd.DataFrame(results)\n    print(\"Network Reliability and Performance Metrics for 192.168.0.0/16\")\n    print(\"----------------------------------------------------------------\")\n    print(df_churn)\n\n    high_churn_intervals = df_churn[df_churn['total_changes'] > 20]\n    if not high_churn_intervals.empty:\n        print(\"\\nHigh churn detected in the following intervals:\")\n        print(high_churn_intervals)\n    else:\n        print(\"\\nNo high churn intervals detected in the given time range.\")\n\nif __name__ == \"__main__\":\n    analyze_network_reliability()\n"}], "is_classification": false}
{"id": "seed_task_16", "name": "Security-related BGP Behavior: Route Leak Investigation", "instruction": "Investigate potential route leaks involving prefix 100.64.0.0/10 by AS64496 from April 1 00:00 to April 1 01:00, 2025. Check if AS64496 appears in paths where it is neither the origin nor a legitimate transit AS based on prior knowledge (AS64496 should not be an upstream for this prefix). Summarize suspicious announcements.", "instances": [{"input": "Time window: April 1, 2025, 00:00:00 to April 1, 2025, 01:00:00\nSuspected ASN: 64496\nTarget Prefix: 100.64.0.0/10\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\n\ndef investigate_route_leaks():\n    suspect_asn = \"64496\"\n    target_prefix = \"100.64.0.0/10\"\n    from_time_str = \"2025-04-01 00:00:00\"\n    until_time_str = \"2025-04-01 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    suspicious_announcements = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            pfx = elem.fields.get('prefix')\n                            if pfx == target_prefix:\n                                as_path_str = elem.fields.get('as-path', '')\n                                as_path_list = as_path_str.split()\n\n                                if suspect_asn in as_path_list:\n                                    # If suspect_asn is not last (origin) or recognized transit\n                                    # Here, we simply check if suspect_asn is not the last in the path\n                                    # Real logic could compare with known policy data\n                                    if as_path_list[-1] != suspect_asn:\n                                        suspicious_announcements.append({\n                                            \"timestamp\": elem_dt,\n                                            \"prefix\": pfx,\n                                            \"as_path\": as_path_str\n                                        })\n\n    if suspicious_announcements:\n        df_leaks = pd.DataFrame(suspicious_announcements)\n        print(\"Suspicious Route Leaks involving AS64496:\")\n        print(df_leaks)\n    else:\n        print(\"No suspicious route leaks detected for AS64496 in the given timeframe.\")\n\nif __name__ == \"__main__\":\n    investigate_route_leaks()\n"}], "is_classification": false}
{"id": "seed_task_17", "name": "BGP Aggregator Attribute Analysis", "instruction": "Examine the aggregator attribute for any prefixes originated by AS12345 between Feb 10 10:00 and Feb 10 11:00, 2025. Summarize the distinct aggregator ASNs and IPs along with occurrence counts.", "instances": [{"input": "Time window: Feb 10, 2025, 10:00:00 to Feb 10, 2025, 11:00:00\nTarget ASN: 12345\nRecord Type: updates", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_aggregator_attribute():\n    target_asn = \"12345\"\n    from_time_str = \"2025-02-10 10:00:00\"\n    until_time_str = \"2025-02-10 11:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    aggregator_info = defaultdict(int)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            as_path_str = elem.fields.get('as-path', '')\n                            as_path_list = as_path_str.split()\n                            if as_path_list:\n                                origin_as = as_path_list[-1]\n                                if origin_as == target_asn:\n                                    agg_asn = elem.fields.get('aggregator', {}).get('asn')\n                                    agg_ip = elem.fields.get('aggregator', {}).get('ip')\n                                    if agg_asn and agg_ip:\n                                        key = f\"AS{agg_asn} {agg_ip}\"\n                                        aggregator_info[key] += 1\n\n    if aggregator_info:\n        df_agg = pd.DataFrame(\n            [(agg, count) for agg, count in aggregator_info.items()],\n            columns=[\"aggregator\", \"count\"]\n        ).sort_values(by=\"count\", ascending=False)\n        print(\"Aggregator Attributes for Origin AS12345\")\n        print(df_agg)\n    else:\n        print(\"No aggregator attribute data found for the specified time window.\")\n\nif __name__ == \"__main__\":\n    analyze_aggregator_attribute()\n"}], "is_classification": false}
{"id": "seed_task_18", "name": "Longest and Shortest AS Path Detection", "instruction": "Identify the longest and shortest AS paths for any announcements of prefix 10.0.0.0/8 from Mar 1 00:00 to Mar 1 01:00, 2025. Summarize the paths, their lengths, and the corresponding origin AS.", "instances": [{"input": "Time window: Mar 1, 2025, 00:00:00 to Mar 1, 2025, 01:00:00\nTarget Prefix: 10.0.0.0/8\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\n\ndef detect_longest_shortest_as_paths():\n    target_prefix = \"10.0.0.0/8\"\n    from_time_str = \"2025-03-01 00:00:00\"\n    until_time_str = \"2025-03-01 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    paths_collected = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            pfx = elem.fields.get('prefix')\n                            if pfx == target_prefix:\n                                as_path_str = elem.fields.get('as-path', '')\n                                as_path_list = as_path_str.split()\n                                if as_path_list:\n                                    path_length = len(as_path_list)\n                                    origin_as = as_path_list[-1]\n                                    paths_collected.append({\n                                        'as_path': as_path_str,\n                                        'path_length': path_length,\n                                        'origin_as': origin_as\n                                    })\n\n    if paths_collected:\n        df_paths = pd.DataFrame(paths_collected)\n        longest_path_row = df_paths.loc[df_paths['path_length'].idxmax()]\n        shortest_path_row = df_paths.loc[df_paths['path_length'].idxmin()]\n\n        print(\"Longest AS Path Detected:\")\n        print(longest_path_row)\n        print(\"\\nShortest AS Path Detected:\")\n        print(shortest_path_row)\n    else:\n        print(\"No AS paths were collected for the target prefix in the given time window.\")\n\nif __name__ == \"__main__\":\n    detect_longest_shortest_as_paths()\n"}], "is_classification": false}
{"id": "seed_task_19", "name": "MED Variation Analysis for AS65001", "instruction": "Track the changes in the Multi-Exit Discriminator (MED) attribute for routes originating from AS65001 from June 10 00:00 to June 10 01:00, 2025. Summarize each prefix, the range of MED values observed, and any unusual fluctuations.", "instances": [{"input": "Time window: June 10, 2025, 00:00:00 to June 10, 2025, 01:00:00\nTarget ASN: 65001\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_med_variation():\n    target_asn = \"65001\"\n    from_time_str = \"2025-06-10 00:00:00\"\n    until_time_str = \"2025-06-10 01:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    med_records = defaultdict(list)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                # Filter by time window\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            as_path = fields.get('as-path', '').split()\n                            if as_path and as_path[-1] == target_asn:\n                                prefix = fields.get('prefix')\n                                med_value = fields.get('med')\n                                if prefix and med_value is not None:\n                                    med_records[prefix].append(med_value)\n\n    results = []\n    for prefix, med_list in med_records.items():\n        if med_list:\n            min_med = min(med_list)\n            max_med = max(med_list)\n            avg_med = sum(med_list) / len(med_list)\n            results.append({\n                'prefix': prefix,\n                'min_med': min_med,\n                'max_med': max_med,\n                'avg_med': avg_med,\n                'observations': len(med_list)\n            })\n\n    if results:\n        df_results = pd.DataFrame(results)\n        print(\"MED Variation Analysis for AS65001:\")\n        print(df_results)\n    else:\n        print(\"No MED data found for AS65001 in the specified time window.\")\n\nif __name__ == \"__main__\":\n    analyze_med_variation()\n"}], "is_classification": false}
{"id": "seed_task_20", "name": "Newly Appearing Prefixes Detection", "instruction": "Identify any newly appearing prefixes in the local BGP updates data from Sept 5 12:00 to Sept 5 13:00, 2025 by comparing against a known set of historical prefixes. Summarize the newly introduced prefixes, their origin AS, and first-seen timestamps.", "instances": [{"input": "Current Time Window: Sept 5, 2025, 12:00:00 to Sept 5, 2025, 13:00:00\nKnown Historical Prefixes File: /path/to/historical_prefixes.txt\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\n\n# Assume we have a file listing historically known prefixes\ndef load_historical_prefixes(filepath):\n    with open(filepath, 'r') as f:\n        return set(line.strip() for line in f if line.strip())\n\ndef detect_newly_appearing_prefixes():\n    known_prefixes_file = \"/path/to/historical_prefixes.txt\"\n    known_prefixes = load_historical_prefixes(known_prefixes_file)\n\n    from_time_str = \"2025-09-05 12:00:00\"\n    until_time_str = \"2025-09-05 13:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    new_prefix_data = {}\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            prefix = elem.fields.get('prefix')\n                            if prefix and prefix not in known_prefixes:\n                                # If this prefix is not known historically, record it\n                                if prefix not in new_prefix_data:\n                                    as_path_str = elem.fields.get('as-path', '')\n                                    as_path_list = as_path_str.split()\n                                    origin_as = as_path_list[-1] if as_path_list else None\n                                    new_prefix_data[prefix] = {\n                                        'origin_as': origin_as,\n                                        'first_seen': elem_dt\n                                    }\n\n    if new_prefix_data:\n        df_new = pd.DataFrame([\n            {\n                'prefix': pfx,\n                'origin_as': data['origin_as'],\n                'first_seen': data['first_seen']\n            } \n            for pfx, data in new_prefix_data.items()\n        ])\n        print(\"Newly Appearing Prefixes Detected:\")\n        print(df_new)\n    else:\n        print(\"No newly appearing prefixes were detected in the specified time window.\")\n\nif __name__ == \"__main__\":\n    detect_newly_appearing_prefixes()\n"}], "is_classification": false}
{"id": "seed_task_21", "name": "Extended Community Analysis for AS65002", "instruction": "Analyze extended BGP communities for routes originated by AS65002 from Oct 10 01:00 to Oct 10 02:00, 2025. Summarize the distinct extended communities, their frequencies, and any unusual occurrences (e.g., large or unexpected community values).", "instances": [{"input": "Time window: Oct 10, 2025, 01:00:00 to Oct 10, 2025, 02:00:00\nTarget ASN: 65002\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_extended_communities():\n    target_asn = \"65002\"\n    from_time_str = \"2025-10-10 01:00:00\"\n    until_time_str = \"2025-10-10 02:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    ext_community_counts = defaultdict(int)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            as_path = fields.get('as-path', '').split()\n                            if as_path and as_path[-1] == target_asn:\n                                # In some PyBGPStream versions, extended communities may be in 'ext-communities'\n                                extended_communities = fields.get('ext-communities', [])\n                                if extended_communities:\n                                    for ext_comm in extended_communities:\n                                        # Typically ext_comm could be a tuple or a string; handle both\n                                        if isinstance(ext_comm, tuple):\n                                            ext_comm_str = \":\".join(map(str, ext_comm))\n                                        else:\n                                            ext_comm_str = str(ext_comm)\n                                        ext_community_counts[ext_comm_str] += 1\n\n    if ext_community_counts:\n        df = pd.DataFrame(\n            [(comm, freq) for comm, freq in ext_community_counts.items()],\n            columns=[\"extended_community\", \"frequency\"]\n        ).sort_values(\"frequency\", ascending=False)\n        print(\"Extended Community Analysis for AS65002\")\n        print(df)\n    else:\n        print(\"No extended community data found for the specified time window.\")\n\nif __name__ == \"__main__\":\n    analyze_extended_communities()\n"}], "is_classification": false}
{"id": "seed_task_22", "name": "MOAS and Prefix Hijack Cross-Check", "instruction": "Detect any MOAS (Multiple Origin AS) events for prefix 203.1.2.0/24 from Nov 1 12:00 to Nov 1 13:00, 2025, and cross-check if any of these additional origins might indicate a hijack (e.g., unknown or suspicious AS not typically announcing the prefix). Summarize potential hijack incidents.", "instances": [{"input": "Time window: Nov 1, 2025, 12:00:00 to Nov 1, 2025, 13:00:00\nTarget Prefix: 203.1.2.0/24\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\n# Hypothetical known legitimate origins for the prefix\nLEGITIMATE_ORIGINS = {\"65010\", \"65020\"}\n\ndef detect_moas_and_hijack():\n    target_prefix = \"203.1.2.0/24\"\n    from_time_str = \"2025-11-01 12:00:00\"\n    until_time_str = \"2025-11-01 13:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    origins_seen = defaultdict(set)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        elem_dt = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n\n                        if elem_dt < from_time or elem_dt > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            pfx = elem.fields.get('prefix')\n                            if pfx == target_prefix:\n                                as_path = elem.fields.get('as-path', '').split()\n                                if as_path:\n                                    origin_as = as_path[-1]\n                                    origins_seen[pfx].add(origin_as)\n\n    potential_hijacks = []\n    for pfx, origin_asns in origins_seen.items():\n        if len(origin_asns) > 1:\n            # MOAS scenario\n            for asn in origin_asns:\n                if asn not in LEGITIMATE_ORIGINS:\n                    # Potential hijack\n                    potential_hijacks.append({\n                        'prefix': pfx,\n                        'suspicious_origin_as': asn,\n                        'all_origins': list(origin_asns)\n                    })\n\n    if potential_hijacks:\n        df_hijacks = pd.DataFrame(potential_hijacks)\n        print(\"MOAS and Potential Hijack Incidents:\")\n        print(df_hijacks)\n    else:\n        print(\"No MOAS or hijack events detected for the specified time window.\")\n\nif __name__ == \"__main__\":\n    detect_moas_and_hijack()\n"}], "is_classification": false}
{"id": "seed_task_23", "name": "AS Prepending Detection for Prefix 192.0.2.0/24", "instruction": "Detect instances of AS path prepending in announcements for prefix 192.0.2.0/24 from Dec 20 00:00 to Dec 20 02:00, 2025. Summarize the number of times an AS repeats consecutively and how frequently it occurs in the observed data.", "instances": [{"input": "Time window: Dec 20, 2025, 00:00:00 to Dec 20, 2025, 02:00:00\nTarget Prefix: 192.0.2.0/24\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef detect_as_prepending():\n    prefix_of_interest = \"192.0.2.0/24\"\n    from_time_str = \"2025-12-20 00:00:00\"\n    until_time_str = \"2025-12-20 02:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    prepend_counts = defaultdict(int)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_dt = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        ts = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if ts < from_time or ts > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n\n                            if prefix == prefix_of_interest:\n                                as_path_str = fields.get('as-path', '')\n                                as_path_list = as_path_str.split()\n                                if len(as_path_list) > 1:\n                                    # Detect consecutive repeats in the AS path\n                                    consecutive_count = 1\n                                    for i in range(1, len(as_path_list)):\n                                        if as_path_list[i] == as_path_list[i-1]:\n                                            consecutive_count += 1\n                                        else:\n                                            if consecutive_count > 1:\n                                                # We found a run of repeated ASNs\n                                                run_asn = as_path_list[i-1]\n                                                prepend_counts[run_asn] += 1\n                                            consecutive_count = 1\n                                    # Check the last run\n                                    if consecutive_count > 1:\n                                        run_asn = as_path_list[-1]\n                                        prepend_counts[run_asn] += 1\n\n    if prepend_counts:\n        df_prepends = pd.DataFrame(\n            [(asn, count) for asn, count in prepend_counts.items()],\n            columns=[\"asn\", \"consecutive_repeat_occurrences\"]\n        ).sort_values(by=\"consecutive_repeat_occurrences\", ascending=False)\n\n        print(\"AS Prepending Analysis for 192.0.2.0/24\")\n        print(df_prepends)\n    else:\n        print(\"No AS path prepending detected for prefix 192.0.2.0/24 in the given time window.\")\n\nif __name__ == \"__main__\":\n    detect_as_prepending()\n"}], "is_classification": false}
{"id": "seed_task_24", "name": "Origin AS Change Tracking for Prefix 203.0.113.0/24", "instruction": "Track changes in the origin AS for prefix 203.0.113.0/24 from Nov 10 10:00 to Nov 10 11:00, 2025. Report any occurrences where the origin AS differs from the previously seen origin, indicating an origin change or possible hijack.", "instances": [{"input": "Time window: Nov 10, 2025, 10:00:00 to Nov 10, 2025, 11:00:00\nTarget Prefix: 203.0.113.0/24\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\n\ndef track_origin_as_changes():\n    prefix_of_interest = \"203.0.113.0/24\"\n    from_time_str = \"2025-11-10 10:00:00\"\n    until_time_str = \"2025-11-10 11:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    origin_changes = []\n    last_origin_as = None\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        ts = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if ts < from_time or ts > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n                            if prefix == prefix_of_interest:\n                                as_path_str = fields.get('as-path', '')\n                                as_path_list = as_path_str.split()\n                                if as_path_list:\n                                    current_origin_as = as_path_list[-1]\n                                    if last_origin_as is None:\n                                        last_origin_as = current_origin_as\n                                    elif current_origin_as != last_origin_as:\n                                        origin_changes.append({\n                                            'timestamp': ts,\n                                            'old_origin_as': last_origin_as,\n                                            'new_origin_as': current_origin_as\n                                        })\n                                        last_origin_as = current_origin_as\n\n    if origin_changes:\n        df_changes = pd.DataFrame(origin_changes)\n        print(\"Origin AS Changes Detected for 203.0.113.0/24:\")\n        print(df_changes)\n    else:\n        print(\"No changes in origin AS detected for the given time window.\")\n\nif __name__ == \"__main__\":\n    track_origin_as_changes()\n"}], "is_classification": false}
{"id": "seed_task_25", "name": "Local Preference Changes for AS65501", "instruction": "Analyze changes in the LOCAL_PREF (Local Preference) attribute for routes advertised by AS65501 from Jan 2 08:00 to Jan 2 09:00, 2026. Track distinct LOCAL_PREF values and how often they change for each prefix.", "instances": [{"input": "Time window: Jan 2, 2026, 08:00:00 to Jan 2, 2026, 09:00:00\nTarget ASN: 65501\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom collections import defaultdict\n\ndef analyze_local_pref_changes():\n    target_asn = \"65501\"\n    from_time_str = \"2026-01-02 08:00:00\"\n    until_time_str = \"2026-01-02 09:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    # Dictionary: prefix -> set of observed local_prefs\n    local_pref_data = defaultdict(set)\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                date_str = match.group(1)\n                time_str = match.group(2)\n                file_ts_str = date_str + time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        ts = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if ts < from_time or ts > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            as_path = fields.get('as-path', '').split()\n                            if as_path and as_path[-1] == target_asn:\n                                prefix = fields.get('prefix')\n                                local_pref = fields.get('local-pref')  # depends on PyBGPStream version\n                                if prefix and local_pref is not None:\n                                    local_pref_data[prefix].add(local_pref)\n\n    results = []\n    for pfx, prefs in local_pref_data.items():\n        results.append({\n            'prefix': pfx,\n            'distinct_local_prefs': list(prefs),\n            'changes_count': len(prefs)\n        })\n\n    if results:\n        df_results = pd.DataFrame(results)\n        print(\"Local Preference Changes for AS65501:\")\n        print(df_results)\n    else:\n        print(\"No LOCAL_PREF data found for AS65501 in the specified time window.\")\n\nif __name__ == \"__main__\":\n    analyze_local_pref_changes()\n"}], "is_classification": false}
{"id": "seed_task_26", "name": "AS Path Loop Detection for Prefix 8.8.0.0/16", "instruction": "Detect any loops in the AS path for prefix 8.8.0.0/16 from Feb 1 00:00 to Feb 1 02:00, 2026. An AS path loop occurs if the same ASN appears multiple times non-consecutively within the path. Summarize each path and the loop encountered.", "instances": [{"input": "Time window: Feb 1, 2026, 00:00:00 to Feb 1, 2026, 02:00:00\nTarget Prefix: 8.8.0.0/16\nRecord Type: updates (Announcements)", "output": "import os\nimport re\nimport pybgpstream\nimport pandas as pd\nfrom datetime import datetime, timezone\n\n\ndef detect_as_path_loops():\n    target_prefix = \"8.8.0.0/16\"\n    from_time_str = \"2026-02-01 00:00:00\"\n    until_time_str = \"2026-02-01 02:00:00\"\n\n    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n\n    directory = \"/path/to/local/bgp/updates\"\n    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n\n    loop_records = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            match = re.match(pattern, file)\n            if match:\n                file_date_str = match.group(1)\n                file_time_str = match.group(2)\n                file_ts_str = file_date_str + file_time_str\n                file_dt = datetime.strptime(file_ts_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n\n                if file_dt < from_time or file_dt > until_time:\n                    continue\n\n                file_path = os.path.join(root, file)\n                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n\n                for rec in stream.records():\n                    for elem in rec:\n                        ts = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n                        if ts < from_time or ts > until_time:\n                            continue\n\n                        if elem.type == 'A':\n                            fields = elem.fields\n                            prefix = fields.get('prefix')\n\n                            if prefix == target_prefix:\n                                as_path_str = fields.get('as-path', '')\n                                as_path_list = as_path_str.split()\n\n                                visited = set()\n                                loop_as = None\n                                for asn in as_path_list:\n                                    if asn in visited:\n                                        loop_as = asn\n                                        break\n                                    visited.add(asn)\n                                \n                                if loop_as:\n                                    loop_records.append({\n                                        'timestamp': ts,\n                                        'as_path': as_path_str,\n                                        'loop_asn': loop_as\n                                    })\n\n    if loop_records:\n        df_loops = pd.DataFrame(loop_records)\n        print(\"AS Path Loop Detected for 8.8.0.0/16:\")\n        print(df_loops)\n    else:\n        print(\"No AS path loops were detected for the target prefix in the given time window.\")\n\nif __name__ == \"__main__\":\n    detect_as_path_loops()\n"}], "is_classification": false}
