{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from typing import List\n",
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import os\n",
    " \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97d984b799747d78a250cc7bf61dacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/966 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` attribute will be overwritten with the one you passed to `from_pretrained`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387a0b1f03794a079fb1cc9c70ae9b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/71.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55af1448bd394dc8ade556d572a6d847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953a099d14c9452c8df3e55fee4b3bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fbf36ba6f34c68bd300893d330dc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7533ed87c4c4ad7908a1e09c30f4d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hyonbokan/BGP-llama2 were not used when initializing LlamaForCausalLM: ['model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.32.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.39.self_attn.q_proj.lora_A.default.weight', 'model.layers.35.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight', 'model.layers.36.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.35.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.33.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.34.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.34.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.33.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.37.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.35.self_attn.v_proj.lora_A.default.weight', 'model.layers.38.self_attn.v_proj.lora_B.default.weight', 'model.layers.39.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.39.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.36.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.36.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.36.self_attn.v_proj.lora_B.default.weight', 'model.layers.38.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.38.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.35.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.32.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.33.self_attn.v_proj.lora_A.default.weight', 'model.layers.38.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.34.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.37.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.33.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.32.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.37.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.32.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.37.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.39.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.34.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3e62c42aa54b30b4cd91559e2debde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_id = 'hyonbokan/BGP-llama2'\n",
    "llama2_13 = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# Need auth token for these\n",
    "hf_token = os.environ.get('hf_token')\n",
    "hf_auth = hf_token\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    llama2_13,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] For Packet Tracer BGP Configuration, firstly we need to configure the IP addresses of interfaces as other examples. To do this, as a better network engineering rule, firstly make your IP plan or, use the existing one. Acording to my basic IP plan, I used the below IPs for my interfaces. Explain how this is done [/INST]essoessoessoessoessoachenachenachenachenachenachenachenachenachenachenachenachenachenachenessoachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenessoachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenHCachenachenHCachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenHCachenachenachenachenachenachenachenHCachenHCachenHCachenHCachenachenHCHCHCachenHCachenHCHCachenHCachenHCHCachenachenachenachenachenachenachenachenHCachenachenachenHCachenachenachenachenHCHCachenachenHCachenHCachenachenachenHCachenachenHCHCachenachenHCachenHCHCHCachenHCHCHCHCachenHCachenHCHCHCachenachenHCHCHCHCHCHCHCHCHCHCHCHCHCachenHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHC\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"For Packet Tracer BGP Configuration, firstly we need to configure the IP addresses of interfaces as other examples. To do this, as a better network engineering rule, firstly make your IP plan or, use the existing one. Acording to my basic IP plan, I used the below IPs for my interfaces. Explain how this is done\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
