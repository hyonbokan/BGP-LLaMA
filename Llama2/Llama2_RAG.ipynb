{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10110 doc embeddings, each with a dimensionality of 384.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_file_path = '/home/hb/LLM-research/finetuning_dataset/BGP/PyBGPStream/PyBGPStream_main10K.json'\n",
    "\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Concatenate instruction and output for each item\n",
    "docs = [item['instruction'] + \" \" + item['output'] for item in data]\n",
    "\n",
    "embeddings = embed_model.embed_documents(docs)\n",
    "\n",
    "print(f\"We have {len(embeddings)} doc embeddings, each with \"\n",
    "      f\"a dimensionality of {len(embeddings[0])}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from pinecone import PodSpec\n",
    "from pinecone import ServerlessSpec, PodSpec\n",
    "import os\n",
    "\n",
    "api_key = os.environ.get('PINECONE_API')\n",
    "environment = os.environ.get('PINECONE_ENVIRONMENT')\n",
    "\n",
    "pc = Pinecone(api_key='PINECONE_API')\n",
    "spec = PodSpec(environment='PINECONE_ENVIRONMENT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = 'llama-2-rag'\n",
    "\n",
    "if index_name in pc.list_indexes().names():\n",
    "    pc.delete_index(index_name)\n",
    "\n",
    "# we create a new index\n",
    "pc.create_index(\n",
    "        index_name,\n",
    "        dimension=len(embeddings[0]),\n",
    "        metric='cosine',\n",
    "        spec=spec\n",
    "    )\n",
    "\n",
    "# wait for index to be initialized\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/hb/.cache/huggingface/datasets/json/default-ecefba89b33c753a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'most_similar_instructions', 'avg_similarity_score'],\n",
       "    num_rows: 10110\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files=\"/home/hb/LLM-research/finetuning_dataset/BGP/PyBGPStream/PyBGPStream_main10K.json\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "data_df = data.to_pandas()\n",
    "\n",
    "batch_size = 32\n",
    "total_rows = len(data_df)\n",
    "\n",
    "for i in range(0, total_rows, batch_size):\n",
    "    i_end = min(len(data_df), i+batch_size)\n",
    "    batch = data_df.iloc[i:i_end]\n",
    "    # ids = [f\"{x['instruction']}-{x['output']}\" for i, x in batch.iterrows()]\n",
    "    ids = [f\"id_{j+1}\" for j in range(i, i_end)]\n",
    "\n",
    "    instructions = [row['instruction'] for index, row in batch.iterrows()]\n",
    "    embeds = embed_model.embed_documents(instructions)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'instruction': row['instruction'],\n",
    "         'input': row['input'],\n",
    "         'output': row['output']} for index, row in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a4beeb90e648df8e03719913f096a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "# model_id = 'codellama/CodeLlama-7b-hf'\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Need auth token for these\n",
    "hf_auth = os.environ.get('hf_token')\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "# device_map = {\"\": 0}\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    " \n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    temperature=0.5,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hb/myenv/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.vectorstores.pinecone.Pinecone` was deprecated in langchain-community 0.0.18 and will be removed in 0.2.0. An updated version of the class exists in the langchain-pinecone package and should be used instead. To use it run `pip install -U langchain-pinecone` and import as `from langchain_pinecone import Pinecone`.\n",
      "  warn_deprecated(\n",
      "/home/hb/myenv/lib/python3.8/site-packages/langchain_community/vectorstores/pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = 'text'  # field in metadata that contains text content\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found document with no `code` key. Skipping.\n",
      "Found document with no `code` key. Skipping.\n",
      "Found document with no `code` key. Skipping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Generate Python code using PyBGPStream'\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query=query,  # the search query\n",
    "    k=3,  # returns top 3 most relevant chunks of text\n",
    "    # namespace='my_namespace'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "rag_pipeline = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type='stuff',\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hb/myenv/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Develop a Python script utilizing PyBGPStream to analyze BGP advertisements from AS34549 within the timeframe of 13:00 to 14:00 on January 1, 2024 from collectors rrc00. The script's core objective is to compute metrics that illustrate the degree of similarity or divergence among the AS paths of these advertisements, in comparison to a broader, aggregate perspective of AS34549's routing behavior. The script should unveil any discernible trends or anomalies in AS path configurations, thereby showing the underlying routing strategies or policy adaptations employed by AS34549. Do not use PyBGPStream 'filter' parameter.\\n\\nInput:\\n\\n* A text file containing BGP advertisements from AS34549 during the specified time frame (e.g., as34549-jan1-2024.txt)\\n* An output file where the results will be written (e.g., as34549-jan1-2024-analysis.txt)\\n\\nOutput:\\n\\n* A detailed analysis of the AS paths present in the input advertisements, including their lengths, types, and other relevant attributes.\\n* A set of metrics that quantify the degree of similarity or divergence between the AS paths of individual advertisements, relative to the overall distribution of AS paths observed across all advertisements from AS34549 during the same time period.\\n* Any notable trends or anomalies observed in the AS path configurations, which could indicate specific routing policies or strategy adjustments made by AS34549.\\n\\nExpected Output:\\n\\nThe output file should contain a comprehensive analysis of the BGP advertisements extracted from the input file, along with a set of metrics that describe the similarity or divergence of the AS paths within the advertisements. The output may include the following sections or subsections:\\n\\n1. Introduction: Provide an overview of the analysis, highlighting the purpose and scope of the investigation.\\n2. Advertisement Characterization: Present a summary of the BGP advertisements analyzed, including the number of advertisements, their originators, and any notable characteristics.\\n3. AS Path Analysis: Offer a detailed examination of the AS paths present in the advertisements, such as their lengths, types, and any observed patterns or trends.\\n4. Similarity Metrics: Compute and report metrics that measure the degree of similarity or divergence between the AS paths of individual advertisements, relative to the overall distribution of AS paths observed across all advertisements from AS34549 during the same time period. These metrics may include statistical measures like mean, median, standard deviation, and correlation coefficients.\\n5. Trend Detection: Identify any notable trends or anomalies observed in the AS path configurations, which could indicate specific routing policies or strategy adjustments made by AS34549. This section may include visualizations or graphs to facilitate the interpretation of\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Develop a Python script utilizing PyBGPStream to analyze BGP advertisements from AS34549 within the timeframe of 13:00 to 14:00 on January 1, 2024 from collectors rrc00. The script's core objective is to compute metrics that illustrate the degree of similarity or divergence among the AS paths of these advertisements, in comparison to a broader, aggregate perspective of AS34549's routing behavior. The script should unveil any discernible trends or anomalies in AS path configurations, thereby showing the underlying routing strategies or policy adaptations employed by AS34549. Do not use PyBGPStream 'filter' parameter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'\\n\\nI have attached the file \"5G_Infrastructure/demand_driven_postcode_data_results.csv\" which contains the data for the calculation of the 5G network performance KPIs.\\n\\nPlease provide me with the Python code to perform the calculations as mentioned above.\\n\\nThank you!\\n\\nHere is the file \"5G_Infrastructure/demand_driven_postcode_data_results.csv\":\\n\\n| Postcode | Latitude | Longitude | Demand (GB) | NumPoints |\\n| --- | --- | --- | --- | --- |\\n| SW1A 2AA | 51.498673 | -0.128874 | 200 | 10 |\\n| SW1W 9TQ | 51.495795 | -0.135344 | 150 | 15 |\\n| SW3 4RP | 51.481845 | -0.200892 | 300 | 20 |\\n| SW3 5EA | 51.483893 | -0.206419 | 250 | 15 |\\n| WC2N 5DU | 51.507346 | -0.126876 | 400 | 25 |\\n| WC2H 7BG | 51.509888 | -0.134286 | 350 | 20 |\\n\\nNote that the demand values are in GB (gigabytes) and the number of points (NumPoints) is also provided for each postcode.\\n\\nPlease help me with the Python code to perform the calculations as mentioned above. Thank you!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Develop a Python script utilizing PyBGPStream to analyze BGP advertisements from AS34549 within the timeframe of 13:00 to 14:00 on January 1, 2024 from collectors rrc00. The script's core objective is to compute metrics that illustrate the degree of similarity or divergence among the AS paths of these advertisements, in comparison to a broader, aggregate perspective of AS34549's routing behavior. The script should unveil any discernible trends or anomalies in AS path configurations, thereby showing the underlying routing strategies or policy adaptations employed by AS34549. Do not use PyBGPStream 'filter' parameter.\\n\\nThe script should be able to accomplish the following tasks:\\n\\n1. Load and parse BGP advertisements from AS34549 during the specified time frame using PyBGPStream.\\n2. Extract the AS paths from each advertisement and convert them into a standardized format for analysis.\\n3. Compute similarity/dissimilarity metrics between the AS paths of individual advertisements and the overall distribution of AS paths observed across all advertisements.\\n4. Visualize the results through appropriate visualization tools (e.g., matplotlib, seaborn) to facilitate interpretation and insights.\\n5. Save the output to a file or database for future reference or further analysis.\\n\\nTo achieve this task, you can follow these steps:\\n\\nStep 1: Install required packages\\n\\nYou will need to install the following packages:\\n\\n* `pybgpstream`: This package provides a Python interface to BGP data structures.\\n* `pandas`: This package is used for data manipulation and analysis.\\n* `matplotlib` or `seaborn`: These packages are used for visualization.\\n\\nYou can install these packages using pip:\\n```\\npip install pybgpstream pandas matplotlib seaborn\\n```\\nStep 2: Load BGP data\\n\\nUse the `pybgpstream` module to load BGP data from the specified time frame. You can use the `load_rpc()` function to load data from a remote collector, like `collectors-rrc00`.\\n```python\\nimport pybgpstream\\n\\n# Load BGP data from collectors-rrc00\\ndata = pybgpstream.load_rpc('collectors-rrc00', start='2024-01-01 13:00:00', end='2024-01-01 14:00:00')\\n```\\nStep 3: Extract AS paths\\n\\nUse the `parse_advertisement()` function provided by `pybgpstream` to extract the AS paths from each BGP advertisement.\\n```python\\n# Extract AS paths from each advertisement\\nas_paths = [pybgpstream.parse_advertisement(advertisement).get('as_path') for advertisement in data\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
