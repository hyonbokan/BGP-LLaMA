{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bc5e53bb884ca1a2a93b9c2e46d9a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "import os\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "# bnb_config = transformers.BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type='nf4',\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=bfloat16\n",
    "# )\n",
    "\n",
    "model_id = 'hyonbokan/BGPStream13-10k-cutoff-1024-max-2048'\n",
    "# model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Need auth token for these\n",
    "hf_auth = os.environ.get('hf_token')\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hb/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nimport pandas as pd\\nfrom dateutil.parser import parse\\n\\nstart_time = parse(2022-01-01, hour=13, minute=00)\\nend_time = parse(2022-01-01, hour=13, minute=30)\\n\\ndf = pd.DataFrame([update.create_update() for _ in range(30)])  # Creating 30 update messages\\ndf['timestamp'] = (df.timestamp.apply(lambda x: start_time + (end_time - start_time).total_seconds * i) for i in range(30))\\n\\ndf = df.set_index('timestamp')\\n\\ntotal_annoucements = df.groupby(level=0).size().tolist()\\n\\nprint('Total number of announcements from 1/1/2022 at 13:00 AM to 1/1/2022 at 13:30 AM are: ')\\nprint(total_annoucements)\\nprint('\\\\n')\\nprint('Number of announcements on 1/1/2022 at 13:00 AM: ', df.loc[start_time:end_time + 0.000001].size())\\nprint('Number of announcements on 1/1/2022 at 13:30 AM: ', df.loc[start_time:end_time + 0.00\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=400)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "llm(prompt=\"Use PyBGPStream to collect update messages from 2022-01-01 13:00:00 to 2022-01-01 13:30:00 and tell me the total number of annoucements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    # return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport pybgpstream\\n\\n# Initialize BGPStream\\nstream = pybgpstream.BGPStream(\\n    from_time=\"2021-04-01 00:00:00\", until_time=\"2021-05-01 00:00:00\",\\n    collectors=[\"route-views.jinx\"],\\n    record_type=\"updates\"\\n)\\n\\n# Collect data\\ntotal_advertisements = 0\\nfor rec in stream.records():\\n    for elem in rec:\\n        if elem.fields[\\'as-path\\'].split()[-1] == \\'131072\\':\\n            total_advertisements += 1\\n\\nprint(\"Total Number of Advertisements: {}\".format(total_advertisements))\\n\\n# If there are no records, print a message indicating that.\\nif not stream.get_next_record():\\n    print(\"No records found.\")\\nelse:\\n    print(\"Records collected successfully.\")\\n\\n# Close the stream\\nstream.close()\\n\\n# If there is an error, print it.\\nif stream.status!= \"OK\":\\n    print(\"Error occurred while accessing the stream: {}\".format(stream.status))\\n    print(\"Please check the documentation or contact the developers for more information.\")\\n    print(\"End of program execution.\")\\nelse:\\n    print(\"Program execution completed successfully.\")\\n    print(\"End of program execution.\")\\n\\n# If there is any output, print it.\\nif output:\\n    print(output)\\nelse:\\n    print(\"No output.\")\\n    print(\"End of program execution.\")\\n\\n# End of program execution.\\nprint(\"End of program execution.\")\\nprint(\"End of program execution.\")\\nprint(\"End of program execution.\")\\nprint(\"End of program execution.\")\\nprint(\"End of program execution.\")\\nprint(\"End of program execution.\")\\nprint(\"End of program execution.\")\\nprint(\"End of program execution.\")\\nprint(\"End of program execution.\")\\nprint(\"End of program execution.\")\\nprint(\"End of program execution.\")\\nprint(\"End of program execution.\")\\nprint(\"End of program execution.\")\\nprint(\"End of program execution.\")\\nprint(\"End of program execution.\")\\nprint(\"End of program execution.\")\\nprint'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt=\"Use PyBGPStream to analyze the BGP table statistics for AS131072, focusing on the total number of advertisements and print the result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_csv_agent(llm, \n",
    "                         '/home/hb/5G_dataset/demand_driven_postcode_data_results_reduced.csv', \n",
    "                         verbose=True,\n",
    "                         agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.handle_parsing_errors = True\n",
    "agent.early_stopping_method = \"force\"\n",
    "agent.return_intermediate_steps = False\n",
    "agent.max_iterations = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: I need to carefully read through the header to identify keywords related to KPIs and their column names.\n",
      "\n",
      "Action: \n",
      "python_repl_ast.select_columns(getattr(df, 'name')) == ['total_network_capacity', 'capacity_per_area', 'capacity_per_point', 'cost_per_capacity', 'cost_per_area','surplus_per_area']\n",
      "\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action Input:' after 'Action:'\n",
      "Thought:\u001b[32;1m\u001b[1;3m Let me rephrase that to fit the correct format.\n",
      "\n",
      "Action: df.select_columns(getattr(df, 'name')) == ['total_network_capacity', 'capacity_per_area', 'capacity_per_point', 'cost_per_capacity', 'cost_per_area','surplus_per_area']\n",
      "\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action Input:' after 'Action:'\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Agent stopped due to iteration limit or time limit.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Needs stop-output function\n",
    "agent.run(\"calculate 5G network performance KPIs: Total Network Capacity, Capacity per Area, Capacity per Point, Cost per Capacity, Cost per Area, and Surplus per Area. Use keywords: 'capacity', 'cost', 'area', 'numpoints' to identify relevant columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import sys\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.agents import load_tools\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "class PythonREPL:\n",
    "    \"\"\"Simulates a standalone Python REPL.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass        \n",
    "\n",
    "    def run(self, command: str) -> str:\n",
    "        \"\"\"Run command and returns anything printed.\"\"\"\n",
    "        # sys.stderr.write(\"EXECUTING PYTHON CODE:\\n---\\n\" + command + \"\\n---\\n\")\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = mystdout = StringIO()\n",
    "        try:\n",
    "            exec(command, globals())\n",
    "            sys.stdout = old_stdout\n",
    "            output = mystdout.getvalue()\n",
    "        except Exception as e:\n",
    "            sys.stdout = old_stdout\n",
    "            output = str(e)\n",
    "        # sys.stderr.write(\"PYTHON OUTPUT: \\\"\" + output + \"\\\"\\n\")\n",
    "        return output\n",
    "    \n",
    "# llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "python_repl = Tool(\n",
    "        name=\"Python REPL\",\n",
    "        func=PythonREPL().run,\n",
    "        description=\"\"\"A Python shell. Use this to execute python commands. Input should be a valid python command.\n",
    "        If you expect output it should be printed out.\"\"\",\n",
    "    )\n",
    "\n",
    "tools = [python_repl]\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"count the total number of BGP update announcements from 2022-01-01 13:00:00 to 2022-01-01 13:30:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish, OutputParserException\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the base template\n",
    "template = \"\"\"Provide pybgpstream code script for the query. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: generate python script matching the task described in the query\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input query\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a prompt template\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        return self.template.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_repl_org = PythonREPLTool()\n",
    "# tools = [python_repl_org]\n",
    "tools = [python_repl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise OutputParserException(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "\n",
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    # return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=1028,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain consisting of the LLM and a prompt\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"],\n",
    "    allowed_tools=tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True, \n",
    "    handle_parsing_errors=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought:\n",
      "import pybgpstream\n",
      "\n",
      "stream = pybgpstream.BGPStream(\n",
      "    from_time=\"2022-01-01 13:00:00\", until_time=\"2022-01-01 13:30:00\",\n",
      "    collectors=[\"rrc06\"],\n",
      "    record_type=\"updates\"\n",
      ")\n",
      "\n",
      "total_announcements = 0\n",
      "for rec in stream.records():\n",
      "    for elem in rec:\n",
      "        if elem.type == \"A\":\n",
      "            total_announcements += 1\n",
      "\n",
      "print(\"Total Announcements: {}\".format(total_announcements))\n",
      "\n",
      "Action: run the script\n",
      "Action Input: the script itself\u001b[0m\n",
      "\n",
      "Observation:run the script is not a valid tool, try one of [Python REPL].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Action: open Python REPL\n",
      "Action Input: open() function\u001b[0m\n",
      "\n",
      "Observation:open Python REPL is not a valid tool, try one of [Python REPL].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Action: type 'python' into Python REPL\n",
      "Action Input: 'python'\u001b[0m\n",
      "\n",
      "Observation:type 'python' into Python REPL is not a valid tool, try one of [Python REPL].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Action: type 'pybgpstream' into Python REPL\n",
      "Action Input: 'pybgpstream'\u001b[0m\n",
      "\n",
      "Observation:type 'pybgpstream' into Python REPL is not a valid tool, try one of [Python REPL].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Action: import pybgpstream\n",
      "Action Input: import pybgpstream\u001b[0m\n",
      "\n",
      "Observation:import pybgpstream is not a valid tool, try one of [Python REPL].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Action: use the script to count the total number of announcements in BGP updates from 2022-01-01 13:00:00 to 2022-01-01 13:30:00\n",
      "Action Input: the script itself\u001b[0m\n",
      "\n",
      "Observation:use the script to count the total number of announcements in BGP updates from 2022-01-01 13:00:00 to 2022-01-01 13:30:00 is not a valid tool, try one of [Python REPL].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Action: print the result\n",
      "Action Input: print() function\u001b[0m\n",
      "\n",
      "Observation:print the result is not a valid tool, try one of [Python REPL].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Action: print(\"Total Announcements: {}\".format(total_announcements))\n",
      "Action Input: print(\"Total Announcements: {}\".format(total_announcements))\u001b[0m\n",
      "\n",
      "Observation:print(\"Total Announcements: {}\".format(total_announcements)) is not a valid tool, try one of [Python REPL].\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Action: I now know the final answer\n",
      "Final Answer: the final answer to the original input query\n",
      "\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input query\n",
      "\n",
      "Note: The above response is generated based on the given query and the instructions provided. It is important to note that the actual output may vary depending on the specific implementation and the data used.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the final answer to the original input query\\n\\nNote: The above response is generated based on the given query and the instructions provided. It is important to note that the actual output may vary depending on the specific implementation and the data used.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"count the total number of announcements in BGP updates from 2022-01-01 13:00:00 to 2022-01-01 13:30:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Announcements: 1002005\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pybgpstream\n",
    "\n",
    "# Initialize the stream\n",
    "stream = pybgpstream.BGPStream(\n",
    "    from_time=\"2022-01-01 13:00:00\", until_time=\"2022-01-01 13:30:00\",\n",
    "    collectors=[\"route-views.linx\"],\n",
    "    record_type=\"updates\"\n",
    ")\n",
    "\n",
    "# Create a dictionary to store the counts\n",
    "announcement_counts = 0\n",
    "\n",
    "# Loop through the stream records\n",
    "for rec in stream.records():\n",
    "    for elem in rec:\n",
    "        if elem.type == \"A\":\n",
    "            # Increment the count for each announcement\n",
    "            announcement_counts += 1\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total Announcements: {announcement_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import sys\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.agents import load_tools\n",
    "\n",
    "class PythonREPL:\n",
    "    \"\"\"Simulates a standalone Python REPL.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass        \n",
    "\n",
    "    def run(self, command: str) -> str:\n",
    "        \"\"\"Run command and returns anything printed.\"\"\"\n",
    "        sys.stderr.write(\"EXECUTING PYTHON CODE:\\n---\\n\" + command + \"\\n---\\n\")\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = mystdout = StringIO()\n",
    "        try:\n",
    "            exec(command, globals())\n",
    "            sys.stdout = old_stdout\n",
    "            output = mystdout.getvalue()\n",
    "        except Exception as e:\n",
    "            sys.stdout = old_stdout\n",
    "            output = str(e)\n",
    "        sys.stderr.write(\"PYTHON OUTPUT: \\\"\" + output + \"\\\"\\n\")\n",
    "        return output\n",
    "      \n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "python_repl = Tool(\n",
    "        \"Python REPL\",\n",
    "        PythonREPL().run,\n",
    "        \"\"\"A Python shell. Use this to execute python commands. Input should be a valid python command.\n",
    "        If you expect output it should be printed out.\"\"\",\n",
    "    )\n",
    "\n",
    "tools = [python_repl]\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "agent.run(\"What is the 10th fibonacci number?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
