{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10202d9e9a54a7aa1783202a5c493e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "import os\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "# bnb_config = transformers.BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type='nf4',\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=bfloat16\n",
    "# )\n",
    "\n",
    "# model_id = 'hyonbokan/BGPStream13-10k-cutoff-1024-max-2048'\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Need auth token for these\n",
    "hf_auth = os.environ.get('hf_token')\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2911696/1607540360.py:3: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n",
      "/tmp/ipykernel_2911696/1607540360.py:4: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm(prompt=\"Use PyBGPStream to collect update messages from 2022-01-01 13:00:00 to 2022-01-01 13:30:00 and tell me the total number of annoucements.\")\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Use PyBGPStream to collect update messages from 2022-01-01 13:00:00 to 2022-01-01 13:30:00 and tell me the total number of annoucements. \\n\\nHere is the code:\\n```python\\nimport pybgpstream\\nfrom datetime import datetime\\n\\n# Create a stream object\\nstream = pybgpstream.BGPStream(\\n    from_time=datetime(2022, 1, 1, 13, 0, 0),\\n    to_time=datetime(2022, 1, 1, 13, 30, 0),\\n    file=\"bgpstream_data.txt\"\\n)\\n\\n# Iterate over the stream\\nfor record in stream:\\n    # Check if the record is an update\\n    if record.is_update():\\n        # Get the update message\\n        update = record.get_update()\\n        # Print the total number of announcements\\n        print(\"Total announcements:\", len(update.get_announcements()))\\n\\n```\\n\\nHowever, I get an error message saying \"TypeError: \\'NoneType\\' object has no attribute \\'get_announcements\\'\". This is because `record.get_update()` returns `None` when the record is not an update message. \\n\\nHere\\'s the corrected code:\\n```python\\nimport pybgpstream\\nfrom datetime import datetime\\n\\n# Create a stream object\\nstream = pybgpstream.BGPStream(\\n    from_time=datetime(2022, 1, 1, 13, 0, 0),\\n    to_time=datetime(2022, 1, 1, 13, 30, 0),\\n    file=\"bgpstream_data.txt\"\\n)\\n\\n# Initialize the total number of announcements\\ntotal_announcements = 0\\n\\n# Iterate over the stream\\nfor record in stream:\\n    # Check if the record is an update\\n    if record.is_update():\\n        # Get the update message\\n        update = record.get_update'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=400)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "llm(prompt=\"Use PyBGPStream to collect update messages from 2022-01-01 13:00:00 to 2022-01-01 13:30:00 and tell me the total number of annoucements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    # return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    temperature=0.01,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Use PyBGPStream to analyze the BGP table statistics for AS131072, focusing on the total number of advertisements and print the result. \\n\\n```python\\nimport pybgpstream\\n\\n# Create a reader object that reads from the BGP stream file\\nreader = pybgpstream.BGPSReader(\\'path_to_your_bgp_stream_file\\')\\n\\n# Iterate over all records in the stream\\nfor record in reader.records():\\n    # Check if the record is an advertisement\\n    if isinstance(record, pybgpstream.RecordAdvertisement):\\n        # Get the AS path of the advertisement\\n        as_path = record.as_path()\\n        \\n        # Check if the AS path contains AS131072\\n        if 131072 in as_path:\\n            # Print the total number of advertisements\\n            print(\"Total number of advertisements:\", len(as_path))\\n```\\n\\nThis code will iterate through each record in the BGP stream file, check if it\\'s an advertisement, get its AS path, and then check if AS131072 is present in the AS path. If it is, it prints the total number of advertisements.\\n\\nPlease replace `\\'path_to_your_bgp_stream_file\\'` with your actual BGP stream file path.\\n\\nNote: This script assumes you have the `pybgpstream` library installed. You can install it using pip: `pip install pybgpstream`. Also, this script may take some time to run depending on the size of your BGP stream file. \\n\\nAlso note that the above code snippet does not handle any exceptions that might occur while reading or processing the BGP stream file. In a production environment, you should add proper error handling to make the script more robust. \\n\\nFinally, please be aware that running this script may require significant computational resources and memory, especially when dealing with large BGP stream files. Make sure you have enough resources available before running the script. \\n\\nIf you want to count the total number of advertisements without checking the AS path, you can simplify the code like this:\\n\\n```python\\nimport pybgpstream\\n\\n# Create a reader object that reads from the BGP stream file\\nreader = pybgpstream.BGPSReader(\\'path_to_your_bgp_stream_file\\')\\n\\n# Initialize a counter for the total number of advertisements\\ntotal_advertisements = 0\\n\\n# Iterate over all records in the stream\\nfor record in reader.records():\\n    # Check if the record is an advertisement\\n    if isinstance(record, pybgpstream.RecordAdvertisement):\\n        # Increment the counter\\n        total_advertisements += 1\\n\\n# Print the total number of advertisements\\nprint(\"Total number of advertisements:\", total_advertis'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt=\"Use PyBGPStream to analyze the BGP table statistics for AS131072, focusing on the total number of advertisements and print the result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_csv_agent(llm, \n",
    "                         '/home/hb/dataset_bgp/new_candidates/bgp_features_asn_1136_eu_leak.csv', \n",
    "                         verbose=True,\n",
    "                         agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.handle_parsing_errors = True\n",
    "agent.early_stopping_method = \"force\"\n",
    "agent.return_intermediate_steps = False\n",
    "agent.max_iterations = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Needs stop-output function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdentify changes in routing, such as withdrawals, origin changes, and route changes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "# Needs stop-output function\n",
    "agent.run(\"Identify changes in routing, such as withdrawals, origin changes, and route changes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import sys\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.agents import load_tools\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "class PythonREPL:\n",
    "    \"\"\"Simulates a standalone Python REPL.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass        \n",
    "\n",
    "    def run(self, command: str) -> str:\n",
    "        \"\"\"Run command and returns anything printed.\"\"\"\n",
    "        # sys.stderr.write(\"EXECUTING PYTHON CODE:\\n---\\n\" + command + \"\\n---\\n\")\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = mystdout = StringIO()\n",
    "        try:\n",
    "            exec(command, globals())\n",
    "            sys.stdout = old_stdout\n",
    "            output = mystdout.getvalue()\n",
    "        except Exception as e:\n",
    "            sys.stdout = old_stdout\n",
    "            output = str(e)\n",
    "        # sys.stderr.write(\"PYTHON OUTPUT: \\\"\" + output + \"\\\"\\n\")\n",
    "        return output\n",
    "    \n",
    "# llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "python_repl = Tool(\n",
    "        name=\"Python REPL\",\n",
    "        func=PythonREPL().run,\n",
    "        description=\"\"\"A Python shell. Use this to execute python commands. Input should be a valid python command.\n",
    "        If you expect output it should be printed out.\"\"\",\n",
    "    )\n",
    "\n",
    "tools = [python_repl]\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"count the total number of BGP update announcements from 2022-01-01 13:00:00 to 2022-01-01 13:30:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish, OutputParserException\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the base template\n",
    "template = \"\"\"Provide pybgpstream code script for the query. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: generate python script matching the task described in the query\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input query\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a prompt template\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        return self.template.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_repl_org = PythonREPLTool()\n",
    "# tools = [python_repl_org]\n",
    "tools = [python_repl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise OutputParserException(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "\n",
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    # return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=1028,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain consisting of the LLM and a prompt\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"],\n",
    "    allowed_tools=tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True, \n",
    "    handle_parsing_errors=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.run(\"count the total number of announcements in BGP updates from 2022-01-01 13:00:00 to 2022-01-01 13:30:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Announcements: 1002005\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pybgpstream\n",
    "\n",
    "# Initialize the stream\n",
    "stream = pybgpstream.BGPStream(\n",
    "    from_time=\"2022-01-01 13:00:00\", until_time=\"2022-01-01 13:30:00\",\n",
    "    collectors=[\"route-views.linx\"],\n",
    "    record_type=\"updates\"\n",
    ")\n",
    "\n",
    "# Create a dictionary to store the counts\n",
    "announcement_counts = 0\n",
    "\n",
    "# Loop through the stream records\n",
    "for rec in stream.records():\n",
    "    for elem in rec:\n",
    "        if elem.type == \"A\":\n",
    "            # Increment the count for each announcement\n",
    "            announcement_counts += 1\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total Announcements: {announcement_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import sys\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.agents import load_tools\n",
    "\n",
    "class PythonREPL:\n",
    "    \"\"\"Simulates a standalone Python REPL.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass        \n",
    "\n",
    "    def run(self, command: str) -> str:\n",
    "        \"\"\"Run command and returns anything printed.\"\"\"\n",
    "        sys.stderr.write(\"EXECUTING PYTHON CODE:\\n---\\n\" + command + \"\\n---\\n\")\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = mystdout = StringIO()\n",
    "        try:\n",
    "            exec(command, globals())\n",
    "            sys.stdout = old_stdout\n",
    "            output = mystdout.getvalue()\n",
    "        except Exception as e:\n",
    "            sys.stdout = old_stdout\n",
    "            output = str(e)\n",
    "        sys.stderr.write(\"PYTHON OUTPUT: \\\"\" + output + \"\\\"\\n\")\n",
    "        return output\n",
    "      \n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "python_repl = Tool(\n",
    "        \"Python REPL\",\n",
    "        PythonREPL().run,\n",
    "        \"\"\"A Python shell. Use this to execute python commands. Input should be a valid python command.\n",
    "        If you expect output it should be printed out.\"\"\",\n",
    "    )\n",
    "\n",
    "tools = [python_repl]\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "agent.run(\"What is the 10th fibonacci number?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
