{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuned Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hb/myenv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e47127c9752407f9e0bf31579990c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "import transformers\n",
    "import os\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "# model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "# model_id = 'codellama/CodeLlama-7b-hf'\n",
    "# model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "new_model = \"/home/hb/dataset_bgp/llm_finetuned/llama2-7b-table283New-10split-5k-instruct-1e5rate-loraa64drop01\"\n",
    "\n",
    "hf_auth = os.environ.get('hf_token')\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hb/myenv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0adcd622cd1442db3ada5be4f662653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "import transformers\n",
    "import os\n",
    "\n",
    "model_id = \"hyonbokan/BGP-LLaMA13-BGPStream5k-cutoff-1024-max-2048-fpFalse\"\n",
    "\n",
    "# Need auth token for these\n",
    "hf_auth = os.environ.get('hf_token')\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tablular_eval_util import combine_csv_files, split_dataframe, preprocess_data, run_llm_inference\n",
    "\n",
    "directory = '/home/hb/dataset_bgp/bgp_tab_dataset_test'\n",
    "combined_df = combine_csv_files(directory)\n",
    "\n",
    "if 'anomaly_status' in combined_df.columns:\n",
    "    combined_df = combined_df.drop(columns=['anomaly_status'])\n",
    "    \n",
    "# Split the DataFrame into smaller chunks\n",
    "split_size = 20\n",
    "data_list = split_dataframe(combined_df, split_size)\n",
    "\n",
    "# Preprocess the data into the required format\n",
    "formatted_data = [preprocess_data(chunk) for chunk in data_list]\n",
    "\n",
    "formatted_data_file = f'llm_table_bgp_data_test_{split_size}.json'\n",
    "with open(formatted_data_file, 'w') as f:\n",
    "    json.dump(formatted_data, f, indent=4)\n",
    "\n",
    "with open(formatted_data_file, 'r') as f:\n",
    "    formatted_data = json.load(f)\n",
    "\n",
    "output_results_file = f'table135-20split-2k-with-outputs-{split_size}.json'\n",
    "run_llm_inference(formatted_data, model, tokenizer, max_length=2050, output_results_file=output_results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from tablular_eval_util import combine_csv_files, split_dataframe, preprocess_data, run_llm_inference, shuffle_and_split_dataframe\n",
    "\n",
    "# directory = '/home/hb/dataset_bgp/bgp_tab_dataset_test'\n",
    "# combined_df = combine_csv_files(directory)\n",
    "\n",
    "# if 'anomaly_status' in combined_df.columns:\n",
    "#     combined_df = combined_df.drop(columns=['anomaly_status'])\n",
    "    \n",
    "# # Split the DataFrame into smaller chunks\n",
    "# split_size = 20\n",
    "# data_list = shuffle_and_split_dataframe(combined_df, split_size)\n",
    "\n",
    "# # Preprocess the data into the required format\n",
    "# formatted_data = [preprocess_data(chunk) for chunk in data_list]\n",
    "\n",
    "# formatted_data_file = f'llm_table_bgp_data_test_{split_size}_shuffled.json'\n",
    "# with open(formatted_data_file, 'w') as f:\n",
    "#     json.dump(formatted_data, f, indent=4)\n",
    "\n",
    "# with open(formatted_data_file, 'r') as f:\n",
    "#     formatted_data = json.load(f)\n",
    "\n",
    "# output_results_file = f'table135-20split-2k-with-outputs-{split_size}_shuffled.json'\n",
    "# run_llm_inference(formatted_data, model, tokenizer, max_length=3050, output_results_file=output_results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022-03-28 07:10:00', '2022-03-28 10:15:00', '2022-03-28 10:15:00', '2022-03-28 10:25:00', '2022-03-28 10:30:00', '2022-03-28 10:35:00', '2022-03-28 10:45:00', '2022-03-28 13:25:00', '2022-03-28 13:45:00', '2022-03-28 13:55:00', '2022-03-28 14:05:00', '2022-03-28 14:15:00', '2022-03-28 14:25:00', '2022-03-28 14:35:00', '2022-03-28 14:45:00', '2022-03-28 16:45:00', '2022-03-28 16:45:00', '2019-05-08 14:55:00', '2018-11-12 00:05:00', '2018-11-12 00:10:00', '2018-11-12 00:15:00', '2018-11-12 00:20:00', '2018-11-12 01:15:00', '2018-11-12 02:05:00', '2018-11-12 02:10:00', '2018-11-12 02:15:00', '2018-11-12 02:20:00', '2018-11-12 03:00:00', '2018-11-12 03:10:00', '2018-11-12 03:55:00', '2018-11-12 04:00:00', '2018-11-12 04:15:00', '2018-11-12 05:05:00', '2018-11-12 05:10:00', '2018-11-12 05:15:00', '2018-11-12 05:20:00', '2018-11-12 05:25:00', '2018-11-12 05:30:00', '2018-11-12 05:35:00', '2018-11-12 06:35:00', '2018-11-12 07:00:00', '2018-11-12 07:10:00', '2018-11-12 07:20:00', '2018-11-12 07:55:00', '2018-11-12 08:00:00', '2018-11-12 08:05:00', '2018-11-12 09:45:00', '2018-11-12 10:00:00', '2018-11-12 10:10:00', '2018-11-12 10:20:00', '2018-11-12 12:05:00', '2018-11-12 13:45:00', '2018-11-12 14:20:00', '2018-11-12 14:25:00', '2018-11-12 13:45:00', '2018-11-12 14:20:00', '2018-11-12 15:40:00', '2018-11-12 15:45:00', '2018-11-12 15:55:00', '2018-11-12 16:00:00', '2018-11-12 16:35:00', '2018-11-12 16:45:00', '2018-11-12 17:00:00', '2018-11-12 17:25:00', '2018-11-12 18:20:00', '2018-11-12 18:25:00', '2018-11-12 18:30:00', '2018-11-12 18:35:00', '2018-11-12 20:45:00', '2018-11-12 20:55:00', '2018-11-12 21:00:00', '2018-11-12 21:15:00', '2018-11-12 21:20:00', '2018-11-12 21:35:00', '2018-11-12 22:55:00', '2018-11-12 23:00:00', '2018-11-12 23:05:00', '2018-11-12 23:10:00', '2018-11-13 00:45:00', '2018-11-13 01:00:00', '2018-11-13 01:25:00', '2018-11-13 01:45:00', '2018-11-13 00:45:00', '2018-11-13 01:00:00', '2018-11-13 01:25:00', '2018-11-13 02:15:00', '2018-11-13 02:20:00', '2018-11-13 02:30:00', '2018-11-13 02:35:00', '2018-11-13 02:45:00', '2018-11-13 04:00:00', '2018-11-13 04:10:00', '2018-11-13 05:35:00', '2018-11-13 05:40:00', '2017-12-12 04:40:00', '2017-12-12 06:40:00', '2017-12-12 07:35:00', '2017-12-12 07:40:00']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "output_file = pd.read_json(f\"/home/hb/LLM-research/finetune_main/finetuning_tabular/table_read/table135-20split-2k-with-outputs-{split_size}.json\")\n",
    "output_file = output_file[\"output\"]\n",
    "output_file.to_csv(f\"table135-20split-2k-outputs-{split_size}.csv\")\n",
    "\n",
    "timestamps = []\n",
    "\n",
    "timestamp_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}')\n",
    "\n",
    "for output in output_file:\n",
    "    matches = timestamp_pattern.findall(output)\n",
    "    timestamps.extend(matches)\n",
    "\n",
    "bgp_llm_output = list(set(timestamps))\n",
    "\n",
    "print(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# gpt4o_test = pd.read_csv(\"/home/hb/LLM-research/finetune_main/finetuning_tabular/table_read/gpt4o_test_all.csv\")\n",
    "# gpt4o_test = gpt4o_test[\"timestamp\"].tolist()\n",
    "# print(gpt4o_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the dataset\n",
    "# file_path = '/home/hb/dataset_bgp/test_all.csv'\n",
    "# data = pd.read_csv(file_path)\n",
    "\n",
    "# # Calculate summary statistics\n",
    "# summary_stats = data.describe()\n",
    "\n",
    "# # Identifying potential anomalies using the 3-sigma rule\n",
    "# anomalous_rows = pd.DataFrame()\n",
    "\n",
    "# for column in data.columns[3:]:\n",
    "#     if column in summary_stats.columns:\n",
    "#         threshold = summary_stats.loc['mean', column] + 3 * summary_stats.loc['std', column]\n",
    "#         anomalies = data[data[column] > threshold]\n",
    "#         anomalous_rows = pd.concat([anomalous_rows, anomalies])\n",
    "\n",
    "# # Remove duplicates\n",
    "# anomalous_rows = anomalous_rows.drop_duplicates()\n",
    "\n",
    "# # Save the anomalous timestamps to a CSV file\n",
    "# anomalous_timestamps = anomalous_rows['timestamp'].tolist()\n",
    "# print(anomalous_timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022-03-28 10:25:00', '2022-03-28 10:30:00', '2022-03-28 10:35:00', '2022-03-28 10:40:00', '2022-03-28 10:55:00', '2022-03-28 11:00:00', '2022-03-28 11:35:00', '2022-03-28 11:40:00', '2022-03-28 11:45:00', '2022-03-28 13:25:00', '2022-03-28 13:30:00', '2022-03-28 13:35:00', '2022-03-28 14:00:00', '2022-03-28 14:05:00', '2022-03-28 14:15:00', '2022-03-28 14:20:00', '2022-03-28 14:25:00', '2022-03-28 15:55:00', '2019-05-08 15:05:00', '2019-05-08 15:10:00', '2019-05-08 15:15:00', '2019-05-08 15:30:00', '2018-11-12 01:25:00', '2018-11-12 01:30:00', '2018-11-12 02:05:00', '2018-11-12 02:10:00', '2018-11-12 04:20:00', '2018-11-12 04:25:00', '2018-11-12 07:50:00', '2018-11-12 07:55:00', '2018-11-12 08:05:00', '2018-11-12 08:10:00', '2018-11-12 08:45:00', '2018-11-12 08:50:00', '2018-11-12 08:55:00', '2018-11-12 09:25:00', '2018-11-12 09:50:00', '2018-11-12 10:00:00', '2018-11-12 13:25:00', '2018-11-12 14:20:00', '2018-11-12 14:25:00', '2018-11-12 15:40:00', '2018-11-12 15:45:00', '2018-11-12 16:40:00', '2018-11-12 16:45:00', '2018-11-12 16:55:00', '2018-11-12 17:00:00', '2018-11-12 18:25:00', '2018-11-12 18:30:00', '2018-11-12 20:40:00', '2018-11-12 20:45:00', '2018-11-12 20:50:00', '2018-11-12 21:10:00', '2018-11-12 21:15:00', '2018-11-12 21:20:00', '2018-11-12 21:25:00', '2018-11-12 21:30:00', '2018-11-12 22:55:00', '2018-11-12 23:00:00', '2018-11-13 00:10:00', '2018-11-13 04:15:00', '2018-11-13 04:20:00', '2018-11-13 04:25:00', '2018-11-13 04:30:00', '2018-11-13 04:35:00', '2018-11-13 05:40:00', '2017-12-12 04:40:00', '2017-12-12 04:45:00', '2017-12-12 04:50:00', '2017-12-12 07:05:00', '2017-12-12 07:10:00']\n"
     ]
    }
   ],
   "source": [
    "from tablular_eval_util import combine_csv_files\n",
    "\n",
    "directory = '/home/hb/dataset_bgp/bgp_tab_dataset_test'\n",
    "combined_df = combine_csv_files(directory)\n",
    "combined_df = combined_df[['anomaly_status']]\n",
    "\n",
    "# Filter the rows with \"anomaly detected\" in the anomaly_status\n",
    "filtered_df = combined_df[combined_df['anomaly_status'].str.contains('anomaly detected', na=False)]\n",
    "# filtered_df.to_csv('/home/hb/dataset_bgp/bgp_tab_dataset_test/test_true_label.csv', index=False)\n",
    "\n",
    "# Extract the date from the anomaly_status string\n",
    "true_label = filtered_df['anomaly_status'].str.extract(r'anomaly detected at (\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})')\n",
    "\n",
    "# Convert the dates to a list\n",
    "true_label = true_label[0].tolist()\n",
    "\n",
    "print(true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.30\n",
      "Recall: 0.38\n",
      "F1 Score: 0.33\n",
      "True Positives: 27\n",
      "False Positives: 64\n",
      "False Negatives: 44\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tablular_eval_util import evaluate_llm_results\n",
    "\n",
    "evaluation_result = evaluate_llm_results(true_anomalies=true_label, llm_results=bgp_llm_output)\n",
    "# print(f\"Evaluation Results {split_size} split:\")\n",
    "print(f\"Precision: {evaluation_result['precision']:.2f}\")\n",
    "print(f\"Recall: {evaluation_result['recall']:.2f}\")\n",
    "print(f\"F1 Score: {evaluation_result['f1_score']:.2f}\")\n",
    "print(f\"True Positives: {evaluation_result['true_positives']}\")\n",
    "print(f\"False Positives: {evaluation_result['false_positives']}\")\n",
    "print(f\"False Negatives: {evaluation_result['false_negatives']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General BGP Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Perform BGP analysis with the given data below: \\n [TLE] The section is related to a specific time period of BGP monitoring. [TAB] col: | timestamp | asn | num_routes | num_new_routes | num_withdrawals | num_origin_changes | num_route_changes | max_path_length | avg_path_length | max_edit_distance | avg_edit_distance | num_announcements | num_unique_prefixes_announced | row 1: | 2022-03-28 07:00:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 2: | 2022-03-28 07:05:00 | 8342 | 7 | 7 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 19 | 7 | [SEP] row 3: | 2022-03-28 07:10:00 | 8342 | 0 | 0 | 7 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 4: | 2022-03-28 07:15:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 5: | 2022-03-28 07:20:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 6: | 2022-03-28 07:25:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 7: | 2022-03-28 07:30:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 8: | 2022-03-28 07:35:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 9: | 2022-03-28 07:40:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 10: | 2022-03-28 07:45:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 11: | 2022-03-28 07:50:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 12: | 2022-03-28 07:55:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 13: | 2022-03-28 08:00:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 14: | 2022-03-28 08:05:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 15: | 2022-03-28 08:10:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 16: | 2022-03-28 08:15:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 17: | 2022-03-28 08:20:00 | 8342 | 7 | 7 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 19 | 7 | [SEP] row 18: | 2022-03-28 08:25:00 | 8342 | 0 | 0 | 7 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 19: | 2022-03-28 08:30:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 20: | 2022-03-28 08:35:00 | 8342 | 1 | 1 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 1 | 1 | [SEP]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from tablular_eval_util import combine_csv_files, split_dataframe, preprocess_data, run_llm_inference\n",
    "\n",
    "directory = '/home/hb/dataset_bgp/bgp_tab_dataset_test'\n",
    "combined_df = combine_csv_files(directory)\n",
    "\n",
    "if 'anomaly_status' in combined_df.columns:\n",
    "    combined_df = combined_df.drop(columns=['anomaly_status'])\n",
    "    \n",
    "# Split the DataFrame into smaller chunks\n",
    "split_size = 20\n",
    "data_list = split_dataframe(combined_df, split_size)\n",
    "\n",
    "\n",
    "# Preprocess the data into the required format\n",
    "formatted_data = [preprocess_data(chunk) for chunk in data_list]\n",
    "input_test = formatted_data[0]['input_seg']\n",
    "prompt = f\"Perform BGP analysis with the given data below: \\n {input_test}\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Perform BGP analysis with the given data below: \n",
      " [TLE] The section is related to a specific time period of BGP monitoring. [TAB] col: | timestamp | asn | num_routes | num_new_routes | num_withdrawals | num_origin_changes | num_route_changes | max_path_length | avg_path_length | max_edit_distance | avg_edit_distance | num_announcements | num_unique_prefixes_announced | row 1: | 2022-03-28 07:00:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 2: | 2022-03-28 07:05:00 | 8342 | 7 | 7 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 19 | 7 | [SEP] row 3: | 2022-03-28 07:10:00 | 8342 | 0 | 0 | 7 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 4: | 2022-03-28 07:15:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 5: | 2022-03-28 07:20:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 6: | 2022-03-28 07:25:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 7: | 2022-03-28 07:30:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 8: | 2022-03-28 07:35:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 9: | 2022-03-28 07:40:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 10: | 2022-03-28 07:45:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 11: | 2022-03-28 07:50:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 12: | 2022-03-28 07:55:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 13: | 2022-03-28 08:00:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 14: | 2022-03-28 08:05:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 15: | 2022-03-28 08:10:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 16: | 2022-03-28 08:15:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 17: | 2022-03-28 08:20:00 | 8342 | 7 | 7 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 19 | 7 | [SEP] row 18: | 2022-03-28 08:25:00 | 8342 | 0 | 0 | 7 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 19: | 2022-03-28 08:30:00 | 8342 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | [SEP] row 20: | 2022-03-28 08:35:00 | 8342 | 1 | 1 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 1 | 1 | [SEP] [/INST]  Sure! Here's the BGP analysis for the given data:\n",
      "\n",
      "1. Number of routes:\n",
      "The number of routes announced by ASN 8342 during the given time period is 7.\n",
      "2. Number of new routes:\n",
      "The number of new routes announced by ASN 8342 during the given time period is 7.\n",
      "3. Number of withdrawals:\n",
      "The number of withdrawals announced by ASN 8342 during the given time period is 0.\n",
      "4. Number of origin changes:\n",
      "The number of origin changes announced by ASN 8342 during the given time period is 0.\n",
      "5. Number of route changes:\n",
      "The number of route changes announced by ASN 8342 during the given time period is 0.\n",
      "6. Maximum path length:\n",
      "The maximum path length observed in the given time period is 19.\n",
      "7. Average path length:\n",
      "The average path length observed in the given time period is 0.0.\n",
      "8. Maximum edit distance:\n",
      "The maximum edit distance observed in the given time period is 19.\n",
      "9. Average edit distance:\n",
      "The average edit distance observed in the given time period is 0.0.\n",
      "10. Number of announcements:\n",
      "The number of announcements observed in the given time period is 19.\n",
      "11. Number of unique prefixes announced:\n",
      "The number of unique prefixes announced by ASN 8342 during the given time period is 7.\n",
      "\n",
      "Based on the analysis, we can see that ASN 8342 announced 7 new routes during the given time period, with a maximum path length of 19 and an average path length of 0.0. There were no origin changes, route changes, or withdrawals observed during the given time period. The maximum edit distance observed was 19, with an average edit distance of 0.0. Finally, there were 19 announcements and 7 unique prefixes announced by ASN 8342 during the given time period\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2050)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
