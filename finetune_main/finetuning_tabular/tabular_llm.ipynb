{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-llms-huggingface\n",
    "# %pip install llama-index\n",
    "# %pip install llama-index-finetuning\n",
    "# !mkdir data && wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Citation(BaseModel):\n",
    "    \"\"\"Citation class.\"\"\"\n",
    "\n",
    "    author: str = Field(\n",
    "        ..., description=\"Inferred first author (usually last name\"\n",
    "    )\n",
    "    year: int = Field(..., description=\"Inferred year\")\n",
    "    desc: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Inferred description from the text of the work that the author is\"\n",
    "            \" cited for\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"List of author citations.\n",
    "\n",
    "    Extracted over unstructured text.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    citations: List[Citation] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"List of author citations (organized by author, year, and\"\n",
    "            \" description).\"\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'field_validator' from 'pydantic' (/home/hb/myenv/lib/python3.8/site-packages/pydantic/__init__.cpython-38-x86_64-linux-gnu.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnode_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleNodeParser\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceLLM\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
      "File \u001b[0;32m~/myenv/lib/python3.8/site-packages/llama_index/llms/huggingface/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     HuggingFaceInferenceAPI,\n\u001b[1;32m      3\u001b[0m     HuggingFaceLLM,\n\u001b[1;32m      4\u001b[0m     TextGenerationInference,\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFaceLLM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFaceInferenceAPI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextGenerationInference\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/myenv/lib/python3.8/site-packages/llama_index/llms/huggingface/base.py:48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentChatResponse\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTool\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     49\u001b[0m     to_tgi_messages,\n\u001b[1;32m     50\u001b[0m     force_single_tool_call,\n\u001b[1;32m     51\u001b[0m     resolve_tgi_function_call,\n\u001b[1;32m     52\u001b[0m     get_max_input_length,\n\u001b[1;32m     53\u001b[0m     resolve_tool_choice,\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     56\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m     57\u001b[0m     AutoTokenizer,\n\u001b[1;32m     58\u001b[0m     StoppingCriteria,\n\u001b[1;32m     59\u001b[0m     StoppingCriteriaList,\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtext_generation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     62\u001b[0m     Client \u001b[38;5;28;01mas\u001b[39;00m TGIClient,\n\u001b[1;32m     63\u001b[0m     AsyncClient \u001b[38;5;28;01mas\u001b[39;00m TGIAsyncClient,\n\u001b[1;32m     64\u001b[0m )\n",
      "File \u001b[0;32m~/myenv/lib/python3.8/site-packages/llama_index/llms/huggingface/utils.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequence, Union, List, Optional\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     ChatMessage,\n\u001b[1;32m      6\u001b[0m     ChatResponse,\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtext_generation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Message,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve_tgi_function_call\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     14\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/info\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/myenv/lib/python3.8/site-packages/text_generation/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.6.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtext_generation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client, AsyncClient\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtext_generation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InferenceAPIClient, InferenceAPIAsyncClient\n",
      "File \u001b[0;32m~/myenv/lib/python3.8/site-packages/text_generation/client.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ValidationError\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, List, AsyncIterator, Iterator, Union\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtext_generation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     StreamResponse,\n\u001b[1;32m     10\u001b[0m     Response,\n\u001b[1;32m     11\u001b[0m     Request,\n\u001b[1;32m     12\u001b[0m     Parameters,\n\u001b[1;32m     13\u001b[0m     Grammar,\n\u001b[1;32m     14\u001b[0m     ChatRequest,\n\u001b[1;32m     15\u001b[0m     ChatCompletionChunk,\n\u001b[1;32m     16\u001b[0m     ChatComplete,\n\u001b[1;32m     17\u001b[0m     Message,\n\u001b[1;32m     18\u001b[0m     Tool,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtext_generation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_error\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mClient\u001b[39;00m:\n",
      "File \u001b[0;32m~/myenv/lib/python3.8/site-packages/text_generation/types.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, field_validator\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, List, Union, Any\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtext_generation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ValidationError\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'field_validator' from 'pydantic' (/home/hb/myenv/lib/python3.8/site-packages/pydantic/__init__.cpython-38-x86_64-linux-gnu.so)"
     ]
    }
   ],
   "source": [
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from pathlib import Path\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "import torch\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFReader()\n",
    "docs0 = loader.load(file_path=Path(\"./data/llama2.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n",
    "metadata = {\n",
    "    \"paper_title\": \"Llama 2: Open Foundation and Fine-Tuned Chat Models\"\n",
    "}\n",
    "docs = [Document(text=doc_text, metadata=metadata)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1024\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=chunk_size)\n",
    "nodes = node_parser.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    device_map=\"auto\",\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
    ")\n",
    "base_llm.pydantic_program_mode = \"llm\"\n",
    "\n",
    "# setup eval context (for question generation)\n",
    "eval_llm = OpenAI(model=\"gpt-4-0613\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 40\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(nodes[:\u001b[38;5;241m39\u001b[39m]):\n\u001b[1;32m     35\u001b[0m     num_questions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# change this number to increase number of nodes\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     dataset_generator \u001b[38;5;241m=\u001b[39m DatasetGenerator(\n\u001b[1;32m     37\u001b[0m         [node],\n\u001b[1;32m     38\u001b[0m         question_gen_query\u001b[38;5;241m=\u001b[39mquestion_gen_query,\n\u001b[1;32m     39\u001b[0m         text_question_template\u001b[38;5;241m=\u001b[39mquestion_gen_prompt,\n\u001b[0;32m---> 40\u001b[0m         llm\u001b[38;5;241m=\u001b[39m\u001b[43meval_llm\u001b[49m,\n\u001b[1;32m     41\u001b[0m         metadata_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     42\u001b[0m         num_questions_per_chunk\u001b[38;5;241m=\u001b[39mnum_questions,\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     45\u001b[0m     task \u001b[38;5;241m=\u001b[39m dataset_generator\u001b[38;5;241m.\u001b[39magenerate_questions_from_nodes(num\u001b[38;5;241m=\u001b[39mnum_questions)\n\u001b[1;32m     46\u001b[0m     node_questions_tasks\u001b[38;5;241m.\u001b[39mappend(task)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_llm' is not defined"
     ]
    }
   ],
   "source": [
    "# setup dataset generator\n",
    "from llama_index.core.evaluation import DatasetGenerator\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core import PromptTemplate\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "\n",
    "fp = open(\"data/qa_pairs.jsonl\", \"w\")\n",
    "\n",
    "question_gen_prompt = PromptTemplate(\n",
    "    \"\"\"\n",
    "{query_str}\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Questions:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "question_gen_query = \"\"\"\\\n",
    "Snippets from a research paper is given below. It contains citations.\n",
    "Please generate questions from the text asking about these citations.\n",
    "\n",
    "For instance, here are some sample questions:\n",
    "Which citations correspond to related works on transformer models? \n",
    "Tell me about authors that worked on advancing RLHF.\n",
    "Can you tell me citations corresponding to all computer vision works? \\\n",
    "\"\"\"\n",
    "\n",
    "qr_pairs = []\n",
    "node_questions_tasks = []\n",
    "for idx, node in enumerate(nodes[:39]):\n",
    "    num_questions = 1  # change this number to increase number of nodes\n",
    "    dataset_generator = DatasetGenerator(\n",
    "        [node],\n",
    "        question_gen_query=question_gen_query,\n",
    "        text_question_template=question_gen_prompt,\n",
    "        llm=eval_llm,\n",
    "        metadata_mode=\"all\",\n",
    "        num_questions_per_chunk=num_questions,\n",
    "    )\n",
    "\n",
    "    task = dataset_generator.agenerate_questions_from_nodes(num=num_questions)\n",
    "    node_questions_tasks.append(task)\n",
    "node_questions_lists = await tqdm_asyncio.gather(*node_questions_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
