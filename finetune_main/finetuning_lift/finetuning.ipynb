{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from typing import List\n",
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "from datasets import load_dataset\n",
    "import os\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# import seaborn as sns\n",
    "from pylab import rcParams\n",
    "# from trl import SFTTrainer\n",
    " \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hb/myenv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e42e9c082546578fadf00d657c3ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "# model_id = 'codellama/CodeLlama-7b-hf'\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Need auth token for these\n",
    "hf_auth = os.environ.get('hf_token')\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "# device_map = {\"\": 0}\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    " \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/hb/.cache/huggingface/datasets/json/default-f5d2c046a3af28bd/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc20872d41548d2bcfebd1ad9644aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['timestamp', 'nb_A', 'nb_W', 'nb_implicit_W', 'nb_dup_A', 'nb_dup_W', 'nb_A_prefix', 'nb_W_prefix', 'max_A_prefix', 'avg_A_prefix', 'max_A_AS', 'avg_A_AS', 'nb_orign_change', 'nb_new_A', 'nb_new_A_afterW', 'max_path_len', 'avg_path_len', 'max_editdist', 'avg_editdist', 'editdist_7', 'editdist_8', 'editdist_9', 'editdist_10', 'editdist_11', 'editdist_12', 'editdist_13', 'editdist_14', 'editdist_15', 'editdist_16', 'editdist_17', 'nb_tolonger', 'nb_toshorter', 'avg_interarrival', 'anomaly_status'],\n",
       "    num_rows: 78\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files=\"/home/hb/LLM-research/dataset/BGP/BGP_lift/lift_bgp_test1.json\")\n",
    "data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_LEN = 2048\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    # Exclude the 'anomaly_status' from the input data_point\n",
    "    input_data = {key: value for key, value in data_point.items() if key != 'anomaly_status'}\n",
    "    ground_truth = data_point['anomaly_status']\n",
    "    \n",
    "    return f\"\"\"Below is BGP update message data. The task is to determine whether the BGP data indicates an anomaly. The response should be an alert indicating whether it is an anomaly or normal.  # noqa: E501\n",
    "### Input:\n",
    "{input_data}\n",
    "### Response:\n",
    "{ground_truth}\"\"\"\n",
    " \n",
    " \n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    " \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    " \n",
    "    return result\n",
    " \n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/hb/.cache/huggingface/datasets/json/default-f5d2c046a3af28bd/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-eef7a50a59528d8e.arrow and /home/hb/.cache/huggingface/datasets/json/default-f5d2c046a3af28bd/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-133408cb35f59f4e.arrow\n",
      "Loading cached processed dataset at /home/hb/.cache/huggingface/datasets/json/default-f5d2c046a3af28bd/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-b2c92f3b262e86c2.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2d9cf17248437abdfec3b832ce2993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=10, shuffle=True, seed=42\n",
    ")\n",
    "train_data = (\n",
    "    train_val[\"train\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "val_data = (\n",
    "    train_val[\"test\"].map(generate_and_tokenize_prompt)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 52,428,800 || all params: 13,068,293,120 || trainable%: 0.40119087870597137\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    " \n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "output_dir = \"./hyonbo/NLP-BGP-LLaMA13b\"\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 1\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 200\n",
    "logging_steps = 500\n",
    "learning_rate = 1e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 2000\n",
    "warmup_ratio = 0.05\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    num_train_epochs=3.0\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/hb/.cache/huggingface/datasets/json/default-f5d2c046a3af28bd/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-3493477d09028cc1.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e7eb9361234472a1235ac1d0992449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmkkanhb\u001b[0m (\u001b[33mdnlab_2023\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hb/LLM-research/finetune_main/finetuning_lift/wandb/run-20240528_141545-ckq08osk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dnlab_2023/huggingface/runs/ckq08osk' target=\"_blank\">amber-wind-108</a></strong> to <a href='https://wandb.ai/dnlab_2023/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dnlab_2023/huggingface' target=\"_blank\">https://wandb.ai/dnlab_2023/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dnlab_2023/huggingface/runs/ckq08osk' target=\"_blank\">https://wandb.ai/dnlab_2023/huggingface/runs/ckq08osk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 42:13, Epoch 117/118]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.155800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.3736383399963379, metrics={'train_runtime': 2539.3028, 'train_samples_per_second': 3.15, 'train_steps_per_second': 0.788, 'total_flos': 2.018421644967936e+16, 'train_loss': 0.3736383399963379, 'epoch': 117.65})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"anomaly_status\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nl-llama13b-2k/tokenizer_config.json',\n",
       " 'nl-llama13b-2k/special_tokens_map.json',\n",
       " 'nl-llama13b-2k/tokenizer.model',\n",
       " 'nl-llama13b-2k/added_tokens.json',\n",
       " 'nl-llama13b-2k/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = \"nl-llama13b-2k\"\n",
    "\n",
    "trainer.model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hb/myenv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13d5233c18b4766b510f638b4bcdb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "import transformers\n",
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "import os\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "# model_id = 'codellama/CodeLlama-7b-hf'\n",
    "new_model = \"nl-llama13b-2k\"\n",
    "\n",
    "hf_auth = os.environ.get('hf_token')\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "\n",
    "# # Reload model in FP16 and merge it with LoRA weights\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     return_dict=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map='auto',\n",
    "# )\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generate timestamps\n",
    "start_time = datetime(2017, 8, 25, 3, 15, 0)\n",
    "timestamps = [start_time + timedelta(minutes=5 * i) for i in range(5)]\n",
    "\n",
    "# Define normal data and one anomaly\n",
    "data = [\n",
    "    {\n",
    "        \"timestamp\": \"2017-08-25T03:25:00.000\",\n",
    "        \"nb_A\": 3080,\n",
    "        \"nb_W\": 309,\n",
    "        \"nb_implicit_W\": 2199,\n",
    "        \"nb_dup_A\": 320,\n",
    "        \"nb_dup_W\": 0,\n",
    "        \"nb_A_prefix\": 686,\n",
    "        \"nb_W_prefix\": 181,\n",
    "        \"max_A_prefix\": 55,\n",
    "        \"avg_A_prefix\": 4,\n",
    "        \"max_A_AS\": 314,\n",
    "        \"avg_A_AS\": 16,\n",
    "        \"nb_orign_change\": 210,\n",
    "        \"nb_new_A\": 391,\n",
    "        \"nb_new_A_afterW\": 170,\n",
    "        \"max_path_len\": 15,\n",
    "        \"avg_path_len\": 6,\n",
    "        \"max_editdist\": 11,\n",
    "        \"avg_editdist\": 3,\n",
    "        \"editdist_7\": 250,\n",
    "        \"editdist_8\": 25,\n",
    "        \"editdist_9\": 44,\n",
    "        \"editdist_10\": 1,\n",
    "        \"editdist_11\": 1,\n",
    "        \"editdist_12\": 0,\n",
    "        \"editdist_13\": 0,\n",
    "        \"editdist_14\": 0,\n",
    "        \"editdist_15\": 0,\n",
    "        \"editdist_16\": 0,\n",
    "        \"editdist_17\": 0,\n",
    "        \"nb_tolonger\": 889,\n",
    "        \"nb_toshorter\": 1310,\n",
    "        \"avg_interarrival\": 1116,\n",
    "        \"anomaly_status\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"timestamp\": \"2017-08-25T03:35:00.000\",\n",
    "        \"nb_A\": 86458,\n",
    "        \"nb_W\": 356,\n",
    "        \"nb_implicit_W\": 3790,\n",
    "        \"nb_dup_A\": 7500,\n",
    "        \"nb_dup_W\": 0,\n",
    "        \"nb_A_prefix\": 75578,\n",
    "        \"nb_W_prefix\": 203,\n",
    "        \"max_A_prefix\": 46,\n",
    "        \"avg_A_prefix\": 1,\n",
    "        \"max_A_AS\": 19182,\n",
    "        \"avg_A_AS\": 38,\n",
    "        \"nb_orign_change\": 182,\n",
    "        \"nb_new_A\": 75028,\n",
    "        \"nb_new_A_afterW\": 140,\n",
    "        \"max_path_len\": 21,\n",
    "        \"avg_path_len\": 4,\n",
    "        \"max_editdist\": 14,\n",
    "        \"avg_editdist\": 3,\n",
    "        \"editdist_7\": 243,\n",
    "        \"editdist_8\": 58,\n",
    "        \"editdist_9\": 75,\n",
    "        \"editdist_10\": 69,\n",
    "        \"editdist_11\": 9,\n",
    "        \"editdist_12\": 13,\n",
    "        \"editdist_13\": 0,\n",
    "        \"editdist_14\": 1,\n",
    "        \"editdist_15\": 0,\n",
    "        \"editdist_16\": 0,\n",
    "        \"editdist_17\": 0,\n",
    "        \"nb_tolonger\": 1180,\n",
    "        \"nb_toshorter\": 2610,\n",
    "        \"avg_interarrival\": 1087,\n",
    "        # \"anomaly_status\": \"anomaly due to high value of nb_A, nb_A_prefix, avg_A_prefix, max_A_AS, avg_A_AS, nb_new_A, editdist_9\"\n",
    "    },\n",
    "    {\n",
    "        \"timestamp\": \"2017-08-25T04:00:00.000\",\n",
    "        \"nb_A\": 4311,\n",
    "        \"nb_W\": 401,\n",
    "        \"nb_implicit_W\": 3069,\n",
    "        \"nb_dup_A\": 625,\n",
    "        \"nb_dup_W\": 0,\n",
    "        \"nb_A_prefix\": 1030,\n",
    "        \"nb_W_prefix\": 165,\n",
    "        \"max_A_prefix\": 54,\n",
    "        \"avg_A_prefix\": 4,\n",
    "        \"max_A_AS\": 305,\n",
    "        \"avg_A_AS\": 20,\n",
    "        \"nb_orign_change\": 206,\n",
    "        \"nb_new_A\": 367,\n",
    "        \"nb_new_A_afterW\": 250,\n",
    "        \"max_path_len\": 19,\n",
    "        \"avg_path_len\": 6,\n",
    "        \"max_editdist\": 17,\n",
    "        \"avg_editdist\": 3,\n",
    "        \"editdist_7\": 202,\n",
    "        \"editdist_8\": 23,\n",
    "        \"editdist_9\": 36,\n",
    "        \"editdist_10\": 2,\n",
    "        \"editdist_11\": 5,\n",
    "        \"editdist_12\": 7,\n",
    "        \"editdist_13\": 1,\n",
    "        \"editdist_14\": 1,\n",
    "        \"editdist_15\": 0,\n",
    "        \"editdist_16\": 0,\n",
    "        \"editdist_17\": 56,\n",
    "        \"nb_tolonger\": 1318,\n",
    "        \"nb_toshorter\": 1751,\n",
    "        \"avg_interarrival\": 1112,\n",
    "        \"anomaly_status\": \"\"\n",
    "    },\n",
    "        {\n",
    "        \"timestamp\": \"2017-08-25T03:20:00.000\",\n",
    "        \"nb_A\": 2462,\n",
    "        \"nb_W\": 148,\n",
    "        \"nb_implicit_W\": 1721,\n",
    "        \"nb_dup_A\": 347,\n",
    "        \"nb_dup_W\": 0,\n",
    "        \"nb_A_prefix\": 434,\n",
    "        \"nb_W_prefix\": 66,\n",
    "        \"max_A_prefix\": 51,\n",
    "        \"avg_A_prefix\": 6,\n",
    "        \"max_A_AS\": 188,\n",
    "        \"avg_A_AS\": 16,\n",
    "        \"nb_orign_change\": 172,\n",
    "        \"nb_new_A\": 285,\n",
    "        \"nb_new_A_afterW\": 109,\n",
    "        \"max_path_len\": 13,\n",
    "        \"avg_path_len\": 6,\n",
    "        \"max_editdist\": 11,\n",
    "        \"avg_editdist\": 3,\n",
    "        \"editdist_7\": 165,\n",
    "        \"editdist_8\": 36,\n",
    "        \"editdist_9\": 37,\n",
    "        \"editdist_10\": 0,\n",
    "        \"editdist_11\": 1,\n",
    "        \"editdist_12\": 0,\n",
    "        \"editdist_13\": 0,\n",
    "        \"editdist_14\": 0,\n",
    "        \"editdist_15\": 0,\n",
    "        \"editdist_16\": 0,\n",
    "        \"editdist_17\": 0,\n",
    "        \"nb_tolonger\": 738,\n",
    "        \"nb_toshorter\": 983,\n",
    "        \"avg_interarrival\": 1133,\n",
    "        \"anomaly_status\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"timestamp\": \"2017-08-25T03:25:00.000\",\n",
    "        \"nb_A\": 3080,\n",
    "        \"nb_W\": 309,\n",
    "        \"nb_implicit_W\": 2199,\n",
    "        \"nb_dup_A\": 320,\n",
    "        \"nb_dup_W\": 0,\n",
    "        \"nb_A_prefix\": 686,\n",
    "        \"nb_W_prefix\": 181,\n",
    "        \"max_A_prefix\": 55,\n",
    "        \"avg_A_prefix\": 4,\n",
    "        \"max_A_AS\": 314,\n",
    "        \"avg_A_AS\": 16,\n",
    "        \"nb_orign_change\": 210,\n",
    "        \"nb_new_A\": 391,\n",
    "        \"nb_new_A_afterW\": 170,\n",
    "        \"max_path_len\": 15,\n",
    "        \"avg_path_len\": 6,\n",
    "        \"max_editdist\": 11,\n",
    "        \"avg_editdist\": 3,\n",
    "        \"editdist_7\": 250,\n",
    "        \"editdist_8\": 25,\n",
    "        \"editdist_9\": 44,\n",
    "        \"editdist_10\": 1,\n",
    "        \"editdist_11\": 1,\n",
    "        \"editdist_12\": 0,\n",
    "        \"editdist_13\": 0,\n",
    "        \"editdist_14\": 0,\n",
    "        \"editdist_15\": 0,\n",
    "        \"editdist_16\": 0,\n",
    "        \"editdist_17\": 0,\n",
    "        \"nb_tolonger\": 889,\n",
    "        \"nb_toshorter\": 1310,\n",
    "        \"avg_interarrival\": 1116,\n",
    "        \"anomaly_status\": \"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "anomaly_status_list = [\n",
    "    \"\",\n",
    "    \"anomaly due to high value of nb_A, nb_A_prefix, avg_A_prefix, max_A_AS, avg_A_AS, nb_new_A, editdist_9\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for instance 0:\n",
      "<s>[INST] \n",
      "   Below is BGP update message data. Your task is to determine the BGP anomaly status based on the data provided. If an anomaly is detected, specify the reason. If there are no amomalies then state that no anomalies detected. \n",
      "    {'timestamp': '2017-08-25T03:25:00.000', 'nb_A': 3080, 'nb_W': 309, 'nb_implicit_W': 2199, 'nb_dup_A': 320, 'nb_dup_W': 0, 'nb_A_prefix': 686, 'nb_W_prefix': 181, 'max_A_prefix': 55, 'avg_A_prefix': 4, 'max_A_AS': 314, 'avg_A_AS': 16, 'nb_orign_change': 210, 'nb_new_A': 391, 'nb_new_A_afterW': 170, 'max_path_len': 15, 'avg_path_len': 6, 'max_editdist': 11, 'avg_editdist': 3, 'editdist_7': 250, 'editdist_8': 25, 'editdist_9': 44, 'editdist_10': 1, 'editdist_11': 1, 'editdist_12': 0, 'editdist_13': 0, 'editdist_14': 0, 'editdist_15': 0, 'editdist_16': 0, 'editdist_17': 0, 'nb_tolonger': 889, 'nb_toshorter': 1310, 'avg_interarrival': 1116, 'anomaly_status': ''} [/INST]  Based on the provided BGP update message data, I detected the following anomalies:\n",
      "\n",
      "1. High number of nb_A_prefix: The value of nb_A_prefix (686) is\n",
      "\n",
      "\n",
      "Result for instance 1:\n",
      "<s>[INST] \n",
      "   Below is BGP update message data. Your task is to determine the BGP anomaly status based on the data provided. If an anomaly is detected, specify the reason. If there are no amomalies then state that no anomalies detected. \n",
      "    {'timestamp': '2017-08-25T03:35:00.000', 'nb_A': 86458, 'nb_W': 356, 'nb_implicit_W': 3790, 'nb_dup_A': 7500, 'nb_dup_W': 0, 'nb_A_prefix': 75578, 'nb_W_prefix': 203, 'max_A_prefix': 46, 'avg_A_prefix': 1, 'max_A_AS': 19182, 'avg_A_AS': 38, 'nb_orign_change': 182, 'nb_new_A': 75028, 'nb_new_A_afterW': 140, 'max_path_len': 21, 'avg_path_len': 4, 'max_editdist': 14, 'avg_editdist': 3, 'editdist_7': 243, 'editdist_8': 58, 'editdist_9': 75, 'editdist_10': 69, 'editdist_11': 9, 'editdist_12': 13, 'editdist_13': 0, 'editdist_14': 1, 'editdist_15': 0, 'editdist_16': 0, 'editdist_17': 0, 'nb_tolonger': 1180, 'nb_toshorter': 2610, 'avg_interarrival': 1087} [/INST]  Based on the provided BGP update message data, I did not detect any anomalies. All the values are within the expected range, and there are no significant deviations or spikes that could indicate an anomaly.\n",
      "\n",
      "\n",
      "Result for instance 2:\n",
      "<s>[INST] \n",
      "   Below is BGP update message data. Your task is to determine the BGP anomaly status based on the data provided. If an anomaly is detected, specify the reason. If there are no amomalies then state that no anomalies detected. \n",
      "    {'timestamp': '2017-08-25T04:00:00.000', 'nb_A': 4311, 'nb_W': 401, 'nb_implicit_W': 3069, 'nb_dup_A': 625, 'nb_dup_W': 0, 'nb_A_prefix': 1030, 'nb_W_prefix': 165, 'max_A_prefix': 54, 'avg_A_prefix': 4, 'max_A_AS': 305, 'avg_A_AS': 20, 'nb_orign_change': 206, 'nb_new_A': 367, 'nb_new_A_afterW': 250, 'max_path_len': 19, 'avg_path_len': 6, 'max_editdist': 17, 'avg_editdist': 3, 'editdist_7': 202, 'editdist_8': 23, 'editdist_9': 36, 'editdist_10': 2, 'editdist_11': 5, 'editdist_12': 7, 'editdist_13': 1, 'editdist_14': 1, 'editdist_15': 0, 'editdist_16': 0, 'editdist_17': 56, 'nb_tolonger': 1318, 'nb_toshorter': 1751, 'avg_interarrival': 1112, 'anomaly_status': ''} [/INST]  Based on the provided BGP update message data, I did not detect any anomalies. All the values fall within the expected range, and there is no significant deviation from the mean that could indicate an anomaly.\n",
      "\n",
      "\n",
      "Result for instance 3:\n",
      "<s>[INST] \n",
      "   Below is BGP update message data. Your task is to determine the BGP anomaly status based on the data provided. If an anomaly is detected, specify the reason. If there are no amomalies then state that no anomalies detected. \n",
      "    {'timestamp': '2017-08-25T03:20:00.000', 'nb_A': 2462, 'nb_W': 148, 'nb_implicit_W': 1721, 'nb_dup_A': 347, 'nb_dup_W': 0, 'nb_A_prefix': 434, 'nb_W_prefix': 66, 'max_A_prefix': 51, 'avg_A_prefix': 6, 'max_A_AS': 188, 'avg_A_AS': 16, 'nb_orign_change': 172, 'nb_new_A': 285, 'nb_new_A_afterW': 109, 'max_path_len': 13, 'avg_path_len': 6, 'max_editdist': 11, 'avg_editdist': 3, 'editdist_7': 165, 'editdist_8': 36, 'editdist_9': 37, 'editdist_10': 0, 'editdist_11': 1, 'editdist_12': 0, 'editdist_13': 0, 'editdist_14': 0, 'editdist_15': 0, 'editdist_16': 0, 'editdist_17': 0, 'nb_tolonger': 738, 'nb_toshorter': 983, 'avg_interarrival': 1133, 'anomaly_status': ''} [/INST]  Based on the provided BGP update message data, I detected the following anomalies:\n",
      "\n",
      "1. High number of implicit withdrawals (nb_implicit_W = 1721): This is an anomaly because the\n",
      "\n",
      "\n",
      "Result for instance 4:\n",
      "<s>[INST] \n",
      "   Below is BGP update message data. Your task is to determine the BGP anomaly status based on the data provided. If an anomaly is detected, specify the reason. If there are no amomalies then state that no anomalies detected. \n",
      "    {'timestamp': '2017-08-25T03:25:00.000', 'nb_A': 3080, 'nb_W': 309, 'nb_implicit_W': 2199, 'nb_dup_A': 320, 'nb_dup_W': 0, 'nb_A_prefix': 686, 'nb_W_prefix': 181, 'max_A_prefix': 55, 'avg_A_prefix': 4, 'max_A_AS': 314, 'avg_A_AS': 16, 'nb_orign_change': 210, 'nb_new_A': 391, 'nb_new_A_afterW': 170, 'max_path_len': 15, 'avg_path_len': 6, 'max_editdist': 11, 'avg_editdist': 3, 'editdist_7': 250, 'editdist_8': 25, 'editdist_9': 44, 'editdist_10': 1, 'editdist_11': 1, 'editdist_12': 0, 'editdist_13': 0, 'editdist_14': 0, 'editdist_15': 0, 'editdist_16': 0, 'editdist_17': 0, 'nb_tolonger': 889, 'nb_toshorter': 1310, 'avg_interarrival': 1116, 'anomaly_status': ''} [/INST]  Based on the provided BGP update message data, I detected the following anomalies:\n",
      "\n",
      "1. High number of nb_implicit_W: 2199, which is significantly higher than the average value\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text generation pipeline\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "\n",
    "# Loop through each instance in the dataset\n",
    "for i, instance in enumerate(data):\n",
    "    prompt = f\"\"\"\n",
    "   Below is BGP update message data. Your task is to determine the BGP anomaly status based on the data provided. If an anomaly is detected, specify the reason. If there are no amomalies then state that no anomalies detected. \n",
    "    {instance}\"\"\"\n",
    "    \n",
    "    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "    print(f\"Result for instance {i}:\")\n",
    "    print(result[0]['generated_text'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] \n",
      "Given BGP update message data, determine whether the BGP data indicates an anomaly: \n",
      "{'timestamp': '2017-08-25T03:25:00.000', 'nb_A': 3080, 'nb_W': 309, 'nb_implicit_W': 2199, 'nb_dup_A': 320, 'nb_dup_W': 0, 'nb_A_prefix': 686, 'nb_W_prefix': 181, 'max_A_prefix': 55, 'avg_A_prefix': 4, 'max_A_AS': 314, 'avg_A_AS': 16, 'nb_orign_change': 210, 'nb_new_A': 391, 'nb_new_A_afterW': 170, 'max_path_len': 15, 'avg_path_len': 6, 'max_editdist': 11, 'avg_editdist': 3, 'editdist_7': 250, 'editdist_8': 25, 'editdist_9': 44, 'editdist_10': 1, 'editdist_11': 1, 'editdist_12': 0, 'editdist_13': 0, 'editdist_14': 0, 'editdist_15': 0, 'editdist_16': 0, 'editdist_17': 0, 'nb_tolonger': 889, 'nb_toshorter': 1310, 'avg_interarrival': 1116, 'anomaly_status': ''} [/INST]  Based on the provided BGP update message data, there are several anomalies that can be identified:\n",
      "\n",
      "1. High number of duplicate AS paths: nb_dup_A=320, nb_dup_W=0. This indicates that there are a large number of duplicate AS paths in the update message, which is an anomaly.\n",
      "2. High number of implicit W\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = f\"\"\"\n",
    "Given BGP update message data, determine whether the BGP data indicates an anomaly: \n",
    "{data[0]}\"\"\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
