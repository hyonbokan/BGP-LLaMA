{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python_39_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78db1905d8c41f2a68c13f5d5f18090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Model loaded successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LlamaForCausalLM(\n",
       "   (model): LlamaModel(\n",
       "     (embed_tokens): Embedding(32000, 5120)\n",
       "     (layers): ModuleList(\n",
       "       (0-39): 40 x LlamaDecoderLayer(\n",
       "         (self_attn): LlamaSdpaAttention(\n",
       "           (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "           (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "           (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "           (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): LlamaMLP(\n",
       "           (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "           (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "           (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "         (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "       )\n",
       "     )\n",
       "     (norm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "     (rotary_emb): LlamaRotaryEmbedding()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       " ),\n",
       " LlamaTokenizerFast(name_or_path='hyonbokan/BGPStream13-10k-cutoff-1024-max-2048', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " })"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from threading import Lock\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Define model IDs\n",
    "CUSTOM_MODEL = \"hyonbokan/BGPStream13-10k-cutoff-1024-max-2048\"\n",
    "LLAMA3_8B_INSTRUCT = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize model, tokenizer, and lock\n",
    "model = None\n",
    "tokenizer = None\n",
    "model_lock = Lock()\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    global model, tokenizer\n",
    "    if model is None or tokenizer is None:\n",
    "        with model_lock:\n",
    "            if model is None or tokenizer is None:\n",
    "                try:\n",
    "                    model_id = CUSTOM_MODEL\n",
    "                    hf_auth = os.environ.get('HF_TOKEN')\n",
    "                    \n",
    "                    model_config = AutoConfig.from_pretrained(\n",
    "                        model_id,\n",
    "                        use_auth_token=hf_auth\n",
    "                    )\n",
    "                    model = AutoModelForCausalLM.from_pretrained(\n",
    "                        model_id,\n",
    "                        trust_remote_code=True,\n",
    "                        config=model_config,\n",
    "                        device_map='auto',\n",
    "                        use_auth_token=hf_auth\n",
    "                    )\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(\n",
    "                        model_id,\n",
    "                        use_auth_token=hf_auth\n",
    "                    )\n",
    "                    \n",
    "                    tokenizer.pad_token = tokenizer.eos_token\n",
    "                    tokenizer.padding_side = \"left\"\n",
    "                    tokenizer.truncation_side = \"left\"\n",
    "\n",
    "                    logger.info(\"Model loaded successfully\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to load the model: {str(e)}\")\n",
    "                    raise\n",
    "    return model, tokenizer\n",
    "\n",
    "# Generate response function\n",
    "def generate_llm_response(query):\n",
    "    logger.info(f\"User query: {query}\")\n",
    "    try:\n",
    "        # Ensure model and tokenizer are loaded\n",
    "        model, tokenizer = load_model()\n",
    "\n",
    "        # Tokenize the input query\n",
    "        inputs = tokenizer(\n",
    "            query,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1500\n",
    "        )\n",
    "\n",
    "        input_ids = inputs.input_ids.to(model.device)\n",
    "        attention_mask = inputs.attention_mask.to(model.device)\n",
    "        \n",
    "        # Generation settings - best so far\n",
    "        generation_kwargs = dict(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=712,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.0,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )        \n",
    "        \n",
    "        # generation_kwargs = dict(\n",
    "        #     input_ids=input_ids,\n",
    "        #     attention_mask=attention_mask,\n",
    "        #     max_new_tokens=712,\n",
    "        #     do_sample=True,\n",
    "        #     temperature=0.7,\n",
    "        #     top_p=0.9,\n",
    "        #     repetition_penalty=1.2,\n",
    "        #     eos_token_id=tokenizer.eos_token_id,\n",
    "        #     pad_token_id=tokenizer.pad_token_id,\n",
    "        # )\n",
    "\n",
    "        # Generate output\n",
    "        generated_ids = model.generate(**generation_kwargs)\n",
    "\n",
    "        # Decode and print the generated text\n",
    "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        print(\"Generated text:\")\n",
    "        print(generated_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating LLM response: {str(e)}\")\n",
    "\n",
    "# Load the model once at startup\n",
    "load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_llm_response(\"Generate a Python script using `pybgpstream` to collect BGP update messages from route collectors. The script should retrieve updates from all available route collectors for a specific 10-minute time range. The output should display the timestamp, collector, and raw update message. The code must be written inside a code block using ```[code]```.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_39_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
