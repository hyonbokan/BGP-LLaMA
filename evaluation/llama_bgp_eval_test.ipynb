{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python_39_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from torch import cuda\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from threading import Lock\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Define model IDs\n",
    "CUSTOM_MODEL = \"hyonbokan/BGPStream13-10k-cutoff-1024-max-2048\"\n",
    "LLAMA3_8B_INSTRUCT = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize model, tokenizer, and lock\n",
    "model = None\n",
    "tokenizer = None\n",
    "model_lock = Lock()\n",
    "\n",
    "# Load the model only once\n",
    "def load_model():\n",
    "    global model, tokenizer\n",
    "    if model is None or tokenizer is None:  # Check if model or tokenizer is already loaded\n",
    "        with model_lock:\n",
    "            if model is None or tokenizer is None:  # Double-check inside the lock\n",
    "                try:\n",
    "                    model_id = LLAMA3_8B_INSTRUCT\n",
    "                    hf_auth = os.environ.get('HF_TOKEN')\n",
    "                    \n",
    "                    model_config = AutoConfig.from_pretrained(\n",
    "                        model_id,\n",
    "                        use_auth_token=hf_auth\n",
    "                    )\n",
    "                    model = AutoModelForCausalLM.from_pretrained(\n",
    "                        model_id,\n",
    "                        trust_remote_code=True,\n",
    "                        config=model_config,\n",
    "                        device_map='auto',\n",
    "                        use_auth_token=hf_auth\n",
    "                    )\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(\n",
    "                        model_id,\n",
    "                        use_auth_token=hf_auth\n",
    "                    )\n",
    "                    \n",
    "                    # Set padding token if not set\n",
    "                    if tokenizer.pad_token is None:\n",
    "                        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "                    tokenizer.padding_side = \"right\"\n",
    "                    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "                    logger.info(\"Model loaded successfully\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to load the model: {str(e)}\")\n",
    "                    raise\n",
    "    return model, tokenizer\n",
    "\n",
    "# Generate response function\n",
    "def generate_llm_response(query):\n",
    "    logger.info(f\"User query: {query}\")\n",
    "    try:\n",
    "        # Ensure model and tokenizer are loaded\n",
    "        model, tokenizer = load_model()\n",
    "\n",
    "        # Tokenize the input query\n",
    "        inputs = tokenizer(\n",
    "            query,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1500\n",
    "        )\n",
    "\n",
    "        input_ids = inputs.input_ids.to(model.device)\n",
    "        attention_mask = inputs.attention_mask.to(model.device)\n",
    "        \n",
    "        # Generation settings\n",
    "        generation_kwargs = dict(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=712,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        # Generate output\n",
    "        generated_ids = model.generate(**generation_kwargs)\n",
    "\n",
    "        # Decode and print the generated text\n",
    "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        print(\"Generated text:\")\n",
    "        print(generated_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating LLM response: {str(e)}\")\n",
    "\n",
    "# Load the model once at startup\n",
    "load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Announcements: 0\n",
      "Withdrawals: 0\n"
     ]
    }
   ],
   "source": [
    "import pybgpstream\n",
    "from datetime import datetime\n",
    "import ipaddress\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define time range\n",
    "from_time = \"2024-03-15 13:00:00\"\n",
    "until_time = \"2024-03-15 14:00:00\"\n",
    "\n",
    "# Initialize BGPStream\n",
    "stream = pybgpstream.BGPStream(\n",
    "    from_time=from_time,\n",
    "    until_time=until_time,\n",
    "    record_type=\"updates\",\n",
    "    collectors=[\"rrc00\", \"route-views.amsix\"]\n",
    ")\n",
    "\n",
    "# Process records and elements\n",
    "announcements = defaultdict(int)\n",
    "withdrawals = defaultdict(int)\n",
    "prefix_as_paths = {}\n",
    "community_counts = defaultdict(int)\n",
    "\n",
    "for rec in stream.records():\n",
    "    for elem in rec:\n",
    "        elem_time = datetime.utcfromtimestamp(elem.time)\n",
    "        elem_type = elem.type\n",
    "        fields = elem.fields\n",
    "        prefix = fields.get(\"prefix\")\n",
    "        if prefix is None:\n",
    "            continue\n",
    "        \n",
    "        as_path_str = fields.get('as-path', \"\")\n",
    "        as_path = as_path_str.split()\n",
    "        \n",
    "        peer_asn = elem.peer_asn\n",
    "        collector = rec.collector\n",
    "        \n",
    "        communities = fields.get('communities', [])\n",
    "        \n",
    "        try:\n",
    "            network = ipaddress.ip_network(prefix)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        target_asn = '3356'\n",
    "        if target_asn not in as_path:\n",
    "            continue\n",
    "        \n",
    "        target_prefixes = ['192.0.2.0/24', '198.51.100.0/24']\n",
    "        if prefix not in target_prefixes:\n",
    "            continue\n",
    "        \n",
    "        if elem_type == 'A':\n",
    "            announcements[prefix] += 1\n",
    "        elif elem_type == 'W':\n",
    "            withdrawals[prefix] += 1\n",
    "        \n",
    "        if prefix in prefix_as_paths:\n",
    "            if as_path!= prefix_as_paths[prefix]:\n",
    "                prefix_as_paths[prefix] = as_path\n",
    "        else:\n",
    "            prefix_as_paths[prefix] = as_path\n",
    "        \n",
    "        for community in communities:\n",
    "            community_str = f\"{community[0]}:{community[1]}\"\n",
    "            community_counts[community_str] += 1\n",
    "\n",
    "# Print summary of announcements and withdrawals\n",
    "print(f\"Announcements: {sum(announcements.values())}\")\n",
    "print(f\"Withdrawals: {sum(withdrawals.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA2-7B:\n",
    "`Multiple Choice v2`:\n",
    "    Accuracy: 48.57%\n",
    "    Correct answers: 17\n",
    "    Incorrect answers: 18\n",
    "\n",
    "`Multiple Choice v1`:\n",
    "    Accuracy: 51.43%\n",
    "    Correct answers: 18\n",
    "    Incorrect answers: 17\n",
    "\n",
    "### LLaMA2-13B:\n",
    "`Multiple Choice v2`:\n",
    "    Accuracy: 60.00%\n",
    "    Correct answers: 21\n",
    "    Incorrect answers: 14\n",
    "\n",
    "`Multiple Choice v1`:\n",
    "    Accuracy: 71.43%\n",
    "    Correct answers: 25\n",
    "    Incorrect answers: 10\n",
    "\n",
    "### BGPtest9- cosine - `Best`:\n",
    "`Multiple Choice v2`:\n",
    "    Accuracy: 68.57%\n",
    "    Correct answers: 24\n",
    "    Incorrect answers: 11\n",
    "\n",
    "`Multiple Choice v1`:\n",
    "    - Accuracy: 82.86%\n",
    "    - Correct answers: 29\n",
    "    - Incorrect answers: 6\n",
    "\n",
    "### BGPtest10- constant:\n",
    "`Multiple Choice v2`:\n",
    "    Accuracy: 62.86%\n",
    "    Correct answers: 22\n",
    "    Incorrect answers: 13\n",
    "\n",
    "`Multiple Choice v1`:\n",
    "    - Accuracy: 71.43%\n",
    "    - Correct answers: 25\n",
    "    - Incorrect answers: 10\n",
    "\n",
    "`Fill the blank v1`:\n",
    "    - Accuracy: 88.57%\n",
    "    - Correct answers: 31\n",
    "    - Incorrect answers: 4\n",
    "\n",
    "`Fill the blank v2`:\n",
    "    - Accuracy: 91.42%\n",
    "    - Correct answers: 33\n",
    "    - Incorrect answers: 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BGP-LLaMA-1-cosine-2k-alpha64:\n",
    "`Multiple Choice v2`:\n",
    "    Accuracy: 51.43%\n",
    "    Correct answers: 18\n",
    "    Incorrect answers: 17\n",
    "\n",
    "`Multiple Choice v1`:\n",
    "    Accuracy: 68.57%\n",
    "    Correct answers: 24\n",
    "    Incorrect answers: 11\n",
    "\n",
    "### BGP-LLaMA-2-cosine-20k-alpha16:\n",
    "`Multiple Choice v1`:\n",
    "    Accuracy: 68.57%\n",
    "    Correct answers: 24\n",
    "    Incorrect answers: 11\n",
    "\n",
    "`Multiple Choice v2`:\n",
    "    Accuracy: 54.29%\n",
    "    Correct answers: 19\n",
    "    Incorrect answers: 16\n",
    "\n",
    "\n",
    "### BGP-LLaMA-2(combined)-cosine-20k-alpha16:\n",
    "`Multiple Choice v1`:\n",
    "    Accuracy: 80.00%\n",
    "    Correct answers: 28\n",
    "    Incorrect answers: 7\n",
    "\n",
    "`Multiple Choice v2`:\n",
    "\n",
    "    Accuracy: 65.71%\n",
    "    Correct answers: 23\n",
    "    Incorrect answers: 12\n",
    "\n",
    "### BGP-LLaMA-13b-2iter-40k-cutoff-max-2048: ??\n",
    "`Multiple Choice v1`:\n",
    "    Accuracy: 77.14%\n",
    "    Correct answers: 27\n",
    "    Incorrect answers: 8\n",
    "\n",
    "    \n",
    "`Multiple Choice v2`:\n",
    "    Accuracy: 62.86%\n",
    "    Correct answers: 22\n",
    "    Incorrect answers: 13\n",
    "\n",
    "`PyBGPStream`: not pass\n",
    "\n",
    "### BGP-LLaMA-13b-3-30k-cutoff-max-2048:\n",
    "`Multiple Choice v1`:\n",
    "    Accuracy: 57.14%\n",
    "    Correct answers: 20\n",
    "    Incorrect answers: 15\n",
    "\n",
    "`Multiple Choice v2`:\n",
    "    Accuracy: 54%\n",
    "\n",
    "`PyBGPStream`: pass\n",
    "\n",
    "### BGP-LLaMA-13b-50k-cutoff-max-2048: \n",
    "`Multiple Choice v1`:\n",
    "    Accuracy: 60.00%\n",
    "    Correct answers: 21\n",
    "    Incorrect answers: 14\n",
    "`PyBGPStream`: pass\n",
    "\n",
    "### BGP-LLaMA-13b-20k-cutoff-max-2048: \n",
    "`Multiple Choice v1`\n",
    "    Accuracy: 48.57%\n",
    "    Correct answers: 17\n",
    "    Incorrect answers: 18\n",
    "`PyBGPStream`: pass\n",
    "\n",
    "\n",
    "### BGP-LLaMA-13b-20k-cutoff-1024-max-none:\n",
    "`Multiple Choice v1`\n",
    "    Accuracy: 45.71%\n",
    "    Correct answers: 16\n",
    "    Incorrect answers: 19\n",
    "`PyBGPStream`: pass\n",
    "\n",
    "\n",
    "### BGP-LLaMA-13b-30k-cutoff-1024-max-none:\n",
    "`Multiple Choice v1`\n",
    "Accuracy: 62.86%\n",
    "Correct answers: 22\n",
    "Incorrect answers: 13\n",
    "\n",
    "`PyBGPStream`: pass\n",
    "\n",
    "\n",
    "### BGP-LLaMA-13b-40k-cutoff-1024-max-none:\n",
    "`Multiple Choice v1`\n",
    "    Accuracy: 48.57%\n",
    "    Correct answers: 17\n",
    "    Incorrect answers: 18\n",
    "`PyBGPStream`: not pass\n",
    "\n",
    "### BGP-LLaMA-13b-50k-cutoff-1024-max-none:\n",
    "`Multiple Choice v1`\n",
    "    Accuracy: 40.00%\n",
    "    Correct answers: 14\n",
    "    Incorrect answers: 21\n",
    "`PyBGPStream`: not pass\n",
    "\n",
    "\n",
    "### BGP-LLaMA26k-13b-30k-cutoff-1024-max-None:\n",
    "`Multiple Choice v1`: not pass\n",
    "\n",
    "`PyBGPStream`: pass\n",
    "\n",
    "### BGP-LLaMA26k-13b-20k-cutoff-1024-max-None:\n",
    "`Multiple Choice v1`:\n",
    "Incorrect\n",
    "Accuracy: 37.14%\n",
    "Correct answers: 13\n",
    "Incorrect answers: 22\n",
    "`PyBGPStream`: pass\n",
    "\n",
    "### BGP-LLaMA26k-13b-10k-cutoff-1024-max-2048\n",
    "`Multiple Choice v1`:\n",
    "Accuracy: 62.86%\n",
    "Correct answers: 22\n",
    "Incorrect answers: 13\n",
    "`PyBGPStream`: pass (better than previous)\n",
    "\n",
    "\n",
    "### BGP-LLaMA26k-13b-5k-cutoff-1024-max-2048 - `Best Combined`\n",
    "`Multiple Choice v1`:\n",
    "Accuracy: 82.86%\n",
    "Correct answers: 29\n",
    "Incorrect answers: 6\n",
    "\n",
    "`Multiple Choice v2`:\n",
    "Accuracy: 71.43%\n",
    "Correct answers: 25\n",
    "Incorrect answers: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BGP-LLaMA-knowledge-20k-cutoff-1024-max-2048\n",
    "`Multiple Choice v1`:\n",
    "Accuracy: 62.86%\n",
    "Correct answers: 22\n",
    "Incorrect answers: 13\n",
    "\n",
    "`Multiple Choice v2`:\n",
    "Accuracy: 48.57%\n",
    "Correct answers: 17\n",
    "Incorrect answers: 18\n",
    "\n",
    "### BGP-LLaMA-knowledge-5k-cutoff-1024-max-2048 `best knowledge`\n",
    "`Multiple Choice v1`:\n",
    "Accuracy: 85.71%\n",
    "Correct answers: 30\n",
    "Incorrect answers: 5\n",
    "\n",
    "`Multiple Choice v2`:\n",
    "Accuracy: 71.43%\n",
    "Correct answers: 25\n",
    "Incorrect answers: 10\n",
    "\n",
    "### BGP-LLaMA-knowledge-3k-cutoff-1024-max-2048\n",
    "`Multiple Choice v1`:\n",
    "Accuracy: 82.86%\n",
    "Correct answers: 29\n",
    "Incorrect answers: 6\n",
    "\n",
    "`Multiple Choice v2`:\n",
    "Accuracy: 74.29%\n",
    "Correct answers: 26\n",
    "Incorrect answers: 9\n",
    "\n",
    "### BGP-LLaMA-knowledge-2k-cutoff-1024-max-2048\n",
    "`Multiple Choice v1`:\n",
    "Accuracy: 82.86%\n",
    "Correct answers: 29\n",
    "Incorrect answers: 6\n",
    "\n",
    "`Multiple Choice v2`:\n",
    "Accuracy: 71.43%\n",
    "Correct answers: 25\n",
    "Incorrect answers: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_39_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
