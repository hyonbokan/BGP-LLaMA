{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe0fdaed384450d9d2620494011073f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hb/myenv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57dd5f9c4b0244d28f2c275025ef72b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "import os\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "# bnb_config = transformers.BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type='nf4',\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=bfloat16\n",
    "# )\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Need auth token for these\n",
    "hf_auth = os.environ.get('hf_token')\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/3\n",
      "Processed 2/3\n",
      "Processed 3/3\n",
      "[{'instruction': 'The goal for this task is to determine if the data indicates an anomaly. The context, section, and table columns provide important information for identifying the correct anomaly type.', 'input_seg': '[TLE] The context is about BGP data analysis for detecting anomalies. The section is related to a specific time period of BGP monitoring. [TAB] col: | timestamp | asn | num_routes | num_new_routes | num_withdrawals | num_origin_changes | num_route_changes | max_path_length | avg_path_length | max_edit_distance | avg_edit_distance | num_announcements | num_unique_prefixes_announced | anomaly_status | row 1: | 2017-12-12 04:00:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 2: | 2017-12-12 04:05:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 3: | 2017-12-12 04:10:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 4: | 2017-12-12 04:15:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 5: | 2017-12-12 04:20:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 6: | 2017-12-12 04:25:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 7: | 2017-12-12 04:30:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 8: | 2017-12-12 04:35:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 9: | 2017-12-12 04:40:00 | 39523 | 77 | 77 | 393 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 12086 | 77 | anomaly detected at 2017-12-12 04:40:00 due to high value of num_routes=77, num_new_routes=77, num_announcements=12086, num_unique_prefixes_announced=77 | [SEP] row 10: | 2017-12-12 04:45:00 | 39523 | 50 | 3 | 1470 | 0 | 47 | 8 | 5.66 | 5 | 2.94 | 3427 | 50 | anomaly detected at 2017-12-12 04:45:00 due to high value of num_new_routes=3, num_route_changes=47, max_path_length=8, avg_path_length=5.66, max_edit_distance=5, avg_edit_distance=2.94 | [SEP] row 11: | 2017-12-12 04:50:00 | 39523 | 0 | 0 | 50 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | anomaly detected at 2017-12-12 04:50:00 due to high value of num_withdrawals=50 | [SEP] row 12: | 2017-12-12 04:55:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 13: | 2017-12-12 05:00:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 14: | 2017-12-12 05:05:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 15: | 2017-12-12 05:10:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 16: | 2017-12-12 05:15:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 17: | 2017-12-12 05:20:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 18: | 2017-12-12 05:25:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 19: | 2017-12-12 05:30:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 20: | 2017-12-12 05:35:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP]', 'question': \"Based on the data provided, does the data indicate an anomaly? If an anomaly is detected, include the timestamp of the anomaly data and provide a reason explaining which values are anomalous. For example, 'anomaly detected at 2024-06-10 12:00:00 due to high value of num_routes=77'\", 'output': 'Based on the data provided in the table, there are two anomalies detected:\\n\\n1. Anomaly detected at 2017-12-12 04:40:00 due to high value of num_routes=77, num_new_routes=77, num_announcements=12086, num_unique_prefixes_announced=77.\\n2. Anomaly detected at 2017-12-12 05:10:00 due to high value of num_withdrawals=50.\\n\\nThe reason for the anomaly in the first case is that the number of routes, new routes, and announcements are all high compared to the average values, indicating a possible increase in routing activity. The high value of num_unique_prefixes_announced=77 is also notable, as it is higher than the average value.\\n\\nThe reason for the anomaly in the second case is that the number of withdrawals is high compared to the average value, indicating a possible issue with route withdrawals.\\n\\nTherefore, the data indicates two anomalies: one at 2017-12-12 04:40:00 and another at 2017-12-12 05:10:00.'}, {'instruction': 'The goal for this task is to determine if the data indicates an anomaly. The context, section, and table columns provide important information for identifying the correct anomaly type.', 'input_seg': '[TLE] The context is about BGP data analysis for detecting anomalies. The section is related to a specific time period of BGP monitoring. [TAB] col: | timestamp | asn | num_routes | num_new_routes | num_withdrawals | num_origin_changes | num_route_changes | max_path_length | avg_path_length | max_edit_distance | avg_edit_distance | num_announcements | num_unique_prefixes_announced | anomaly_status | row 1: | 2017-12-12 05:40:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 2: | 2017-12-12 05:45:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 3: | 2017-12-12 05:50:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 4: | 2017-12-12 05:55:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 5: | 2017-12-12 06:00:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 6: | 2017-12-12 06:05:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 7: | 2017-12-12 06:10:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 8: | 2017-12-12 06:15:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 9: | 2017-12-12 06:20:00 | 39523 | 31 | 31 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 57 | 31 | no anomalies detected | [SEP] row 10: | 2017-12-12 06:25:00 | 39523 | 0 | 0 | 31 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 11: | 2017-12-12 06:30:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 12: | 2017-12-12 06:35:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 13: | 2017-12-12 06:40:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 14: | 2017-12-12 06:45:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 15: | 2017-12-12 06:50:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 16: | 2017-12-12 06:55:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 17: | 2017-12-12 07:00:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 18: | 2017-12-12 07:05:00 | 39523 | 74 | 74 | 2211 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 16467 | 74 | anomaly detected at 2017-12-12 07:05:00 due to high value of num_routes=74, num_new_routes=74, num_withdrawals=2211, num_announcements=16467, num_unique_prefixes_announced=74 | [SEP] row 19: | 2017-12-12 07:10:00 | 39523 | 20 | 2 | 96 | 0 | 16 | 8 | 6.3 | 4 | 2.6 | 73 | 20 | anomaly detected at 2017-12-12 07:10:00 due to high value of num_routes=20, num_new_routes=2, num_withdrawals=96, max_path_length=8, avg_path_length=6.3, max_edit_distance=4, avg_edit_distance=2.6, num_announcements=73, num_unique_prefixes_announced=20 | [SEP] row 20: | 2017-12-12 07:15:00 | 39523 | 0 | 0 | 20 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP]', 'question': \"Based on the data provided, does the data indicate an anomaly? If an anomaly is detected, include the timestamp of the anomaly data and provide a reason explaining which values are anomalous. For example, 'anomaly detected at 2024-06-10 12:00:00 due to high value of num_routes=77'\", 'output': 'Based on the data provided, an anomaly is detected at 2017-12-12 07:05:00. The reason for the anomaly is:\\n\\n\"anomaly detected at 2017-12-12 07:05:00 due to high value of num_routes=74, num_new_routes=74, num_withdrawals=2211, num_announcements=16467, num_unique_prefixes_announced=74.\"\\n\\nThe values that are anomalous are:\\n\\n* num_routes = 74 (high value)\\n* num_new_routes = 74 (high value)\\n* num_withdrawals = 2211 (high value)\\n* num_announcements = 16467 (high value)\\n* num_unique_prefixes_announced = 74 (high value)\\n\\nThese values are higher than the normal or expected values, indicating an anomaly in the BGP data.'}, {'instruction': 'The goal for this task is to determine if the data indicates an anomaly. The context, section, and table columns provide important information for identifying the correct anomaly type.', 'input_seg': '[TLE] The context is about BGP data analysis for detecting anomalies. The section is related to a specific time period of BGP monitoring. [TAB] col: | timestamp | asn | num_routes | num_new_routes | num_withdrawals | num_origin_changes | num_route_changes | max_path_length | avg_path_length | max_edit_distance | avg_edit_distance | num_announcements | num_unique_prefixes_announced | anomaly_status | row 1: | 2017-12-12 07:20:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 2: | 2017-12-12 07:25:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 3: | 2017-12-12 07:30:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 4: | 2017-12-12 07:35:00 | 39523 | 1 | 1 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 181 | 1 | no anomalies detected | [SEP] row 5: | 2017-12-12 07:40:00 | 39523 | 0 | 0 | 1 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 6: | 2017-12-12 07:45:00 | 39523 | 1 | 1 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 1 | 1 | no anomalies detected | [SEP] row 7: | 2017-12-12 07:50:00 | 39523 | 0 | 0 | 1 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 8: | 2017-12-12 07:55:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP] row 9: | 2017-12-12 08:00:00 | 39523 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | no anomalies detected | [SEP]', 'question': \"Based on the data provided, does the data indicate an anomaly? If an anomaly is detected, include the timestamp of the anomaly data and provide a reason explaining which values are anomalous. For example, 'anomaly detected at 2024-06-10 12:00:00 due to high value of num_routes=77'\", 'output': 'Based on the data provided, there is an anomaly detected at timestamp 2017-12-12 07:55:00.\\n\\nThe values that are anomalous are:\\n\\n* num_routes = 1: This is an anomaly because the number of routes is not equal to 0, which is the expected value for this time period.\\n* num_new_routes = 1: This is an anomaly because the number of new routes is not equal to 0, which is the expected value for this time period.\\n\\nReason: Anomaly detected at 2017-12-12 07:55:00 due to non-zero values of num_routes and num_new_routes.\\n\\nThe rest of the values in the table are within the expected range, so no anomalies are detected for those columns.'}]\n",
      "Results saved to llm_7B_bgp_data_with_outputs_20.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tablular_eval_util import combine_csv_files, split_dataframe, preprocess_data, run_llm_inference\n",
    "\n",
    "directory = '/home/hb/dataset_bgp/bgp_tab_dataset_test'\n",
    "combined_df = combine_csv_files(directory)\n",
    "\n",
    "# Split the DataFrame into smaller chunks\n",
    "split_size = 20\n",
    "data_list = split_dataframe(combined_df, split_size)\n",
    "\n",
    "# Preprocess the data into the required format\n",
    "formatted_data = [preprocess_data(chunk) for chunk in data_list]\n",
    "\n",
    "formatted_data_file = 'llm_table_bgp_data_test_20.json'\n",
    "with open(formatted_data_file, 'w') as f:\n",
    "    json.dump(formatted_data, f, indent=4)\n",
    "\n",
    "with open(formatted_data_file, 'r') as f:\n",
    "    formatted_data = json.load(f)\n",
    "\n",
    "output_results_file = 'llm_7B_bgp_data_with_outputs_20.json'\n",
    "run_llm_inference(formatted_data, model, tokenizer, max_length=3050, output_results_file=output_results_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2017-12-12 07:10:00', '2017-12-12 04:45:00', '2017-12-12 07:05:00', '2017-12-12 04:40:00', '2017-12-12 04:50:00'}\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_true_labels(csv_file):\n",
    "    \"\"\"\n",
    "    Load the true anomaly labels from the CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    true_anomalies = df.loc[df['anomaly_status'].str.contains('anomaly detected', case=False, na=False), 'timestamp'].tolist()\n",
    "    return set(true_anomalies)\n",
    "\n",
    "def load_llm_output(LLM_output_list):\n",
    "    \"\"\"\n",
    "    Convert the provided LLM output list to a set of timestamps.\n",
    "    \"\"\"\n",
    "    llm_anomalies = set(LLM_output_list)\n",
    "    return llm_anomalies\n",
    "\n",
    "def evaluate_anomalies(true_anomalies, llm_anomalies):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of LLM anomaly detection.\n",
    "    \"\"\"\n",
    "    # True positives, false positives, and false negatives\n",
    "    tp = len(true_anomalies & llm_anomalies)\n",
    "    fp = len(llm_anomalies - true_anomalies)\n",
    "    fn = len(true_anomalies - llm_anomalies)\n",
    "    \n",
    "    # Precision, recall, and F1 score\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "# Load true labels and LLM output\n",
    "true_labels_file = \"/home/hb/dataset_bgp/bgp_tab_dataset_test/bgp_features_asn_39523_russian_hijack_with_labels.csv\"\n",
    "\n",
    "llm_output_list = [\n",
    "    '2017-12-12 04:00:00',\n",
    "    '2017-12-12 04:10:00',\n",
    "    '2017-12-12 04:15:00',\n",
    "    '2017-12-12 04:20:00',\n",
    "    '2017-12-12 04:25:00',\n",
    "    '2017-12-12 04:30:00',\n",
    "    '2017-12-12 04:35:00',\n",
    "    '2017-12-12 05:45:00',\n",
    "    '2017-12-12 05:50:00',\n",
    "    '2017-12-12 05:55:00',\n",
    "    '2017-12-12 06:00:00',\n",
    "    '2017-12-12 06:05:00',\n",
    "    '2017-12-12 06:10:00',\n",
    "    '2017-12-12 06:15:00',\n",
    "    '2017-12-12 06:20:00',\n",
    "    '2017-12-12 07:45:00',\n",
    "]\n",
    "\n",
    "true_anomalies = load_true_labels(true_labels_file)\n",
    "print(true_anomalies)\n",
    "llm_anomalies = load_llm_output(llm_output_list)\n",
    "\n",
    "precision, recall, f1 = evaluate_anomalies(true_anomalies, llm_anomalies)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] \n",
      "The goal for this task is to identify the type of BGP anomaly based on the given data. The context, section, and table columns provide important information for identifying the correct anomaly type.. \n",
      "[TLE] The context is about BGP data analysis for detecting anomalies. The section is related to a specific time period of BGP monitoring. [TAB] col: | timestamp | asn | num_routes | num_new_routes | num_withdrawals | num_origin_changes | num_route_changes | max_path_length | avg_path_length | max_edit_distance | avg_edit_distance | num_announcements | num_unique_prefixes_announced | row 1: | 2024-06-10 12:00:00 | 12345 | 100 | 5 | 3 | 1 | 2 | 4 | 3.5 | 2 | 1.5 | 10 | 8 | [SEP] row 2: | 2024-06-10 12:05:00 | 12346 | 95 | 7 | 4 | 0 | 1 | 5 | 3.2 | 1 | 1.7 | 9 | 7 | [SEP] row 3: | 2024-06-10 12:10:00 | 12347 | 102 | 4 | 5 | 2 | 3 | 4.1 | 3 | 2.5 | 11 | 10 |\n",
      "Based on the data provided, identify the type of BGP anomaly. The possible anomaly types are: route_flapping, outage, prefix_hijacking, normal. The data contains various metrics such as num_routes, num_new_routes, num_withdrawals, etc.\n",
      " [/INST]  Based on the data provided, the most likely type of BGP anomaly is route flapping. This is because the data shows a rapid change in the number of routes (num_routes) and the number of new routes (num_new_routes) over a short period of time (less than 5 minutes). Specifically, the change in num_routes from row 1 to row 2 is 50 (100 - 50), and the change in num_new_routes is 7 (95 - 38). These changes are significant and indicate route flapping.\n",
      "\n",
      "Other possible types of BGP anomalies that could be observed in this data are:\n",
      "\n",
      "* Outage: A sudden drop in the number of routes (num_routes) or the number of new routes (num_new_routes) could indicate an outage. However, the data does not show a significant drop in either metric, which suggests that an outage is unlikely.\n",
      "* Prefix hijacking: A sudden change in the number of unique prefixes announced (num_unique_prefixes_announced) could indicate prefix hijacking. However, the data does not show a significant change in this metric, which suggests that prefix hijacking is unlikely.\n",
      "* Normal: The data does not show any significant changes in the other metrics, such as the number of withdrawals (num_withdrawals), origin changes (num_origin_changes), or route changes (num_route_changes). This suggests that the observed changes in num_routes and num_new_routes are likely due to normal BGP behavior rather than an anomaly.\n",
      "\n",
      "Therefore, based on the data provided, the most likely type of BGP anomaly is route flapping.\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = f\"\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1528)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Create a Python script with the Scapy library to detect DDoS attacks from a PCAP file. The script should read the file, analyze traffic patterns by monitoring packets per second against a set threshold within defined time windows, and reset counters for each time window. It should also log any potential attacks showing the source IP and packet rate. Include a final check for the last time window at the end of the file.\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=712)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Generate Python code to calculate and visualize 5G network performance KPIs: Total Network Capacity, Capacity per Area, Capacity per Point, Cost per Capacity, Cost per Area, and Surplus per Area. Load data from '5G_Infrastructure/demand_driven_postcode_data_results.csv'. Use keywords: 'capacity', 'cost', 'area', 'numpoints' to identify relevant columns. [/INST]  Sure! Here is some Python code that can be used to calculate and visualize 5G network performance KPIs based on the provided CSV file:\n",
      "```\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Load the data from the CSV file\n",
      "df = pd.read_csv('5G_Infrastructure/demand_driven_postcode_data_results.csv')\n",
      "\n",
      "# Identify the relevant columns\n",
      "capacity = df['capacity']\n",
      "cost = df['cost']\n",
      "area = df['area']\n",
      "numpoints = df['numpoints']\n",
      "\n",
      "# Calculate the KPIs\n",
      "total_network_capacity = capacity.sum()\n",
      "capacity_per_area = capacity / area\n",
      "capacity_per_point = capacity / numpoints\n",
      "cost_per_capacity = cost / capacity\n",
      "cost_per_area = cost / area\n",
      "surplus_per_area = cost - (capacity * cost_per_point)\n",
      "\n",
      "# Visualize the results\n",
      "sns.barplot(x=range(len(capacity)), y=capacity, hue='numpoints')\n",
      "sns.lineplot(x=range(len(capacity)), y=capacity_per_point, label='Capacity per Point')\n",
      "sns.lineplot(x=range(len(area)), y=capacity_per_area, label='Capacity per Area')\n",
      "sns.lineplot(x=range(len(cost)), y=cost_per_capacity, label='Cost per Capacity')\n",
      "sns.lineplot(x=range(len(area)), y=cost_per_area, label='Cost per Area')\n",
      "sns.barplot(x=range(len(surplus)), y=surplus_per_area, label='Surplus per Area')\n",
      "\n",
      "# Add axis labels and titles\n",
      "plt.xlabel('Postcode')\n",
      "plt.ylabel('Capacity/Cost')\n",
      "plt.title('5G Network Performance KPIs')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "```\n",
      "This code first loads the data from the `demand_driven_postcode_data_results.csv` file using the `pd.read_csv()` function. It then identifies the relevant columns in the data frame using the `capacity`, `cost`, `area`, and `numpoints` keywords.\n",
      "\n",
      "Next, the code calculates the KPIs using the identified columns. Specifically, it calculates the total network capacity by summing the `capacity` column, the capacity per area by dividing the `capacity` column by the `area` column, and the capacity per point by dividing the `capacity` column by the `\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Generate Python code to calculate and visualize 5G network performance KPIs: Total Network Capacity, Capacity per Area, Capacity per Point, Cost per Capacity, Cost per Area, and Surplus per Area. Load data from '5G_Infrastructure/demand_driven_postcode_data_results.csv'. Use keywords: 'capacity', 'cost', 'area', 'numpoints' to identify relevant columns.\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=712)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Choice\n",
    "`5G configuration analysis v1`:\n",
    "Accuracy: 66%\n",
    "Correct answers: 13 + 7 = 20\n",
    "Incorrect answers: 10\n",
    "\n",
    "`5G configuration analysis v2`:\n",
    "Accuracy: 56%\n",
    "Correct answers: 17\n",
    "Incorrect answers: 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
