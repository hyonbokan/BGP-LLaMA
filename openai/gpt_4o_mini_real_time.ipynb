{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt\n",
    "\n",
    "You are tasked with generating Python scripts that perform real-time BGP analysis using the pybgpstream library. Please adhere to the following guidelines when writing the code:\n",
    "\n",
    "- Script Structure\n",
    "Include a __main__ block or a usage example to demonstrate how to run the script.\n",
    "Implement time-based stop triggers to gracefully stop data collection and processing after a specified duration.\n",
    "Use Separate Time Variables for Collection Duration and Metrics Interval\n",
    "Collection Duration Tracking:\n",
    "Use collection_start_time to track the total duration of data collection.\n",
    "Metrics Interval Tracking:\n",
    "Use a separate interval_start_time to track intervals for periodic tasks like printing metrics.\n",
    "Do not reset collection_start_time inside loops, as it affects the stop condition.\n",
    "\n",
    "- Key Processing Guidelines\n",
    "Initialize BGPStream Without Filters:\n",
    "stream = pybgpstream.BGPStream(\n",
    "    project=\"ris-live\",\n",
    "    record_type=\"updates\",\n",
    ")\n",
    "Implement Time-Based Stop Triggers:\n",
    "import time\n",
    "\n",
    "collection_start_time = time.time()\n",
    "interval_start_time = collection_start_time\n",
    "COLLECTION_DURATION = 300\n",
    "\n",
    "Checking for Stop Conditions Within the Loop:\n",
    "while True:\n",
    "    # Check if the total collection duration has been exceeded\n",
    "    if time.time() - collection_start_time >= COLLECTION_DURATION:\n",
    "        break\n",
    "\n",
    "    for rec in stream.records():\n",
    "        for elem in rec:\n",
    "            Processing logic goes here\n",
    "            Check if the collection duration has been exceeded inside nested loops\n",
    "            if time.time() - collection_start_time >= COLLECTION_DURATION:\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "Graceful Shutdown:\n",
    "Ensure that the script can be stopped gracefully after the specified duration or when a stop event is triggered.\n",
    "Clean up resources, close streams, and terminate processes properly.\n",
    "\n",
    "- Main Loop Processing\n",
    "Do not use any filter attributes like stream.add_filter() or set filter parameters when initializing BGPStream.\n",
    "All filtering and processing should occur within the main loop where you iterate over records and elements.\n",
    "for rec in stream.records():\n",
    "    for elem in rec:\n",
    "        Processing logic goes here\n",
    "\n",
    "Handling Potential Blocking in Data Streams:\n",
    "Be aware that data streams may block if no new data is received.\n",
    "Implement mechanisms to periodically check stop conditions even when data is not being received.\n",
    "Consider using timeouts, non-blocking iterators, or running data collection in a separate thread or process.\n",
    "\n",
    "- Accessing Element Attributes\n",
    "Timestamp:\n",
    "from datetime import datetime\n",
    "\n",
    "elem_time = datetime.utcfromtimestamp(elem.time)\n",
    "elem_type = elem.type  'A' for announcements, 'W' for withdrawals\n",
    "\n",
    "Fields Dictionary:\n",
    "fields = elem.fields\n",
    "\n",
    "Prefix:\n",
    "prefix = fields.get(\"prefix\")\n",
    "if prefix is None:\n",
    "    continue\n",
    "\n",
    "AS Path:\n",
    "as_path_str = fields.get('as-path', \"\")\n",
    "as_path = as_path_str.split()\n",
    "\n",
    "Peer ASN and Collector:\n",
    "peer_asn = elem.peer_asn\n",
    "collector = rec.collector\n",
    "\n",
    "Communities:\n",
    "communities = fields.get('communities', [])\n",
    "\n",
    "Validating and Parsing IP Prefixes:\n",
    "import ipaddress\n",
    "try:\n",
    "    network = ipaddress.ip_network(prefix)\n",
    "except ValueError:\n",
    "    continue\n",
    "\n",
    "Filtering Logic Within the Loop\n",
    "Filtering for a Specific ASN in AS Path:\n",
    "target_asn = '3356'\n",
    "if target_asn not in as_path:\n",
    "    continue\n",
    "\n",
    "Filtering for Specific Prefixes:\n",
    "target_prefixes = ['192.0.2.0/24', '198.51.100.0/24']\n",
    "if prefix not in target_prefixes:\n",
    "    continue\n",
    "\n",
    "- Processing Key Values and Attributes\n",
    "Counting Announcements and Withdrawals:\n",
    "from collections import defaultdict\n",
    "announcements = defaultdict(int)\n",
    "withdrawals = defaultdict(int)\n",
    "\n",
    "if elem_type == 'A':\n",
    "    announcements[prefix] += 1\n",
    "elif elem_type == 'W':\n",
    "    withdrawals[prefix] += 1\n",
    "\n",
    "Detecting AS Path Changes:\n",
    "prefix_as_paths = {}\n",
    "\n",
    "if prefix in prefix_as_paths:\n",
    "    if as_path != prefix_as_paths[prefix]:\n",
    "        # AS path has changed\n",
    "        # Handle AS path change\n",
    "        prefix_as_paths[prefix] = as_path\n",
    "else:\n",
    "    prefix_as_paths[prefix] = as_path\n",
    "\n",
    "\n",
    "- Analyzing Community Attributes:\n",
    "community_counts = defaultdict(int)\n",
    "\n",
    "for community in communities:\n",
    "    community_str = f\"{community[0]}:{community[1]}\"\n",
    "    community_counts[community_str] += 1\n",
    "\n",
    "- Calculating Statistics (e.g., Average MED):\n",
    "med_values = []\n",
    "\n",
    "med = fields.get('med')\n",
    "if med is not None:\n",
    "    try:\n",
    "        med_values.append(int(med))\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "Calculate average MED\n",
    "if med_values:\n",
    "    average_med = sum(med_values) / len(med_values)\n",
    "\n",
    "\n",
    "- Anomaly Detection Tasks\n",
    "Implement functions or logic within the main loop to detect:\n",
    "Hijacks:\n",
    "Compare the observed origin AS with the expected origin AS for target prefixes.\n",
    "Example:\n",
    "\n",
    "expected_origins = {'192.0.2.0/24': '64500', '198.51.100.0/24': '64501'}\n",
    "observed_origin = as_path[-1] if as_path else None\n",
    "expected_origin = expected_origins.get(prefix)\n",
    "if expected_origin and observed_origin != expected_origin:\n",
    "    print(f\"Possible hijack detected for {prefix}: expected {expected_origin}, observed {observed_origin}\")\n",
    "\n",
    "Outages:\n",
    "Monitor for sustained withdrawals of prefixes without re-announcements.\n",
    "Keep track of withdrawn prefixes and their timestamps.\n",
    "Example:\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "withdrawals_timestamps = {}\n",
    "\n",
    "if elem_type == 'W':\n",
    "    withdrawals_timestamps[prefix] = datetime.utcnow()\n",
    "elif elem_type == 'A' and prefix in withdrawals_timestamps:\n",
    "    del withdrawals_timestamps[prefix]\n",
    "\n",
    "outage_threshold = timedelta(minutes=30)\n",
    "current_time = datetime.utcnow()\n",
    "for prefix, withdrawal_time in list(withdrawals_timestamps.items()):\n",
    "    if current_time - withdrawal_time > outage_threshold:\n",
    "        # Outage detected\n",
    "        print(f\"Outage detected for {prefix} since {withdrawal_time}\")\n",
    "        del withdrawals_timestamps[prefix]\n",
    "\n",
    "\n",
    "MOAS (Multiple Origin AS) Conflicts:\n",
    "Monitor prefixes announced by multiple origin ASNs.\n",
    "Example:\n",
    "prefix_origins = defaultdict(set)\n",
    "\n",
    "origin_asn = as_path[-1] if as_path else None\n",
    "if origin_asn:\n",
    "    prefix_origins[prefix].add(origin_asn)\n",
    "    if len(prefix_origins[prefix]) > 1:\n",
    "        origins = ', '.join(prefix_origins[prefix])\n",
    "        print(f\"MOAS conflict for {prefix}: announced by ASNs {origins}\")\n",
    "\n",
    "AS Path Prepending:\n",
    "Detect AS path prepending by identifying consecutive repeated ASNs in the AS path.\n",
    "Example:\n",
    "prepending_counts = defaultdict(int)\n",
    "last_asn = None\n",
    "consecutive_count = 1\n",
    "\n",
    "for asn in as_path:\n",
    "    if asn == last_asn:\n",
    "        consecutive_count += 1\n",
    "    else:\n",
    "        if consecutive_count > 1:\n",
    "            prepending_counts[last_asn] += consecutive_count - 1\n",
    "        consecutive_count = 1\n",
    "    last_asn = asn\n",
    "\n",
    "Handle the last ASN in the path\n",
    "if consecutive_count > 1 and last_asn:\n",
    "    prepending_counts[last_asn] += consecutive_count - 1\n",
    "\n",
    "Report ASes performing prepending\n",
    "for asn, count in prepending_counts.items():\n",
    "    print(f\"ASN {asn} prepended {count} times\")\n",
    "\n",
    "Here is your task:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Monitor Current Trends\n",
    "Track real-time AS-level trends over period of 1 minute for AS3356. Summarize metrics like announcements, withdrawals, unique prefixes, and top AS paths by time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics Summary:\n",
      "Total Announcements: 15766\n",
      "Total Withdrawals: 502\n",
      "Unique Prefixes: 3077\n",
      "Top AS Paths:\n",
      "Path: 211509->48752->213045->34549->16509->14618, Count: 618\n",
      "Path: 211509->48752->29632->9002->16509->14618, Count: 568\n",
      "Path: 48362->1299->6453->4755, Count: 292\n",
      "Path: 35598->174->6453->4755, Count: 231\n",
      "Path: 62167->3356->6453->4755, Count: 227\n",
      "\n",
      "Metrics Summary:\n",
      "Total Announcements: 15766\n",
      "Total Withdrawals: 502\n",
      "Unique Prefixes: 3077\n",
      "Top AS Paths:\n",
      "Path: 211509->48752->213045->34549->16509->14618, Count: 618\n",
      "Path: 211509->48752->29632->9002->16509->14618, Count: 568\n",
      "Path: 48362->1299->6453->4755, Count: 292\n",
      "Path: 35598->174->6453->4755, Count: 231\n",
      "Path: 62167->3356->6453->4755, Count: 227\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import ipaddress\n",
    "import pybgpstream\n",
    "\n",
    "# Define constants\n",
    "COLLECTION_DURATION = 60  # Total duration for data collection (in seconds)\n",
    "INTERVAL_DURATION = 10     # Metrics interval duration (in seconds)\n",
    "\n",
    "def main():\n",
    "    # Initialize BGPStream\n",
    "    stream = pybgpstream.BGPStream(project=\"ris-live\", record_type=\"updates\")\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    collection_start_time = time.time()\n",
    "    interval_start_time = collection_start_time\n",
    "\n",
    "    # Metrics storage\n",
    "    announcements = defaultdict(int)\n",
    "    withdrawals = defaultdict(int)\n",
    "    unique_prefixes = set()\n",
    "    prefix_as_paths = defaultdict(list)\n",
    "\n",
    "    while True:\n",
    "        # Check if the total collection duration has been exceeded\n",
    "        if time.time() - collection_start_time >= COLLECTION_DURATION:\n",
    "            break\n",
    "\n",
    "        for rec in stream.records():\n",
    "            for elem in rec:\n",
    "                # Access element attributes\n",
    "                elem_time = datetime.utcfromtimestamp(elem.time)\n",
    "                elem_type = elem.type  # 'A' for announcements, 'W' for withdrawals\n",
    "                fields = elem.fields\n",
    "                \n",
    "                # Get prefix and validate\n",
    "                prefix = fields.get(\"prefix\")\n",
    "                if prefix is None:\n",
    "                    continue\n",
    "\n",
    "                # Validate and parse IP Prefix\n",
    "                try:\n",
    "                    network = ipaddress.ip_network(prefix)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "                # Track unique prefixes\n",
    "                unique_prefixes.add(prefix)\n",
    "\n",
    "                # Count announcements and withdrawals\n",
    "                if elem_type == 'A':\n",
    "                    announcements[prefix] += 1\n",
    "                    as_path_str = fields.get('as-path', \"\")\n",
    "                    as_path = as_path_str.split()\n",
    "                    prefix_as_paths[prefix].append(as_path)\n",
    "\n",
    "                elif elem_type == 'W':\n",
    "                    withdrawals[prefix] += 1\n",
    "\n",
    "                # Check for stop conditions within nested loops\n",
    "                if time.time() - collection_start_time >= COLLECTION_DURATION:\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        # Print metrics at the end of each interval\n",
    "        if time.time() - interval_start_time >= INTERVAL_DURATION:\n",
    "            print_metrics(announcements, withdrawals, unique_prefixes, prefix_as_paths)\n",
    "            interval_start_time = time.time()  # Reset interval timer\n",
    "\n",
    "    # Final metrics after collection ends\n",
    "    print_metrics(announcements, withdrawals, unique_prefixes, prefix_as_paths)\n",
    "\n",
    "def print_metrics(announcements, withdrawals, unique_prefixes, prefix_as_paths):\n",
    "    print(\"\\nMetrics Summary:\")\n",
    "    print(f\"Total Announcements: {sum(announcements.values())}\")\n",
    "    print(f\"Total Withdrawals: {sum(withdrawals.values())}\")\n",
    "    print(f\"Unique Prefixes: {len(unique_prefixes)}\")\n",
    "\n",
    "    # Calculate and display top AS paths\n",
    "    as_path_counts = defaultdict(int)\n",
    "    for as_paths in prefix_as_paths.values():\n",
    "        for as_path in as_paths:\n",
    "            as_path_str = \"->\".join(as_path)\n",
    "            as_path_counts[as_path_str] += 1\n",
    "\n",
    "    top_as_paths = sorted(as_path_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    print(\"Top AS Paths:\")\n",
    "    for path, count in top_as_paths:\n",
    "        print(f\"Path: {path}, Count: {count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics Summary:\n",
      "Total Announcements: 3338\n",
      "Total Withdrawals: 0\n",
      "Unique Prefixes: 814\n",
      "Top AS Paths:\n",
      "  Path: 20514 -> 2119 -> 3356 -> 58453 -> 9808 -> 56046, Count: 87\n",
      "  Path: 20514 -> 2119 -> 3356 -> 58453 -> 9808 -> 56040, Count: 85\n",
      "  Path: 20514 -> 2119 -> 3356 -> 58453 -> 9808 -> 24445, Count: 74\n",
      "  Path: 20514 -> 2119 -> 3356 -> 58453 -> 9808 -> 56042, Count: 58\n",
      "  Path: 48070 -> 174 -> 3356 -> 46573, Count: 51\n"
     ]
    }
   ],
   "source": [
    "import pybgpstream\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Constants\n",
    "TARGET_ASN = '3356'\n",
    "COLLECTION_DURATION = 60  # Duration in seconds (e.g., 5 minutes)\n",
    "\n",
    "def main():\n",
    "    # Initialize BGPStream\n",
    "    stream = pybgpstream.BGPStream(\n",
    "        project=\"ris-live\",\n",
    "        record_type=\"updates\",\n",
    "    )\n",
    "\n",
    "    # Start time tracking\n",
    "    collection_start_time = time.time()\n",
    "    interval_start_time = collection_start_time\n",
    "    announcements = defaultdict(int)\n",
    "    withdrawals = defaultdict(int)\n",
    "    unique_prefixes = set()\n",
    "    prefix_as_paths = defaultdict(list)\n",
    "\n",
    "    # Main loop for real-time processing\n",
    "    while True:\n",
    "        # Check for time-based stop condition\n",
    "        if time.time() - collection_start_time >= COLLECTION_DURATION:\n",
    "            break\n",
    "        \n",
    "        for rec in stream.records():\n",
    "            for elem in rec:\n",
    "                try:\n",
    "                    # Access element attributes\n",
    "                    elem_time = datetime.utcfromtimestamp(elem.time)\n",
    "                    elem_type = elem.type  # 'A' for announcements, 'W' for withdrawals\n",
    "                    fields = elem.fields\n",
    "\n",
    "                    # Prefix and AS Path handling\n",
    "                    prefix = fields.get(\"prefix\")\n",
    "                    if prefix is None:\n",
    "                        continue\n",
    "\n",
    "                    as_path_str = fields.get('as-path', \"\")\n",
    "                    as_path = as_path_str.strip().split()\n",
    "\n",
    "                    # Filter for specific ASN in AS Path\n",
    "                    if TARGET_ASN not in as_path:\n",
    "                        continue  # Skip this element if target ASN is not in the AS path\n",
    "\n",
    "                    # Proceed with processing after filtering\n",
    "                    peer_asn = elem.peer_asn\n",
    "                    collector = rec.collector\n",
    "                    communities = fields.get('communities', [])\n",
    "\n",
    "                    # Update counts for announcements and withdrawals\n",
    "                    if elem_type == 'A':\n",
    "                        announcements[prefix] += 1\n",
    "                        unique_prefixes.add(prefix)\n",
    "                        prefix_as_paths[prefix].append(as_path)\n",
    "                    elif elem_type == 'W':\n",
    "                        withdrawals[prefix] += 1\n",
    "\n",
    "                    # Print metrics every minute\n",
    "                    if time.time() - interval_start_time >= 60:\n",
    "                        print_metrics(announcements, withdrawals, unique_prefixes, prefix_as_paths)\n",
    "                        interval_start_time = time.time()  # Reset interval start time\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing element: {e}\")\n",
    "                    continue  # Continue with the next element\n",
    "\n",
    "            # Optional: Handle case where no data is received\n",
    "            if time.time() - collection_start_time >= COLLECTION_DURATION:\n",
    "                break\n",
    "\n",
    "    # Final print of metrics after the main loop ends\n",
    "    print_metrics(announcements, withdrawals, unique_prefixes, prefix_as_paths)\n",
    "\n",
    "def print_metrics(announcements, withdrawals, unique_prefixes, prefix_as_paths):\n",
    "    print(f\"\\nMetrics Summary:\")\n",
    "    print(f\"Total Announcements: {sum(announcements.values())}\")\n",
    "    print(f\"Total Withdrawals: {sum(withdrawals.values())}\")\n",
    "    print(f\"Unique Prefixes: {len(unique_prefixes)}\")\n",
    "\n",
    "    # Calculate and print top AS paths\n",
    "    top_paths = defaultdict(int)\n",
    "    for prefix, paths in prefix_as_paths.items():\n",
    "        for path in paths:\n",
    "            top_paths[tuple(path)] += 1\n",
    "    \n",
    "    print(\"Top AS Paths:\")\n",
    "    for path, count in sorted(top_paths.items(), key=lambda x: x[1], reverse=True)[:5]:  # Top 5 paths\n",
    "        print(f\"  Path: {' -> '.join(path)}, Count: {count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_39_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
